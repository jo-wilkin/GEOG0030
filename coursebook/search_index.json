[["index.html", "Geocomputation 2020-2021 Work Book Module Introduction Learning Objectives", " Geocomputation 2020-2021 Work Book Module Introduction Welcome to this year’s Geocomputation module, a course that introduces you to both the principles of spatial analysis and the use of programming for data analysis. Over the next ten weeks, you’ll learn about the theory, methods and tools of spatial analysis whilst implementing small research projects, first using Q-GIS, and then using the R programming language within the R-Studio software environment. You’ll learn how to find, manage and clean spatial, demographic and socio-economic datasets, and then analyse them using core spatial and statistical analysis techniques. The course is an excellent precursor for those of you interested in a career in (spatial) data science! The course will consist of approximately 10 lectures, 10 self-led practicals, 5 seminars (held online) and 5 coding help sessions (held online), further details of which are provided on the next page, Module Information. For now, if you’ve not watched the Introduction video on Moodle, you can catch up below: Geocomputation Introductory Video Remember you must have joined our Geocomputation Team on Microsoft Teams to be able to watch our lecture videos - instructions are provided on Moodle. Learning Objectives As you’ll have read in the Module Catalogue entry, the main learning objectives for the module are as follows: Understand the ways in which digital representations of the observable world are created, and how representations of neighbourhood communities are built from publicly available Open Data. Gain practical experience of the use of analytical methods to profile small areas of London. Understand the nature of geographic data, and the concepts of spatial autocorrelation, modifiable areal units and neighbourhood classification. Understand the sources and operation of uncertainties in the creation of geographic representations, and the importance of generalisation, abstraction and metadata. Gain practical experience of software, map design and visual communication. Develop practical skills in data acquisition and analytics, which may be useful in the planning of dissertations. We hope that you’ll learn many other things during the module and it inspires you to think about how you might use spatial analysis, GIS and programming in your future career! Getting in touch during the module The module is convened and taught by Dr Joanna Wilkin - you can contact her at j.wilkin [at] ucl.ac.uk or, for online office hours, you can book a half hour slot using MS Bookings. The module is further supported by two Postgraduate Teaching Assistants: Jakub (Kuba) Wyszomierski and Nikki Tanu. They will host coding help sessions on the alternative weeks to our scheduled seminar sessions. Acknowledgements Putting together a workbook such as this is no easy feat - but it’s also something that after a little time with R-Studio, you’d be able to produce! The reason for this, as we’ll repeat throughout the course, is that there is an incredible amount of resources available online that can help you learn the skills required to produce a website like this (e.g. using Git with R, using GitHub to host websites, R-Markdown, basic CSS styling). These skills firmly fall outside of the requirements for this course, but something you can build on in your spare time and over your future career. Believe it or not, we as lecturers are also always still learning - particularly, as you’ll find if you continue in spatial data science, the tools and technology available to us is continuously changing! Content-wise, the lectures and practicals for this course are all original this year to the Geography Department at UCL. There is some overlap between this course and the Principles of Spatial Analysis module that is run at the Master’s level, e.g. the extensions offered in several of the practicals. Aesthetics-wise, the R package and analysis artwork used within this book has been produced by allison_horst, whilst much of the artwork used in information boxes has been produced by Desirée De Leon, as well as by Jo. You can find Allison’s images on the stats illustration GitHub repository and Desirée’s on the rstudio4edu GitHub repository. Yihui Xie’s Authoring Books with R Markdown and rstudio4edu’s book, A Handbook for Teaching and Learning with R and RStudio were key resources in the creation and editing of this book. In addition, the CASA0005 practical handbook (by Dr Andy MacLachan and Adam Dennett) alongside our own Principles of Spatial Analysis practical handbook (by myself and Justin van Dijk) served as inspiration for the structure and formatting of this book. For some practicals, additional acknowledgements are made at the end where code or inspiration has also been borrowed! Noticed a mistake in this resource? Please let us know through the GitHub issues tab, send us a message over MS Teams, or contact us by e-mail. "],["module-information.html", "Module Information Self-guided learning for Geocomputation Reading List Troubleshooting Module Content Feedback: Weekly and End of Module", " Module Information If you’ll read one page of this entire workbook in depth, please make sure it is this one!. The following page outlines exactly how we hope you will engage with online learning for this course. Running a practical-based course online is not easy - for both lecturers and students alike! To help, we’ve tried to break down our content into short chunks, using a mix of recorded lecture videos, recorded practical and coding videos, as well as recommended reading and even short explanations of our own. Self-guided learning for Geocomputation The majority of your learning this year for Geocomputation will be ‘self-guided’ - however this is not to say, you’ll be learning alone. We’ll be running fortnightly seminars to check-in on your progress and discuss what you’ve learnt in our small groups (attendance is recorded), whilst also encouraging you to attend (optional) Help/ Study Group sessions on the weeks the seminars are not held. In addition, you’ll have small Assignments to complete, that we’re (time-permitted) hoping to provide you with small feedback on, either during the seminars or through discussion forums (held either on Teams or Moodle, depending on requirements, instructions will be provided as and when). Geocomputation Timetable A typical fortnight for Geocomputation will look something like this: Tuesday 10am: New content is released. Friday 5pm: Post/Send Assignment submissions for online feedback (Optional, but recommended) Monday/Tuesday (allocated slots): Help/Study Group, run by Jakub and Nikki - get help on your practical work if you’ve been unable to submit your work, bring articles you’ve read for discussion, discuss your feedback, and catch-up with friends! Tuesday 10am: New content is released. Friday 5pm: Post/send assignment submissions for seminar (if required) Monday/Tuesday (allocated slots): Seminar, run by Jo - discuss content for the last two weeks, bring articles you’ve read for discussion, discuss your feedback, and ask questions! And repeat! There will of course be a break for Reading Week where we will set you a short Coding Challenge to complete ready for the seminar at the start of the second half of term. In addition to our scheduled fortnightly help sessions, there will also be the weekly Coding Therapy classes run by PhD students within the Department to help you with coding issues if you get stuck and can’t wait for our help sessions. Please do use these classes in addition to our Help Sessions - they are there to help you with any module that requires any type of programming, not just Geocomputation, as well as your Dissertation! We will also provide some additional help sessions in the first two weeks of the Easter break to help you with your first Assessment. The hours for this are TBC. What will I be learning each week? As you’ll soon find out, you will learn a lot of different things in Geocomputation - from spatial analysis techniques to understanding how to write code to process and analyse data, as well as how to organise your investigation and use statistical and spatial analysis to answer research questions. To help, we have broken the course into three main sections: Foundational Concepts Core Spatial Analysis Advanced Spatial Analysis We hope this helps with the various learning curves you’re about to embark on - as outlined and explained further in Week 1’s content. The topics covered over the next ten weeks are: Week Date Section Topic Online Session 1 11/01/2021 Foundational Concepts Geocomputation: An Introduction N/A 2 18/01/2021 Foundational Concepts GIScience and GIS software Seminar 3 25/01/2021 Foundational Concepts Cartography and Visualisation I Help/Study Groups 4 01/02/2021 Foundational Concepts Programming (for Statistical Analysis) Seminar 5 08/02/2021 Foundational Concepts Programming for Spatial Analysis &amp; ESDA Help/Study Groups READING WEEK READING WEEK READING WEEK CODING CHALLENGE - - 6 22/02/2021 Core Spatial Analysis Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Seminar 7 01/03/2021 Core Spatial Analysis Analysing Spatial Patterns II: Geometric Operations &amp; Spatial Queries N/A 8 08/03/2021 Core Spatial Analysis Analysing Spatial Patterns III: Point Pattern Analysis Seminar 9 15/03/2021 Core Spatial Analysis Rasters, Zonal Statistics and Interpolation N/A 10 22/03/2021 Advanced Spatial Analysis Geodemographics Seminar In addition, there are additional optional topics offered at the end of term. The lectures and practicals of this course only form a part of the learning process. You are expected to undertake wider reading (see the Reading List below), particularly to help with your second assessment. In addition to our course, there are many other online resources and tutorials that can help expand on the topics and content we cover - whilst it is not necessary for your assessments, you are encouraged to go beyond our recommendations and fully engage with applied GIS research, methods and visualisation techniques. Following certain ‘movers and shakers’ in the GIScience / Spatial Data Science world is one approach to learn more about what’s happening in the field and might prove inspirational for your dissertation ideas later this year. Reading List We link to books and resources throughout each practical. The full reading list for the course is provided on the UCL library reading list page for the course. Alternatively, you can always easily find the link to the Reading List in the top right of any Moodle page for our module, under “Library Resources”. This Reading List will be updated on a weekly basis, in preparation for the week to come, so you may see some weeks without reading for now. But please check back at the start of each week as the lecture, seminar and/or workshop material is released for that week to check for new readings. All reading for that week will be provided by the time your learning materials are released - so you will not need to check the reading list for updates as the week progresses. Troubleshooting Module Content Spatial analysis can yield fascinating insights into geographical relationships. However, at times it can be difficult to work with - particularly when we combine this with learning how to program at the same time. You will get lots of error messages and have software crash, you’ll end up with bugs in your code that are difficult to find, and you may spend a whole day trying to track down a single dataset. But the rewards of learning how to do all of this (particularly with this year’s emphasis on this online research for your dissertations) will become apparent. To bring in my first of potentially several cycling references, a well-known quote from Tour De France (1986, 1989, 1990) winner Greg Lemond: “It never gets easier, you just go faster.” Resonates quite well with spatial analysis and programming! Even after years of programming, we can still forget the syntax to a for loop or question what kernel density estimation actually shows, but you will - by the end of the next ten weeks - know how to find out the answers to these problems faster! Beyond the help sessions mentioned above, if you need specific assistance with this course please: Attend the fortnightly GIS/coding help sessions (from Week 3) to ask questions directly to Jakub or Nikki. Post in the respective tech-help or r-help channels within the Geocomputation Team. Ask a question at the end of a seminar (time-permitting) Check the Moodle assessment tab for queries relating to the assessment (more information will be provided in Week 5) Attend the Coding Therapy sessions that are run on a weekly basis Due to the size of the class we will only reply to tech and R help messages on Teams so all students can see the discussion. If you have a personal matter in relation to completing the course then please speak to or email Jo. We’d also encourage you to monitor the tech-help or r-help channels and contribute to/answer questions as/if you can! Creating a small community across our course will help all of us in the long-run. If after pursuing all these avenues you still need help, you can book into our office hours. These meetings are to discuss a geographical concept in relation to the material/assessment or for any personal matters relevant to the completion of the module. Additional Online Help &amp; Resources We are here to help you work through these practicals but even we do not know everything. Therefore, it’s a good idea to become familar with online sources of help, such as: Stack Exchange RStudio community QGIS documemtation R documentation ArcGIS help pages Ultimately, if you are struggling to use R don’t worry…here is some advice from a tweet and interview with Hadley Wickham, chief scientist at RStudio… You're doing it right if you get frustrated: if you're not frustrated, you're (probably) not stretching yourself mentally — Hadley Wickham (@hadleywickham) 11. Februar 2015 It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. We highly advocate taking a break if you get stuck - the Workbook will not disappear, and you can complete the content at your own pace! Expanding your R learning (to infinity and beyond!) In addition to our course, there are many online tutorials that can help with learning R, specifically: Free RStudio Education resources Codeacademy YaRrr! The Pirate’s Guide to R Feedback: Weekly and End of Module We can only make this course better through your feedback. We collect feedback: At the end of the module. The standard UCL feedback form will be available to fill in on Moodle. Acknowledgements Part of this page is adapted from CASA0005. "],["what-is-this-workbook.html", "What is this Workbook? Using this Workbook Workbook Functionality", " What is this Workbook? All course content, including lectures and practical material, will be contained within this Workbook (hence it’s name!). As outlined earlier, you will also need to be part of our Geocomputation Team to have access to the lectures within the workbook. Any official course requirements (e.g. submission links for assessments) will be on Moodle. Each week’s content will be uploaded to the Workbook for Tuesday 10am UK time after the third seminar or coding help sessions. Using this Workbook Alongisde the recorded lectures and practical instructions, key things to look out for in the Workbook are Assignments, which are short optional submissions, Key Reading(s) and Points of Information, including Learning Objectives, Tips and Recap. To help, we’ll try to highlight them as follows: Assignment Each week, you’ll have 1-3 short assignments where we would like you to submit either a response, map or code prior to our seminar session or have it ready to present during the session itself. Key Reading(s) The recommended readings for this week will be highlighted in a Reading Box as and when appropriate in the week’s content. You’ll be able to find direct links to them within the E-Reading list. Learning Objectives Each week, we’ll start with a highlight of the learning objectives we hope you’ll achieve through the practical and lecture content. Get Ahead Tips Tips for effective note-taking during the practicals such as recording the functions you end up using in our practicals and your understanding of the arguments that they require. Recap A recap at the end of each section or week - make sure you take a note of these and are confident that you understand the points addressed. Workbook Functionality To get the most out of this book spend a few minutes learning how to control it, in the top left of this webpage you will see this toolbar: These buttons will let you: control the side bar search the entire book for a specific word change the text size, font, colour propose an edit if you see a mistake that I can review view the webpage in the ‘raw’ RMarkdown format, we cover RMarkdown in the course information about shortcuts for this book and most others like it In addition the icon in the top right of the page takes you to the GitHub repository for this book, where the online files for the book are stored. Acknowledgements Part of this page is adapted from CASA0005. "],["software-installation.html", "Software Installation QGIS R and R-Studio ArcGIS Installation Issues", " Software Installation This course primarily uses the R data science programming language and we strongly advise you complete the assignment using it. We briefly touch upon QGIS in the first few weeks to give you a basic foundation in spatial analysis alongside the range of spatial software available. Please follow the instructions below before completing the first practical session (in Week 2) to install the software on your local computer. Alternatively, both software are available on UCL’s computers and therefore can be accessed through Desktop Anywhere - however depending on your internet connection, this may be slow to use and, as a result, a highly frustrating experience! As outlined below, we have an online version of R-Studio available for use, but as yet, we do not have one for Q-GIS. If you are unable to download Q-GIS for your own computer, please let us know through the form below. QGIS QGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. To get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html We recommend installing the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates. R and R-Studio R is both a programming language and software environment - in the form of R-Studio- originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks. R-Studio Setup 1 Search for and open RStudio. You can install R Studio on your own machine from: https://www.rstudio.com/products/rstudio/download/#download R studio requires R which you can download from: https://cran.rstudio.com/ RStudio is a free and open-source integrated development environment for R — it makes R much easier to use. If you are using a Mac and run into issues, firstly follow the instructions below then check out the [Mac R issues] section if the problem persists. R-Studio Setup 2 UCL students (and staff) can now also make use of R Studio Server. It’s RStudio on a webpage, so no installation is required. Access information will be provided on Moodle in Week 2. ArcGIS ArcGIS Pro (previously ArcMap) is the main commercial GIS software that you may have already used - or seen/heard about through other modules or even job aderts. We do not use ArcGIS Pro in our Practicals for several reasons: Computing requirements for ArcGIS Pro are substantial and it only operates on the Windows Operating System. For Mac users, using ArcGIS Pro (and ArcMap) would require using iether a Virtual Machine or “splitting your own harddrive” to install a Windows OS. It is proprietary software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license (around £100!) to continue using the software and repeat any analysis you’ve used the software for. Whilst ArcPro can use pure Python (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own ArcPy and ArcGIS API for Python packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others if they do not have their own ArcGIS license. Recent developments in the ArcPro software however does make it an attractive tool for spatial data science - it has cross-user functionality, from data analysts who like to use a tool called Notebooks for their code development, to those focused more on cartography and visualisation with in-built bridges to Adobe’s Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but for this course, we want to ensure the reproducibility of work (you’ll learn more about this in Week 1’s lectures). Therefore, the analysis for your coursework must be completed in R/R-Studio and QGIS (where permissible, see guidance in Week 5). Installation Issues If you have any issues with installing either Q-GIS or R, please let us know during Week 1 via the tech-help channel within the Geocomputation Team. PLEASE CONFIRM YOUR SOFTWARE INSTALLATION We would appreciate it if you can fill in this Installation Confirmation Form to confirm whether you have been able to install the relevant software by Friday 15th January 2021 5pm UK time. Acknowledgements Part of this page is adapted from CASA0005. "],["external-usage.html", "External Usage Issues / Contributions License Version", " External Usage All the required data to run this course or individual practicals is publicly available through the direct links provided in the practicals. For UCL students, access to the Geocomputation Moodle page will enable you to access the pre-formatted datasets, when appropriate. There are two main options to adopt the written content and practicals of this course: Adopt the course in its entirety by forking the repository on GitHub and Pulling to your local machine or simply download a .zip file containing the entire course. Adopt a single practical by downloading the .rmd file. You will still need to follow the instructions within each practical to download the data - and format it as appropriate. For external users, you are welcome to get in touch with Jo (see previous details) directly if you would like access to the formatted files or help in how to format them. Issues / Contributions To raise an issue simply log it on the GitHub issues tab for the repository. To propose an edit click on the edit symbol in the top tool bar (see [How to use this book]) and submit it for review. If you wish to contribute material or data then please contact the course convenor Jo Wilkin (details below). License If you use this material for teaching, research or anything else please let me (Andy) know via Twitter or email — j [dot] wilkin [at] ucl [dot] ac [dot] uk). This practical book is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. You are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material for any purpose, even commercially. However, you give appropriate credit, provide a link to the license, and indicate if changes were made. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. But, you do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. The code within this pracical book is available under the MIT license; so it is free to use (for any purpose) as long as you cite the source. Version This is version 1.0 of the Workbook. Acknowledgements Part of this page is adapted from CASA0005. "],["geocomputation-an-introduction.html", "1 Geocomputation: An Introduction Getting Ready for our Practicals", " 1 Geocomputation: An Introduction Welcome to the first week of Geocomputation! Week 1 in Geocomp This week’s content provides you with a thorough introduction into what is Geocomputation, outlining how and why it is different to a traditional ‘GIScience’ course. We set the scene for the remainder of the module and explain how the foundational concepts that you’ll learn about in the first half of term fit together to form the overall Geocomputation curriculum. We also outline how the course is a great step towards those interested in a career in (spatial) data science. For this week only, there is no practical per se, but you will need to complete a few practical tasks in preparation for our future practicals. We appreciate that to get to this point in our content, you will have read a lot on both the Moodle and Coursebook and we do have a few readings we’d like you to do in anticipation of next week’s seminar. There are however 3 assignments that we’d like you to complete, two of which involve setting up your computer ready for next week’s practical. Learning Objectives By the end of this week, you should be able to: Understand the differences between traditional GIScience and Geocomputation Explain what spatial analysis is and why it is important for Geocomputation Understand why we will use programming as our main tool for data analysis Know how you will access both of required software for this course: QGIS and R-Studio Establish good file management practices, ready for the module’s practical content, starting next week. What is Geocomputation? According to Lovelace et al (2020): Geocomputation is a young term, dating back to the first conference on the subject in 1996…[Geocomputation] is closely related to other terms including: Geographic Information Science (GIScience); Geomatics; Geoinformatics; Spatial Information Science; [Spatial Data Science]; and Geographic Data Science (GDS). Each term shares an emphasis on a ‘scientific’ (implying reproducible and falsifiable) approach influenced by GIS, although their origins and main fields of application differ. GDS, for example, emphasizes ‘data science’ skills and large datasets, while Geoinformatics tends to focus on data structures…Geocomputation is a recent term but is influenced by old ideas. It can be seen as a part of Geography, which has a 2000+ year history (Talbert 2014); and an extension of Geographic Information Science and Systems (Neteler and Mitasova 2008), which emerged in the 1960s (Coppock and Rhind 1991). Geocomputation is part of but also separate to the wider discipline of GIScience (and Systems). As geographers, particularly ones at UCL, you are likely to have come across GIScience in one of its many forms, including the use of GIScience software, known simply as GIS software, such as ArcGIS Pro or ArcMap. What differentiates Geocomputation from traditional GIScience is that it is: working with geographic data in a computational way, focusing on code, reproducibility and modularity. Lovelace et al, 2020 We would also add that its main focus is on the analysis of data, rather than wider technological and informational challenges that GIScience also addresses. Suggested Reading If you’d like to read where the above quote is from, you’re welcome to get ahead of Week 5’s reading by looking at Lovelace et al (2020) linked below. This is just suggested reading for this week - and may make a little more sense when we come to Week 5. But there’s always benefits in doing (and reading!) things twice. Book Chapter (10 mins): Lovelace et al, An Introduction to Geocomputation with R, Preface and Introduction. What is important to recognise is that Geocomputation benefits from many of the epistemological and ontological developments that were made in the 1960s onwards within GIScience to enable us now to process substantial amount of spatial data, geographic and non-geographic, at signficiant speeds and visualise our results accordingly. This includes how we capture, record and store the world around us in a digital format, how to take this data and turn it into insight and also the more technical issues of data formats, storing assigned metadata such as projections, and ensuring cross-compatibility across different GIS software and programming languages. Key Definitions Geographic refers to space on the earth’s surface and near-surface. Non-geographic can refer to other types of space, such as network and graph space. The use of spatial incorporates both geographic and non-geographic space. You’ll also see geospatial analysis mentioned which subset of spatial analysis applied specifically to the Earth’s surface and near-surface. Although there are subtle distinctions between the terms geographic(al), spatial, and geospatial, for many practical purposes they can be used interchangeably. Within this module, our focus will be on how we can analyse spatial data in a computational way to address specific research questions - we will try to focus on issues that often concern geographers, including socio-economic and environmental challenges, such as driving factors of crime and deprivation, inequalities in access to greenspace and food and health establishments, and exposure to environmental concerns, such as poor air quality. To achieve this, we need to draw on specific foundational concepts from GIScience, such as spatial data models and data interoperability (Week 2), Cartography and Visualisation, including map projections (Week 3), alongside traditional Data Analysis, including using Statistics (Week 4), and also learn how to Program effectively and efficiently, particularly when it comes to using spatial data (Week 4 and 5). The remainder of this week’s content provides you with a brief introduction into each of these foundational concepts for Geocomputation. Before you get started with the rest of this week’s content, however, we’d like you to make sure you’ve installed the software ready for next week. It should also serve as a good break between reading and watching our lecture videos. Assignment 1: Download Q-GIS and R-Studio software Your first assignment for this week is to complete the steps found in Software Installation and complete the Installation Confirmation Form once done. GIScience: A Short History Almost everything that happens, happens somewhere. Longley et al, 2015 Geographic information has an important role across a multitude of applications, from epidemiology, disaster management and demography, to resource management, urban and transport planning, infrastructure modelling and many more. With almost all human activities involving an important geographic component, understanding where something happens – and also why – can often be the most critically important piece of information when decisions need to be made that are likely to affect individuals, communities, our increasingly connected societies, as well as the environment and ecology that exist in the area of study. Current methods of analysing geographic information have its roots firmly within the discipline of Geographic Information Science (GIScience), which first came into prominence in the 60s and 70s as the first Geographic Information System (GIS) was conceptualised by the “Father of Geographic Information Science and Systems”: Roger Tomlinson. He formalised the ideas within his Doctoral Thesis here at UCL in 1974, under the title “The application of electronic computing methods and techniques to the storage, compilation, and assessment of mapped data”. Whilst the thesis is nearly fifty years old, much of its content remains extensively relevant to the problems faced by the collection and processing of geographic data and analysis of geographic information today. Furthermore, he identifies two important requirements for the success of GIScience: Within the discipline of geography, it is suggested that the mutual development of formal spatial models and geographic information systems will lead to future beneficial shifts of emphasis in both fields of endeavour. Tomlinson, 1974 For GIScience to work as a discipline, there was a need to focus on both the development of spatial modelling (i.e. how to represent and analyse real world spatial phenomena in digital systems as spatial data) and of geographic information systems (i.e. how this data is stored, managed, retrieved, queried and visualised as information). Much of Tomlinson’s work contributed to establishing both the spatial models and GISystems we use today - and UCL has remained active in the development of this knowledge, culminating in our course textbook by Professor Paul Longley et al, who you will find in our Department. The foundations of GIScience have been built upon, with fifty years of development in the digital collection, recording, management, sharing and analysis of geographic data and information. For spatial modelling, the discipline has seen researchers develop and implement new methods and techniques of spatial representation and analysis to augment and extend the capabilities of working with spatial data. For GIS, the discipline has spawned a new industry focusing on the (commercial) development of GIS tools, software and applications. These tools have enabled different types of GIS, from databases to analytical software to online data services and servers. Furthermore, for both to work in unison with one another, GIScience has seen the establishment of the Open Geospatial Consortium, which aims to provide consensus on the standards and codes used with geographic data, information, content and services. The following short lecture provides an introduction to GIScience, including the topics that we’ll cover in more detail in next week’s lecture and practical. What is GIScience: past, present and future Slides | Video on Stream In addition to the short lecture, and in preparation for next week’s seminar, please read the following two Book Chapters: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 1: Geographic information: Science, Systems, and Society. Article (15 mins): Albrecht, J. (2020). Philosophical Perspectives. The Geographic Information Science &amp; Technology Body of Knowledge. We’ll cover the history of GIS software in a little more detail next week. Spatial Analysis: An Overview “80% of all data is geographic.” A geographer Whilst no one really knows the origin of this urban legend within GIScience, it is a quote that has been heavily used across the GIScience industry to explain the importance of spatial analysis and the untapped potential of spatial data. It is incredible to believe that just over a decade ago, when I was in your exact position, there was a need to justify why studying, collecting and analysing geographic information was important. In just over a decade, we have now seen a substantial transformation where analysing geographic data is no longer a niche activity, but almost omnipresent to our personal lives and our society at large. Suggested Reading Article (3 mins): Forbes, 2020, Mapping the way forward: GIS is powering solutions to global challenges We now have an entire map in our pocket, which not only provides us with spatial analysis on the fly, but the device on which this map exists itself provides data to others to conduct spatial analysis on our own behaviours. Not only can we find out the best route for us to drive to our favourite park at the touch of a button, we can also passively inform others on how long it will take them to get there too. We all now actively create, use and analyse spatial data - whether we are aware of it or not! The question that has faced those working in GIScience - and now in fields and disciplines beyond - is how to formalise these analyses into identifiable and, importantly, rigorous methods and techniques. Over the course of this module, we’ll introduce you to some of the core and advanced analysis methods that will be essential to analysing spatial data, including geometric operations, spatial autocorrelation, spatial regression, cluster analysis, interpolation and network analysis. These methods have been developed by those working actively in GIScience and spatial analysis, such as Dr Luc Anselin and his development of spatial autocorrelation methods. What you’ll learn - and quickly find out through your own application of these analysis methods - is that understanding the theory and principles behind them is just essential as knowing how to implement them, either through GIS software or programming. The following short lecture provides an introduction to spatial analysis and the techniques you’ll come across in the following weeks. Spatial Analysis: A key component of GIScience and beyond Slides | Video on Stream Assignment 2: Spatial Analysis and the COVID-19 Pandemic For your second assignment this week, we’d like to you think about how spatial analysis has been used in the current pandemic (don’t worry, this is one of the few times we’ll reference it moving forward!). Prior to next’s week seminar (and preferably by Friday 5pm), we’d like you to submit a short description (100~ words or less!) of an application you may be using, or have seen in the news, where you think spatial analysis has been critical to its success. You don’t need to know exactly how spatial analysis is being used, but you’re welcome to make a guess - you can also submit a reference as well, if you’d like. Also - an application does not necessary mean a phone app, but can be a tool, website, or dashboard - or anything else you can think of that has a spatial component to it! Please submit your description here! Programming for Data Analysis Concurrent to the developments within GIScience and spatial analysis, particularly over the last twenty years or so, we have begun to see a growing dataficaton of our everyday lives, where we: “take all aspects of life and turn them into data.” Cukier &amp; Mayer-Schöenberg, 2013 Our personal use of digital sensors - from our mobile phone data, use of online social networks and fitness trackers, to our travel and credit card - has created a deluge of data, most commonly known as ‘big data’. What sets ‘big data’ apart from traditional data is these data are often substantial in their volume, velocity and variety - making them difficult to manage, store, process and analyse. The hope, however, has been with big data is that by harnessing and ‘wrangling’ ita, we will be able to derive new insight from this data that can help address real world challenges, from something as simple as Google’s Traffic alerts within its Maps application, to tracking food security in areas where access for surveys are unfeasible. For GIScience and spatial analysis, what is important to note is that this data deluge and resulting specialisation has created a new approach to the analysis of data that goes beyond traditional data analysis, known as data science, which has worked its way into analysis streams and lexicon of many industries, from commercial organisations to academic research institutions. What distinguishes data science from traditional data analysis is that data scientists are able extract knowledge from these substantial datasets by using an intersection of three skills: hacking skills (or computational skills), statistical analysis and domain expertise. These computational skills - in the form of 1) programming, 2) distributed computing and 3) large-scale (complex) analysis - often set these data analysts apart from their traditional counterparts. The increasing popularity of data science is having a signficant impact on how we “do” spatial anaysis as more and more data scientists become involved with spatial analysis as, for many of these datasets, location is a key component for its management, processing and analysis - after all, everything happens somewhere. Concomittantly, there has also been an growing availability and accessibility of other geographical data, such as satellite and UAV imagery, that have significant interest to those working in these computational fields, such as computer vision and machine learning, such as extracting building and/or roads from true-colour satellite imagery. As a result, the “world” of geographic information has transformed rapidly from a data-scarce to a data-rich environment (Miller and Goodchild, 2015) and has garnered significant interest from those who do not necessarily consider themselves as working within the GIScience discipline. Their involvement has increased the utility of computational tools, such as programming languages and data servers, to ensure that traditional programming, data analysis and statistical langauges, such as Python and R can incorporate and conduct spatial analysis. This has involved the creation of many GIScience and spatial analysis focused libraries or packages (to be explained further in Week 4) within these programming languages, that have enabled analysts and researchers to run specific techniques or algorithms (such as calculating a buffer around a specific point) but for substantially larger datasets than traditional software can normally handle. Whilst this adoption of spatial analysis and GIS by non-GIScience practitioners certainly has (and continues to have) its pitfalls (as you’ll see later on in the module), there is also a growing influence and appeal of data science to many working in GIScience (and its related fields) – including its focus on analysing large-scale datasets that may have the potential to study geographic phenomena at unprecedented scales and detail. Unlike GIScience of ten years ago, there is, as a result, a pertinent need to teach these computational skills - first in the form of programming - to you as future GIS analysts, researchers or even data scientists, alongside the theory and principles of spatial analysis and the wider GIScience knowledge base. As you might guess where this is going, our focus on Geocomputation is a first step in this direction - which you may build upon in your third year, following through with modules such as Mining Social and Geographic Datasets. The following short lecture outlines the key reasons why we should program for spatial analysis: Slides | Video on Stream Programmming vs. GUI-GIS If you’ve not programmed before, the learning curve to program can be daunting - and also very frustrating! To be honest, even when you know how to program, it can still be incredibly frustrating! But the benefits of being able to program your data processing and analysis using a Command Line Interface (CLI) program, compared to using a traditional Graphical User Interface (GUI) sotware, are substantial. This does not mean using a GUI GIS does not have purpose or its own advantages. In my opinion, GUI GIS are incredibly useful tools to understand the “spatialness” of your spatial data and your spatial analysis, particularly when looking at spatial operations and spatial neighbours. The scripting aspect of R/Python often shield or hide you from this spatiality, which when you’re starting out with GIS and spatial analysis, is also an important learning curve! Furthermore, with GUI GIS, if you are interested in making paper-based maps and establishing your own “James Cheshire and Oliver Uberti” coffee table map books, learning map-making in a GUI GIS can be incredibly helpful in terms of understanding the flexibility of styling, label placement etc. ArcGIS, for example, has bridges with Adobe and its Creative Suite catalogue of software, enabling you to easily format maps you’ve made in ArcGIS Pro within Illustrator and/or InDesign. As a result, for the first two weeks of practicals - and part of the final practical - we will use Q-GIS so you have a basic understanding of how to use the software, and can then develop your use of the software outside of our course if and when you need. To understand more about what spatial (geographic) data science is (and why we program!), please read our other two key readings for this week: Key Reading(s) Article (25 mins): Brunsdon and Comber, 2020, Opening Practice: Supporting Reproducibility and Critical Spatial Data Scinece Article (10 mins): Singleton and Arribas-Bel, 2019, Geographic Data Science, Geographic Analysis In addition, you can watch this short video from Carto, a major commercial organisation working in several aspects of spatial data science. It outlines these key skills you’ll need to learn to become a competent spatial data scientist, including an understanding of spatial data, which many data scientists often lack prior to engaging with spatial data. Carto’s ‘What is spatial data science’ video What’s next for us in Geocomputation We believe strongly that effective users of GI systems require some awareness of all aspects of geographic information, from the basic principles and techniques to concepts of management and familiarity with applications. Longley et al, 2015 pg.32 For the next few weeks, we’ll be taking a deeper look at many of these foundational concepts that will ultimately enable you to be able to confidently and competently analyse spatial data using both programming and GIS software. As you might guess, you’ll therefore be going on many learning curves over the coming weeks - some that may feel familiar (e.g. applying descriptive statistics) and others that are more challenging (e.g. learning how to write code and debug it as you find errors). To help with this, I highly recommend that you try to stay organised with your work, including taking notes and making yourself a coding handbook. I’d also list the different datasets you come across - and importantly, the scales and different projections you use them at - more on this in the next two weeks. Finally, you should also make notes about the different spatial analysis techniques you’ll come across, including the different properties they assess and parameters they require to run. Furthermore, over the next nine weeks, you’ll learn how to plan, structure and conduct your own spatial analysis using programming – whilst making decisions on how to best present your work, which is a crucial aspect of any type of investigation but of particular relevance to your dissertation. Establishing an organised file system, for both your data and your documents, is essential to working effectively and efficiently as a researcher, whether in Gecomputation, Spatial Data Science or any other application you might think of! To this end, we move to the final part of our content for this week: creating our folders to establish good File Management procedures. Getting Ready for our Practicals To get ready for our practicals, which start next week, we would like you to set-up a file management system as follows (either on your local computer or DesktopAnywhere VM) - this will also help ensure any code you use from Week 4 onwards works without issue (*theoretically!): Create a GEOG0030 folder in your Documents folder on your computer (most likely inside a UCL or Undergrad or Geography folder, and then again within a Year 2 folder - although we are not mindreaders here ;) ). Next, within your GEOG0030 folder, create the following subfolders: data lecture_slides maps qgis notes any other folder types you may think you need for this course (although you can of course add these as the module continues) Note the _ separating the two words in lecture_slides folder. PLEASE DO NOT LEAVE ANY GAPS INBETWEEN YOUR FOLDER NAMES (OR FILE NAMES). We will explain why in our seminar next week. Also note we do not use any capitals in our folder names. Within your data folder, create the following subfolders: raw working final If you’ve downloaded the lecture slides, move these into your lecture_slides folder. We’ll explain more about establishing good file management procedures in the seminar at the beginning of next week. Assignment 3 Follow the above guidelines to create your folders in your local system ready for our practicals to begin next week. And that’s it. You’re now ready to start our practicals next week. We look forward to meeting you all in our first seminars next week and address any questions you might have from this week’s content! Week 1 Recap This week, we’ve provided you with an introduction to the Foundational Concepts you’ll be coming across in our course as we train you to become competent spatial data analysist. You should now: Understand the differences between traditional GIScience and Geocomputation. Be able to explain what spatial analysis is and why it is important for Geocomputation. Understand why we will use programming as our main tool for data analysis. Know how you will access both of required software for this course: QGIS and R-Studio - or flagged this as an issue to us via the form! Have created your file system for GEOG0030 ready to practice good file management for the module’s practical content, starting next week. "],["giscience-and-gis-software.html", "2 GIScience and GIS software", " 2 GIScience and GIS software Welcome to Week 2 in Geocomputation! I hope you have your favourite caffeinated (or not!) beverage at hand and some good concentration music because this will be a longer than usual week of work to get through - but if you concentrate, take your notes, and complete our practicals, it will hold you in good stead as you progress along our course. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 3 will be shorter in content, therefore you will have time to catch up before the seminars at the start of Week 4. Week 2 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with GIScience and GIS software. Out of all our foundational concepts you’ll come across in the next four weeks, this is probably the most substantial to get to grips with - and has both significant theoretical and practical aspects to its learning. This week’s content is split into 6 parts: What is Representation? (5 minutes) Geographic Representation (25 minutes) Spatial Structure, Sampling and Scale (25 minutes) Spatial Data Models (45 minutes) Spatial Data File Formats (20 minutes) Practical 1: Exploring Population Changes Across London (1 hour) Videos can be found in Parts 2-5, alongisde Key and Suggested Reading and the first two of 3 assignments. Video content this week is a mixture of short lectures from myself, and two videos from YouTube. The two explanations from YouTube summarise the content presented in this workbook succinctly and with some really interesting examples. Using these videos have allowed me to spend more time on your practical - including ensuring there is a practical for those of you who cannot download Q-GIS. Part 6 is our Practical for this week, where you will be introduced to Q-GIS and apply the knowledge gained in the previous parts from Parts 1-5 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical. Learning Objectives By the end of this week, you should be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and their respective file types Know how to manage and import different spatial file types into a GIS software Learn how to use attributes to join table data to vector data Introduce you to the concept of an Administrative Geography (more next week!) As stated above, there is a lot to go through in this week’s content - but everything you will learn this week will provide you with a comprehensive background for the following weeks in the module. What is Representation? To be able to conduct any spatial analysis using our GIS tools or software, we first need to establish how we capture the geographical features, processes and/or phenomena that we wish to study as digital data that is readable by ourselves and by our computers. Coming to GIScience, at this point of time in its development, we often take this above statement for granted – we are, as mentioned last week, surrounded by geographical (and social) data, where much of this conceptual work has been done and, as a result, is often hidden from us as users of this data and/or technology. We can, for example, access data on every single train station in England - we can download this data directly from OpenStreetMap. But you’ll have a choice - depending on how you wish to represent your train stations, which is usually determined by your work purpose. For example, are you looking to show the distribution of stations over England? Do you therefore download the train stations as individual points that you could label with their name? Train stations in England represented as points on a map. Data from © OpenStreetMap and its contributors However, is this truly enough to represent a “train station” - surely, you might want to have the building instead, because this is the actual “station” itself? But then again, is this still enough? Do you need to have the components that constitute a train station - the railway tracks, the ticket office (or ticket stations more common now!), and even the waiting room - to truly represent every train station in England? In our case example, when looking only at the distribution of train stations, a point representation is likely to be sufficient enough - but this representation does not tell us much about the size and service area of each of the stations - or much else about them! We often do not think nor question the representations used to present data to us when we use or interact with spatial data - until, for example, we see something wrong that does not fit with our expectations or does not contain the information we want or expect for our purpose. Often, at times, representations can also be misleading, if the right information is not conveyed - or conveyed in the wrong way. However, as you’ll find out below, we often need to weigh up including too much detail in our representations, particularly if this detail is redundant to the information we wish to convey. We therefore use representations to convey information about something in the real world - but these representations almost always simplify the truth. We simply can’t fit every piece of information about the world around us within a representation - we have to select what bits are most important and relevant to what we are trying to convey. What this also means is that for these representations to mean something to us (i.e. can be interpreted), they need to fit this information into a standard form or model that we have come to expect in their representation. In our case above, we are able to associate a point on a map as a point of interest - and understand the distribution of the train stations thusly - because this has become the most dominant way to represent the location of an entity in a simple format on a map. As a result: the creation of these representations have required significant epistemological and ontological developments in order to turn the complexities of the world around us into information that we can understand. This includes: How to “view” the world around us in ways that lend themselves to be modelled by digital data. How to “sample” the world around us to be able to model these “views” as digital data. How to structure these models as digital data to facilitate their processing and analysis. How to create standardised formats to store and share these digital data across programs, software and computers. As a result, there are established rules, and classification schema (“models”) to how we represent geographic pheonomena and processes, which you will learn about today. Definitions Epistemological: ‘what we know and how we can know it’ - the theory of how a piece of knowledge has come into being, including the methods behind generating its “truth” and the validity of these methods, the belief in this truth, and the justification of holding these beliefs. Ontological: ‘studying what there is’ – questioning how we see our reality and categorise it in order to determine how things come into being. Geographic Representation To be able to convert the world around us into digital geographic data, we first need to understand how we can represent the features, processes and phenomena we may want to study. As Longley et al (2015) explain: “Representations help us assemble far more knowledge about the Earth than is possible on our own…They are reinforced by the rules and laws that we humans have learned to apply to the unobserved world around us.” As outlined above, increasingly due to our use of digital technology, this representation itself is rarely seen or really understood by the users of the data – only those creating the data are likely to ever see its individual elements and/or components. But behind the data that you’ll become familiar with over the course of this module, there are significant and specific decisions that have been made, which you should be aware of in order to understand these data (and their limitations) fully. One of the major developments in GIScience was the creation of representations that can capture the different types of geographic phenomena and processes around us - which could then ultimately be modelled and turned into digital data. These representations view the world in two fundamental ways: as discrete objects and as continuous fields. In summary, the discrete object view represents the geographic world as objects with well-defined boundaries, within larger objects of well-defined boundaries, in otherwise empty space, i.e. similar to our reference mapping schematisation. In comparision, the continuous field view represents the real world as a finite number of variables, that can each be defined at every possible position to create a continuous surface of the respective variable. The following short video outlines these in more detail, with examples: Understanding how to represent the world around us However, one thing to note is that many geographic phenomena have both object and field characteristics. When representing and modelling many features, the boundaries are not often clearly continuous or discrete. A continuum is created in representing geographic features, with the extremes being pure discrete and pure continuous features. Most features fall somewhere between the extremes. An example could be looking at edges of forest and trying to define their boundaries – does the boundary stop at the tree trunk or the diffuse layering of leaves? A recent tweet from MapMaker David - one of my Twitter follow recommendations! This question actually poses itself to even the most experienced of GIS-ers and cartographers! David Garcia (aka Mapmaker David) is a Filipino Geographer and Cartographer and someone who I would highly advocate following on Twitter. He often raises a lot of questions about the epistemological and ontological aspects of GIScience and their development from essentially cartography and the role this has in minimising indigenous knowledge - he also makes beautiful maps. This critical approach to GIScience is something that we’ll look into a bit more in Week 5, in order to have time to give this content due justice! Assignment 1: Discrete Objects and Continuous Fields Let’s think about spatial representation models in more detail. Below are links to four spatial datasets that I’d like you to think about whether they represent discrete objects or continuous fields. Click on each link and note down your answer - I’ll be asking for these in our seminar in Week 4: Dataset Spatial Model Type USA Tree Canopy Cover ? Global Land Cover ? OS Open Rivers ? World Population Density Estimate ? Ultimately though, continuous fields and discrete objects only define two conceptual views of geographic phenomena, but do not solve the problem of digital representation, i.e. how do we capture this representation using computers. A continuous field view still potentially contains infinite amount of information as it aims to defines the values of the variable at every point – and there are an infinite number of points in any defined geographic area. In contrast, discrete objects can also require an infinite amount of information in order to provide a full description (e.g. our train station dataset above!). Neither of these approaches are designed to deal with the limitations of computers and the need to store this representation digitally - for this, we need to understand the spatial structure of the phenomena or process at study alongside the scale at which we want to represent them in order to devise a sampling scheme behind our data creation. Spatial Structure, Sampling and Scale Why do we need to sample our data? Well – if we try to include everything in our representation, we’d end up with a map the size of the world, which would be pretty useless! This issue is quite eloquently expressed by the Argentine writer, Jorge Luis Borges, who made up a fictional short story of the issue of an Empire aiming to create a map that was so perfect it could represent the whole empire - because it was the size of the Empire itself, coinciding point for point. As a result, the map, whilst perfect, was useless and was offered up to the elements to essentially destroy by the following generations! A short story on the issue of representation in science, geography and map-making On Exactitude in Science Jorge Luis Borges, Collected Fictions, translated by Andrew Hurley. …In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast Map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography. —Suarez Miranda, Viajes de varones prudentes, Libro IV,Cap. XLV, Lerida, 1658 (Borges’ fictional character of “the time”) Find more here: There is No Perfect Map by Marcelo Gleiser (5 mins) and Why a 70 year-old short story goes to the heart of modern map making by Ian Delaney (3 mins). To be able to create accurate representations of our geographic phenomena and processes, we therefore need to find a way to sample our phenomena or process to reduce the information whilst still retaining the most important pieces of information. You have probably come across the concept of sampling before when it comes to surveys and statistics and the need to create samples from a population. In this case, whenever we look to derive an accurate sample from a population, we look to create a sample frame or scheme to extract statistically significant information. In your previous research experience, you may have come across the ideas of random, systematic and stratified sampling - and that you choose the sampling approach that most reflects the likely structure or distirbution of the population you are targeting to sample. We can think of converting our geographic representations into digital data as a similar kind of sample, in that the elements of reality that are retained are abstracted from the observable real-world in accordance with some overall design. Therefore, to create digital data from our representation, we need to design a way to sample it. To do this, we first need to understand the structure of the data in order to deduce a good ‘sampling strategy’. The next lecture in this workshop provides an introduction to how we can use the structure of spatial data to determine appropriate sampling schemes. Understanding the structure of spatial data to determine sampling schemes Slides | Video on Stream When looking at the representation of geographic phenomena as digital data, the scale and level of detail of that is needed for the analysis will therefore determine the spatial sample design and how we can then generalise from these measurements. As a result, scale and level of detail are key to building appropriate representations of the world. Assignment 2: Digitising the River Thames, London, U.K We can put these ideas into practice by thinking about how we could create our own digital data. Let’s take what should be a straight-forward example of digitising the River Thames in London. The River Thames in London. Image: Esri. We’re going to use a very light online tool that allows us to create digital data (and as you’ll see later in the workshop, export the data we create as actual raw files). Head to geojson.io - it should load directly, zoomed into London. In the bottom left-hand corner, select Satellite as your map option. Next, click on the Draw a Polyline tool: Now digitise the river - simply click from a starting point on the left- or right-hand side of the map, and digitise the whole river. Once you’re done, simply double-click your final point to end your line. You can then click on the line and select info to find out how long the line is. For this assignment, I’d like you take a screenshot of you final line. When you click on the line, you can use Properties to style the line to make it more visible, e.g. change the colour and the width of the line. Please then post your screenshot on a new slide in your respective group’s Powerpoint you can find here and add a text-box stating how long your line is (in Km) (don’t worry, you don’t need to add your name). We’ll look at each other’s digitisation attempts during our seminar this week – but the questions to think about are: How easy did you find it to digitise the data and what decisions did you make in your own ‘sample scheme’? How close together are your clicks between lines? Did you sacrifice detail over expediency or did you spend perhaps a little too long trying to capture ever small bend in the river? How well do you think your line represents the River Thames? In the activity above, we were looking at the river as a discrete field – imagine then if I asked you to find a way to collect data on and then digitise the air quality over the same area of London? How would you go about creating an appropriate sample scheme to accurately represent air quality – without spending too much time on collecting the data that is becomes almost redundant? In both of these scenarios, you are using your a priori knowledge of the spatial structure of the phenomena to determine your spatial sampling scheme. However, in some scenarios, we may not know this structure before sampling nor can you always control for all variations in all characteristics. When looking to record a phenomenom as digital data at a fine scale, i.e. a high spatial resolution, we need to ensure our sample scheme reflects the minimal variation in the spatial autocorrelation with a feature. To record digital data at a coarse scale, i.e. a low spatial resolution, we can be more flexible with our sample scheme – but should ensure it reflects larger changes within our phenomenom. Whilst ideally we would want to capture our representation in as fine scale as possible as this is likely to be the most accurate, this sometimes can be detrimental to our capturing and storage of the representation as digital data (see the next section). Ultimately, a sampling scheme will be a best guess: we must remember that GIScience is about representing spatial and temporal phenomena in the observable world, and because the observable world is complicated (and does not always adhere to Tobler’s principles), this task is difficult, error prone, and often uncertain. As a result, with any data you use from other sources, always remember to consider its quality, accuracy and precision in representing geographic phenomena. Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 2: The Nature of Geographic Data. Computational considerations of the impact of scale and sampling One final thing to note when it comes to sampling spatial data at various scales is that if we try to sample and study complex phenomena at fine spatial resolutions but over significant extents, we may ultimately create many issues from a computational perspective. Whilst we may be able to sample our spatial phenomenon at a increasingly fine detail (e.g. satellite imagery can now collect data at less than a meter precision), this data ultimately has to be stored digitally. As a result, when looking to use increasing levels of precision over vast scales in terms of spatial coverage/extent, we can inadvertently create substantially large datasets that computers can struggle to visualise and process. As a result, we need to be conscientious about the data we are trying to create and use - for example, the Ordnance Survey’s MasterMap topography layer contains 400 million individual features (i.e. records). Trying to load even a subset of this on your computer can often cause significant processing problems! Usually, this means you have a choice. You can study something at a fine resolution, but you’ll need to keep your spatial coverage small. In comparison, you can expand your coverage if you reduce the resolution of your data. This all depends on the computational processing capability and capacity you have at your disposal, as well as what you are trying to achieve with your analysis, i.e. what detail do you need to answer your research questions. In addition, generalising is a key approach within GIScience that focuses on removing detail that is unnecessary for an application, in order to reduce data volume and speed up our processing. There are many approaches to generalising spatial data, which we come across in more detail over the coming weeks including simplification, smoothing, aggregation and amalgamation. Ultimately, we need a priori information to inform our understanding of whether our sampling scheme and resulting digital data is suitable for our analysis, i.e. it is accurate enough without hindering processing power. Determining an appropriate sampling scheme and resulting method of capturing this representation as digital data will therefore be determined by the phenomenom at study – and the limitations of those using and processing the resulting data. Spatial Data Models We can now see how we convert the observable world around us into spatial representations – and how we then need to consider scale and level of detail, alongside spatial structure, to determine our spatial sampling scheme. The next step is to convert our sampled observations (how ever they are collected) into digital data. Digital data at its basics is a form of binary data entry: the representation system in digital computers uses only two numbers (0 and 1). As a result, “Every item of useful information about the Earth’s surface is ultimately reduced by a GI database to some combination of 0s and 1s.” Longley et al, 2015 To create our modern day digital geographic data, we need to devise spatial formats that can ultimately be ‘written’ (or rather, ‘coded’) using this binary entry. Many of these decisions formed much of Roger Tomlinson’s original body of work (and others!). In this thesis, he outlined how to capture “real world data elements” as digitised geometries (points, lines, polygons) and grids - and how to store them in a coded digital format: Tomlinson’s original proposal for coding spatial data formats. Image: Tomlinson, 1974 These formats are the basis to the two main spatial data models we use. These are called raster and vector data formats, which are explained in further detail in this short video: Raster and Vector Spatial Data The below text summarises what was presented in the above video. Raster Data Format A raster dataset is a pixel-based grid data format. For any variable studied, a grid is created within which each pixel represents a value or measure for the variable: A raster grid and pixel. Image: QGIS Raster data only contain a single “attribute” for the variable it represents – and the attribute will be coded according to the data measurement scale and attribute type (see below). Rasters are primarily stored as a type of image file, that is either geo-referenced (e.g. a GeoTIFF) or comes with an additional georeferencing file (normally called a World file). Vector Data Format In comparison, vector data contains geometries: the points, lines and polygons we’ve seen earlier in the workshop. To provide the “geographic” component of these geometries, they actual geometry itself is specified using a pair of coordinates, preferably assigned to a specific coordinate reference system (the below diagrams simply use a graph!): Vector data: points, lines (polylines) and polygons (on a graph). Image: mgimond. As you can see, the three types of vector geometries are: A point dataset, which will have at least a single pair of coordinates for each point (or more generally “record”) within its dataset A single line, which will have two pairs of coordinates, whilst a polyline (multiple lines connected together) will have a minimum of three pairs. A polygon, which will have a minimum of three pairs (forming some sort of triangle!). Alongside containing these geometries, a vector dataset can also contain multiple attributes for each the records it contains. These attributes are stored in what is known as an Attribute Table. An Attribute Table consists of a set of records/observations (the rows) and attributes/fields (the columns): An example of an attribute table in ArcMap. Source: Esri Each record within the dataset will refer to one point, polygon or line (polyline) and will contain a value for each attribute/field that is part of the dataset. This includes a geometry field, which will contain the coordinates required to map and display the dataset correctly within its Coordinate Reference System (CRS) - more on these next week. The use of “field” for attribute tables At this point, it is important to note that you should not confuse the use of field here with our previous use of field in terms of spatial representation models. ‘Field’ and ‘scale’, as you can tell, have many meanings when used in GIS – but the more you come across the terms within context, the easier you’ll find it to understand which meaning is being referred to! These attributes will be stored in the field as a specific attribute measurement scale and as a specific data type - depending on the variable or data that they represent. Attribute Data Measurement Scales and Types For any data, whether spatial or not, it will collected against a specific measurement scale and, in its digital form, be stored as a specific type of data type. This measurement scale is a classification that describes the nature of the information of the values assigned to the specific variables. Data can be: Measurement Scale Explanation Nominal Has labels without any quantitative value. Ordinal Has an order or scale. Interval Numeric and have a linear scale, however they do not have a true zero and can therefore not be used to measure relative magnitudes. Ratio Interval data with a true zero. In addition, data may also be: Measurement Scale Explanation Binary Can have only two possible outcomes, yes and no or true and false, etc. Image: Allison Horst For example, for our point data set of train stations mentioned earlier: A field that contains the name of each train station would be nominal data. A field that details the class of the train station, e.g. whether it is a mainline, secondary or tertiary line as a type of order or rank, would be ordinal data. A field that details the temperature of the train station in celsius would be interval data A field that details the number of tracks the station contains would be ratio data A field that details whether the station is operational or not could be binary data (a ‘yes’ or ‘no’ or ‘operational’ or ‘non-operational’) Depending on the measurement scale, the attribute data will be stored as one of several data types: Type Stored Values Character Formats Short integer -32,768 to 32,768 Whole numbers Long integer -2,147,483,648 to 2,147,483,648 Whole numbers Float -3.4 * E-38 to 1.2 E38 Real numbers Double -2.2 * E-308 to 1.8 * E308 Real numbers Text Up to 64,000 characters Numbers, letters and words Knowing your measurement scale and data type level are essential to working accurately and effectively with spatial data. If you inadvertently store a float (e.g. values of 1.021, 1.222, 1.456, 1.512, 1.888) as an integer, your number will be rounded (e.g. it would become: 1, 1, 1, 2, 2) which can impact the accuracy of your work. Conversely, while storing whole numbers (integers) as a float or a double would not have an accuracy issue, it will come at a computational cost in terms of storage space. This may not be a big deal if the dataset is small, but if it consists of tens of thousands of records the increase in file size and processing time may become an issue. Being aware of (and checking!) your data types can also help solve initial bugs when loading and trying to analyse or visualise data in both GIS software and programming. For example, one commmon issue with data types when using table data within Excel prior to ingesting your data a GIS software or program is that Excel often converts British National Grid coordinate codes (which are integers) into text - therefore, when you come to display your point data, for example, by their coordinates, this field is not readable by your software or program. You therefore need to force your program to recognise that field as a numeric field - we’ll come across this issue and ways to solve it in Week 5. In addition to these attributes that contain variable information that might be used for analysis or visualisation purposes, each record should contain its own ID that will be used for indexing purposes in both GIS software and programming. This can help you select certain rows for analysis or order your data. Finally, in some cases, a dataset may contain a unique identifier (UID) for each record that can be used for data management purposes. These UID can be used to match with another dataset containing the same UID. In this latter scenario, this helps us join data that we may download as table data (e.g. a spreadsheet of population numbers for the different wards in London) with spatial data (e.g. a spatial dataset that shows the outlines of the wards in London) to create a new spatial dataset that contains the population data as an attribute, ready for its analysis and/or mapping. We’ll see this in action in today’s pratical. Don’t worry if this is a lot to take in right now, we’ll be utilising a lot of what you are reading about here in practice in the coming weeks! Rasterising vector and vectorising raster One additional thing to know about vector and raster data is that, in some cases, it is possible for both data formats to represent the same geographic feature, process or phenomena – but how they do so will look very different: Differences in capturing and storing geographic phenomena as vector and raster data. Image: vebuso. And also each data models comes with both advantages and limitations: Summarising key advantages and vector and raster data. Image: vebuso. There are also tools within our GIS software and programming software that will allow us to convert between the two data formats. This can be of use when we wish to process data faster (e.g. rasterising vector data) or we wish to add attributes to what was a continuous field (i.e. vectorising raster) for analysis. There will, of course, be considerations and limitations when switching between data formats, such as loss of accuracy in either direction of conversion. The results of vectorising elevation represented as a Digital Elevation Model Left image: DEM, Right image: vector version. Image: Esri. You will find more information on Spatial Data Models and the raster and vector data formats in the following two chapters in the Geographic Information Science &amp; Systems (GISS) book: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 3: Representing Geography. Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 7: Geographic Data Modeling. Spatial Data File Formats The final part to our introduction to spatial data is understanding the different file formats in which spatial data is stored. There are a number of commonly used file formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them. Shapefiles Perhaps the most commonly used spatial data file format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications. A shapefile is not a single file, but a collection of files of which at least three are needed for the data to be displayed in GIS software. The files include: File Type Description Required? .shp Contains the feature geometry Mandatory .shx Index file which stores the position of the feature IDs in the .shp file Mandatory .dbf Stores all of the attribute information associated with the records Mandatory .prj Contains all of the coordinate system information. Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used. Optional, but important .xml General metadata Optional, but important .cpg Encoding information Can also be included .sbn Optimization file for spatial queries Can also be included When using shapefiles, it is good to get into a habit of creating zipped archives of your file that you can share with yourself and others – this means selecting all the related files, right-clicking and choosing to compress or archive your data. This creates a single ‘file’ to move, for example across folders, so you do not end up losing any of the files that are critical for the shapefile to display! Copying and pasting the .shp file alone is not enough! This is one of the main criticisms of the shapefile – it is easy to lose files and as a result render your data useless. Other GIS formats such as GeoJSON and the increasingly popular GeoPackage include all of this information in a single file, reducing this risk substantially of this happening. Despite these issues, the shapefile still remains an ever-popular GIS format, and one you’ll use the most in this course. On Twitter and want to see the love for shapefiles….have a look at the shapefile account: GeoJSON GeoJSON (Geospatial Data Interchange format for JavaScript Object Notation) is becoming an increasingly popular spatial data file, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file. Comparing Shapefile and GeoJSON file formats If you would like, you can explore a shapefile (.shp ) and GeoJSON (.geojson) in action - we’ll use the light digitising tool, that we used earlier to digitise the River Thames: Head to: http://geojson.io/#map=16/51.5247/-0.1339 Image: Digitised point, line and polygon examples. 2. Using the drawing tools to the right of the map window, create 3 objects: a point, line and a polygon as shown above. Click on your polygon and colour it red and colour your point green. Using the ‘Save’ option at the top of the map, save two copies of your new data – one in .geojson format and one in .shp format. Open your two newly saved files in a text editor such as notepad or notepad++ on your computer. For the shapefile you might have to unzip the folder then open each file individually. What do you notice about the similarities or differences between the two ways that the data are encoded? I won’t ask you about this in our seminar, but it’s a good way to start getting familiar with the actual structure of our data. If you do end up having issues with your datasets, this may give you an idea of where you might find out if there’s an issue with your raw data itself. Geodatabase A geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab. GeoPackage A GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move. Raster Data Most raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostationary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type. Other Data Formats The aforementioned file types and formats are likely to be the ones you predominately encounter. However there are several more used within spatial analysis. These include: Vector GML (Geography Markup Language —- gave birth to Keyhold Markup Language (KML)) SpatialLite PostGIS Raster Band SeQuential (BSQ) - technically a method for encoding data but commonly referred to as BSQ. Hierarchical Data Format (HDF) Arc Grid There are normally valid reasons for storing data in one of these other file formats, however you do not need to read or know about these for now! In the end, the variety of data formats can be a bit overwhelming. But don’t worry, most of the time in this course you’ll be using shapefiles, table (in the form of csvs) or raster data. Table Data: Comma Separated Values (.csv) v. Excel Spreadsheet (.xls) In addition to spatial data, you will find that in this module (and for your dissertations), you will download and use a lot of table (tabular/spreadsheet) data. When you download this data, you can first inspect that data in Excel or Numbers (or another spreadsheet application of your choice), prior to loading it into either GIS software or programming software (such as R-Studio). The reason why is a lot of the time, you will need to clean this dataset prior to using it within these software/programs. Often the data comes formatted with too many rows, additional formatting, or generally just a lot of additional stuff we just don’t need. We’ll take a deeper look at this need for cleaning in Week 4 as we tackle using R-Studio. One thing to note though is that there are differences between a csv and an Excel spreadsheet, particularly if the latter is contained in a Workbook. There are a few summaries of these differences available online and we will go over the differences in further detail again in Week 4. For now, please be aware that we will be using csv as our default table data format, so if you need to save anything at any point in our practical, please save your file as a csv. GIS Software - a more thorough introduction (moved to Week 5) As outlined last week, this week, we were going to provide you with a more thorough introduction to the different types of GIS software available to you - but we’ve decided that you’ve read/listened/learned enough about spatial data that more information on GIS software is not going to help. Instead, we’ll cover this in Week 5 and now move onto our Practical. If you’d like to get ahead, you can read the following chapter in GISS: Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 6: GI System Software. Practical 1: Exploring Population Changes Across London The first half of this workshop has given you an in-depth introduction into how we can represent the world around us and turn it into digital geographic data – and how we store this data from a technical perspective. The practical component of the week puts some of these learnings into practice with an exploration of population data within London. The datasets you will create in this practical will be used in Week 3 practicals, so make sure to follow every step and export your data into your working folder at the end. The practical component introduces you to attribute joins. You’ll be using these joins throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. A Typical Spatial Data Analysis Workflow When using spatial data, there is generally a very specific workflow that you’ll need to go through - and believe it or not, the majority of this is not actually focused on analysing your data. Along with last week’s “80% of data is geographic data”, the second most oft-quoted GIS-related unreferenced ‘fact’ is that anyone working with spatial data will spend 80% of their time simply finding, retrieving, managing and processing the data – before any analysis can be done. One of the reasons behind this need for a substantial amount of processing is that the data you often need to use is not in the format that you require for analysis. For example, for our investigation, there is not a ‘ready-made’ spatial population dataset (i.e. population shapefile) we can download to explore popuation change across England: Image: Alas a quick google search shows that finding a shapefile of England’s population is incredibly difficult! Instead, we need to go and find the raw datasets and create the data layers that we want. As a result, before beginning any spatial analysis project, it is best-practice to think through what end product you will ultimately need for your analysis. A typical spatial analysis workflow usually looks something like this: Identify the data you need to complete your analysis i.e. answer your research questions. This includes thinking through the scale, coverage and currency of your dataset. Find the data that matches your requirements - is it openly and easily available? Download the data and store it in the correct location. Clean/tidy the data - this may be done before or after ingesting your data into your chosen software/program. Ingest/load the data into your chosen software/program. Transform &amp; process the data - this may require re-projection (next Week), creating joins between datasets, calculating new fields and/or creating selections of the data that you want to work with (Week 5). Run Analysis on your data, whatever technique you are using. Visualise your data and results, including maps, graphs and statistics. Communicate your study and outputs - through good write-ups and explanations of your visualisations. As you can see, the analysis and visualisation part comes quite late in the overall spatial analysis workflow - and instead, the workflow is very top-heavy with data management. Wrangling data is often the most time-consuming part of any spatial analysis project! Image: Allison Horst Often in GIS-related courses, you’ll often be given pre-processed datasets ready to go ahead with analysing the data. Instead, we’re going to start cleaning (the majority of) our data from the get-go. This will help you understand the processes that you’ll need to go through in the future as you search for and download your own data, as well as deal with the data first-hand before ingesting it within our GIS software.Good thing you’ll be learning a lot about these aspects over the coming weeks! Setting the scene: why investigate population change in London? For this practical, we will investigate how population has changed over the last ten years in London. Understanding population change - over space - is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure. We can also see first-hand the impact of wider socio-economic processes, such as urbanisation, or, in the case of the predicted population movements currently, relocation of a certain demographic of urban dwellers to rural areas. For us, the aim for our practical is to actually create population data for London in 2011, 2015 and 2019 at the ward scale that we can use within our future analysis projects, starting next week. This data will be used in our future practicals to normalise certain data, such as the crime datasets for next week. Why do we need to normalise by population? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event, such as crime. We’ll look at this in greater detail next week. Finding our datasets In the U.K, finding authoritative data on population and Administrative Geography boundaries is increasingly straight-forward. Over the last ten years, the UK government has opened up many of its datasets as part of an Open Data precedent that began in 2010 with the creation of data.gov.uk and the Open Government Licence (the terms and conditions for using data). Data.gov.uk is the UK government’s central database that contains open data that the central government, local authorities and public bodies publish. This includes, for example, aggregated census and health data – and even government spending. In addition to this central database, there are other authoritative databases run by the government and/or respective public bodies that contain either a specific type of data (e.g. census data, crime data) or a specific collection of datasets (e.g. health data direct from the NHS, data about London). Some portals are less up-to-date than others, so it’s wise to double-check with the ‘originators’ of the data to see if there are more recent versions. For our practical, we will access data from three portals: For our administrative boundaries, we will download the spatial data from the London Datastore (which is exactly what it sounds like!). For population, we will download table data from the Office of National Statistics (ONS) (for 2019 data to represnt 2020) and the London Datastore (only contains these data until 2018). In our extension activity (available later this week), we will also download a gridded spatial dataset showing how population can be represented in the raster data format from the Worldpop research group at the University of Southampton. Download and process datasets The first step in our practical is to download and process our two main datasets: administrative geography boundaries and population. Administrative Geography Boundaries For our administrative boundaries, we’ll download the ‘Statistical GIS Boundary Files for London’ dataset(s) found in the London Datastore. Navigate to the datasets, here: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london. If you navigate to this page, you will find multiple choices of data to download. We wil need to download the all three zipfiles: statistical-gis-boundaries-london.zip, London-wards-2014.zip and London-wards-2018.zip. The first dataset contains ALL levels of London’s administrative boundaries. In descending size order: Borough, Ward, Middle Super Output Area / MSOA, Lower Super Output Area / LSOA, and Output Area / OA) from 2011. The second dataset contains an UPDATED version of the Ward boundaries, as of 2014. The third dataset contains an UPDATED version of the Ward boundaries, as of 2020. As we will be looking at population data for 2015 and 2020, it is best practice to use those boundaries that are most reflective of the ‘geography’ at the time; therefore, we will use these 2014 / 2018 ward boundaries for our 2015 / 2020 population dataset respecitvely. When downloaded, depending on your operating system, the zip may unzip itself (or you may need to do this manually). When open, you’ll find two folder options: Esri and MapInfo. These folders contain the same set of data, but simply in two data formats: Esri shapefile and MapInfo TAB. MapInfo is another proprietary GIS software, which has historically been used in public sectors services in the UK (and many councils still use the software!), although has generally been replaced by either Esri’s Arc ecosystem or open-source software GIS. The TAB format is the main format that the software uses for vector data, similar to Esri and its shapefile format. In your GEOG0030/data/raw/ folder, create a new folder called boundaries. Within this folder, create three new folders: 2011, 2014 and 2018. Copy the entire contents of Esri folder of each year into their respetive year folder within your new boundaries folder: Note, we do not want to add the additional Esri folder as a step in our file sytem. I.e. your file path should read: GEOG0030/data/raw/boundaries/2011 for the 2011 boundaries, and GEOG0030/data/raw/boundaries/2014 for the 2014 boundaries etc. We now have our Administrative Geography files ready for use. We will ingest these directly into Q-GIS and do not need to do any cleaning at this stage. What are wards and boroughs? A short introduction to Administrative Geographies. Put simply, administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. Administrative areas ensure that each public body has a clearly defined area of responsibility, which can be measured and budgeted for appropriately. They originate from the Roman era who used these geographies, usually defined by topographical and geographical features, to administer these regions including collecting the relevant tax from those living in these areas. These geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. The UK has quite a complex administrative geography (see more here), particularly due to having several countries within one overriding administration and then multiple ways of dividing the countries according to specific applications. For the majority of your practicals, we will be keeping it simple with a focus on London, which is divided into: Boroughs -&gt; Wards OR Boroughs –&gt; Middle Super Output Areas -&gt; Lower Super Output Areas -&gt; Output Areas. We’ll be looking at wards in our practical analysis – although even at this fine scale, the City of London is a little pesky and introduces complexities into our analysis, which we’ll see. We’ll learn more about Administrative Geographies next week. Population Datasets For our population datasets, we will use the ONS mid-year estimates (MYE). These population datasets are estimates that have been modelled based on the previous 2011 census count and then forecasted population growth (plus some additional data). They are released a year, with a delay of a year, i.e. we can only access data for 2019 at the moment, so we’ll use this as our most recent year. As the London Datastore only has these MYE for up to 2018, we’ll need to download the data from ONS directly. It’s always worth checking the ‘originators’ of the data to see if there are more recent versions. Navigate to the Ward level datasets: https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/wardlevelmidyearpopulationestimatesexperimental When you navigate to this page, you will find multiple choices of data to download. We will need to download the estimates for 2011, 2015 and 2019. Click to download each of the zipfiles. Choose the revised versions for 2015 and the (Census-based) on 2011 wards edition for 2011. In your GEOG0030/data/raw/ folder, create a new folder called population and copy the three spreadsheets into this folder. Now it’s time to do some quite extensive data cleaning. Cleaning our Population Datasets If you open up the 2011 ward spreadsheet in Excel (or another spreadsheet program: this could be Numbers or you can upload your data to use it with Google Docs, for example), you’ll quickly see that there are several worksheets to this workbook. We are interested in the Mid-2011 Persons. Click on the Mid-2011 Persons tab and have a look at the data. As you should be able to see, we have a set of different fields (e.g. Ward Code, Ward Name), including population statistics. Right now, we have too much data - so what we will want to do is simplify and extract only the data we need for analysis. For this, we need the total population (All Ages), alongside some identifying information that distinguishes each record from one another. Here we can see that both Ward Code and Ward Name suit this requirement. We can also think that the Local Authority column might be of use - so it might be worthwhile keeping this information as well. Create a new spreadsheet within your program. From the Mid-2011 Persons spreadsheet, copy over all cells from columns A to D and rows 4 to 636 into this new spreadsheet. Row 636 denotes the end of the Greater London wards (i.e. the end of the Westminster LA) which are kept (in most scenarios) at the top of the spreadsheet as their Ward Codes are the first in sequential order. Before we go any further, we need to format our data. First, we want to rename our fields to remove the spaces and superscript formatting. Re-title the fields as follows: ward_code, ward_name, local_authority and POP2011. One further bit of formatting that you MUST do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave this commas in our values, Q-GIS will read them as decimal points, creating decimal values of our population. There are many points at which we could solve this issue, but the easiest point is now - we will strip our population values of the commas and set them to integer (whole numbers) values. To format this column, select the entire column and right-click on the ‘D’ cell. Click on Format Cells and set the Cells to Number with 0 decimal places. You should see that the commas are now removed from your population values. Save your spreadsheet into your working folder as ward_population_2011.csv. We now need to copy over the data from the 2015 and 2019 datasets as well into their own csvs. Open the Mid-2015 ward population xls from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2015 Persons tab. We now need to copy over the data from our 2015 dataset to a new spreadsheet again. However, At first instance, you’ll notice that the City of London wards are missing from this dataset. Then if you scroll to the end of the London Local Authorities, i.e. to the bottom of Westminster, what you should notice is that the final row for the Westminster data is in fact row 575 - this means we’re missing nearly other LAs in addition to our COL LAs and we will need to determine which ones are missing and if we can find them in the 2015 spreadsheet. With this in mind, first copy from row 5 to the end of the grouped London Local Authorities, i.e. to the bottom of Westminster, for columns A to D into a new spreadsheet. Through a quick scroll of the Local Authorities, a.k.a as Boroughs, (and with the extensive knowledge that you will soon build about London Local Authorities!) we can quickly find that we are missing the wards for: Hackney Kensington and Chelsea Tower Hamlets. If we head back to the original 2015 raw dataset, we can actually find this data (as well as the City of London) further down in the spreadsheet. It seems like these LAs had their codes revised in the 2014 revision and are no longer in the same order as the 2011 dataset - oh, the joys of using data! Locate the data for the City of London, Hackney, Kensington and Chelsea and Tower Hamlets and copy this over into our new spreadsheet. Double-check that you now have in total 636 wards within your dataset. Remember to rename the fields as above, but change your population field to POP2015. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2015.csv. We now need to repeat this for our 2019 data. I wonder what surprises this dataset has in store for us! Open the Mid-2019 ward population spreadsheet from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2019 Persons tab. Let’s have a look at our data - once again, there’s a lot to take in - but what we’re interested is in columns A, B, and now D and G. Let’s follow the same process we used above to copy our data across. To make our processing easier, first hide columns C, E and F in our spreadsheet - right-click on the columns at select Hide. Next, copy the data from row 5 to the final row for the Westminster data for columsn A, B, D and G over into a new spreadsheet. Look at the total number of rows you’ve copied over. We can see that we have even fewer wards than the 2015 dataset - yikes! We need to go hunting again for our missing data in the 2019 dataset. For expediency, you need to find and copy over the data for: City of London Hackney Kensington and Chelsea Tower Hamlets (as per 2015) and Bexley Croydon Redbridge Southwark Perhaps now you see why so much time is spent on processing data for spatial analysis! Copy over the remaining wards for these Local Authorities/Boroughs. Once you’ve copied them over - you should now have 640 wards - delete columns C, E and F and rename the remaining fields as you have done previously. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2019.csv. You should now have your three population csv datasets in your working folder. We’re now ready to start using our data within Q-GIS. Using Q-GIS to map our population data We will now use Q-GIS to create population maps for the wards in London across our three time periods. To achieve this, we need to join our table data to our spatial datasets and then map our populations for our visual analysis. Because, as we have seen above, we have issues with the number of wards and changes in boundaries across our three years, we will not (for now) complete any quantitative analysis of these population changes - this would require significant additional processing that we do not have time for today. *Data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to Administrative Geographies. In our extension activity Extension: Population as a Raster Dataset we show how we can complete this calculation easily when we use raster data that has a standardised grid format.* If you do not have access to Q-GIS, please click here to go to the alternative option: Week 2 Practical Alternate: Using AGOL for Population Mapping Start Q-GIS If you are not familiar with the Q-GIS environment, please watch our short video that explains its main components: Let’s start a new project. Click on Project –&gt; New. Save your project into your qgis folder as w2-pop-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. We will explain CRSs and using CRSs in GIS software v. programming in more detail next week. We will first focus on loading and joining the 2011 datasets. Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your 2011 boundary files. Here, we will select the London_Ward.shp dataset: Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. We can take a moment just to look at our Ward data - and recognise the shape of London. Can you see the City of London in the dataset? It has the smallest wards in the entire London area. With the dataset loaded, we can now explore it in a little more detail. We want to check out two things about our data: first, its Properties and secondly, its Attribute Table. The following short video explains these main components to using spatial data within Q-GIS. Right-click on the London_Ward layer and open the Attribute Table and look at how the attributes are stored and presented in the table. Explore the different buttons in the Attribute Table and see if you can figure out what they mean. Once done, close the Attribute Table. Right-click on the London_Ward layer and select Properties. Click through the different tabs and see what they contain. Keep the Properties box open. Before adding our population data, we can make a quick map of the wards in London - we can add labels and change the symbolisation of our wards. In the Properties box, click on the Symbology tab - this is where we can change how our data layer looks. For example, here we can change the line and fill colour of our Wards utilising either the default options available or clicking on Simple Fill and changing these properties directly. Keep the overall styling to a Single Symbol for now - we’ll get back to this once we’ve added the population data. You can also click on the Labels tab - and set the Labels option to Single labels. Q-GIS will default to the NAME column within our data. You can change the properties of these labels using the options available. I’ll add a thin buffer to my labels and change the font to Futura and size 8. You can click Apply to see what your labels look like. In my case, incredibly busy!: As its very busy, you may actually want to remove the labels from your dataset for the remaining processing - but hopefully this helps you understand how to add simple labels to your data. We’ll show you some more complex approaches in Week 10. Click OK once you’re done changing the Symbology and Label style of your data to return to the main window. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using Q-GIS right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 shapefile. First, we need to add the 2011 population data to our project. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text Layer. Click on the three dots button again and navigate to your 2011 population data in your working folder. Your file format should be set to csv. You should have the following boxes clicked: Decimal separator is comma; First record has field names’ Detect field types; Discard empty fields. Q-GIS does many of these by default, but do double-check! Set the Geometry to No geometry (attribute only table). Then click Add and Close*. You should now see a table added to your Layers box. We can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datatsets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. Right-click on your London_Ward layer –&gt; Properties and then click on the Joins tab. Click on the + button. Make sure the Join Layer is set to ward_population_2011. Set the Join field to ward_code. Set the Target field to GSS_code. Click the Joined Fields box and click to only select the POP2011 field. Click on the Custom Field Name Prefix and remove the pre-entered text to leave it blank. Click on OK. Click on Apply in the main Join tab and then click OK to return to the main Q-GIS window. We can now check to see if our join has worked by opening up our London_Ward Attribute Table and looking to see if our wards now have a Population field attached to it: Right-click on the London_Ward layer and open the Attribute Table and check that the population data column has been added to the table. As long as it has joined, you can move forward with the next steps. If your join has not worked, try the steps again - and if you’re still struggling, do let us know. Now, the join that you have created between your ward and population datasets in only held in Q-GIS’s memory. If you were to close the program now, you would lose this join and have to repeat it the next time you opened Q-GIS. To prevent this from happening, we need to export our dataset to a new shapefile - and then re-add this to the map. Let’s do this now: Right-click on your London_Ward shapefile and click Export –&gt; Save Vector Layer as... The format should be set to an ESRI shapefile. Then click on the three dots buttons and navigate to your final folder and enter: ward_population_2011 as your file name. Check that the CRS is British National Grid. Leave the remaing fields as selected, but check that the Add saved file to map is checked. Click OK. You should now see our new shapefile add itself to our map. You can now remove the original London_Ward and ward_population_2011 datasets from our Layers box (Right-click on the layers –&gt; Remove Layer). The final thing we would like to do with this dataset is to style our dataset by our newly added population field to show population distribution around London. To do this, again right-click on the Layer –&gt; Properties –&gt; Symbology. This time, we want to style our data using a Graduated symbology. Change this option in the tab and then choose POP2011 as your column. We can then change the color ramp to suit our aesthetic preferences - Viridis seems to be the cool colour scheme at the moment, and we’ll choose to invert our ramp as well. The final thing we need to do is classify our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation. We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on the drop-down next to Mode, select Natural Breaks, change it to 7 classes and then click Classify. Finally click Apply to style your dataset. A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using Q-GIS, which makes it a good playground to see how certain parameters or settings can change your output. You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Load the respective Ward dataset as a Vector Layer Load the respective Population dataset as a Delimited Text File Layer (remember the settings!) Join the two datasets together using the Join tool in the Ward dataset Properties box. Export your joined dataset into a new dataset within your final folder. Style your data appropriately. To make visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with Q-GIS, so for now, you can leave your symbolisation to the default settings. Alternatively, to set all three datasets to the same breaks, you can do the following: Right-click on the ward_population_2019 dataset and navigate to the Symbology tab. Double-click on the Values for the smallest classifcation group and set the Lower value to 141 (this is the lowest figure across our datasets, found in the 2015 data). Click OK, then Click Apply, then Click OK to return to the main Q-GIS screen. Right-click again on the ward_population_2019 dataset but this time, click on Styles –&gt; Copy Styles –&gt; Symbology. Now right-click on the ward_population_2015 file, but this time after clicking on Styles –&gt; Paste Style –&gt; Symbology. You should now see the classification breaks in the 2015 dataset change to match those in the 2019 data. Repeat this for the 2011 dataset as well. The final thing you need to do is to now change the classification column in the Symbology tab for the 2015 and 2011 datasets back to their original columns and press Apply. You’ll see when you first load up their Symbology options this is set to POP2019, which of course does not exist within this dataset. And that’s it - you can now make direct visual comparisons against your three maps. As you’ll be able to see, population has grown considerably in the London wards and there is are a few spatial patterns to this. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint: Click on Project –&gt; Import/Export –&gt; Export to Image and save your final map in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) QGIS, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. Acknowledgements Part of this page is adapted from CASA0005 and Introduction to GIS by Manuel Gimond. "],["cartography-and-visualisation-i.html", "3 Cartography and Visualisation I", " 3 Cartography and Visualisation I Welcome to Week 3 in Geocomputation! Well done on making it through Week 2 - and welcome to what is a more practical introduction to GIScience where we will be focusing on: how to make a good map. It’s not quite as “light” as promised, but this and the previous week will hold you in good stead as you come to learn about more technical analytical techniques after Reading Week. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 4 will be shorter in content, therefore you will have time to catch up after the seminars at the start of Week 4. The seminar will go through aspects of this week’s work, so it will still be incredibly useful if you do not manage to complete everything we outline in this workshop. Week 3 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with Cartography and Visualisation, where we have three areas of work to focus on: Map Projections Data Visualisation The Modifiable Areal Unit This week’s content is split into 4 parts: Coordinate Systems and Map Projections (40 minutes) Effective Data Visualisation (40 minutes) The Modifiable Areal Unit Problem (20 minutes) Practical 2: Mapping Crime Across London Wards and Boroughs (1 hour) Videos can be found in Parts 1-3, alongisde Key and Suggested Reading. This week, your 1 assignment is creating the final output from our practical. Part 4 is our Practical for this week, where you will be introduced to using the Map Composer with Q-GIS and apply the knowledge gained in the previous parts from Parts 1-3 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical but we recommend reading through the Q-GIS practical as unfortunately we are unable to repeat everything within the AGOL practical. Learning Objectives By the end of this week, you should be able to: Explain what a Geographic Coordinate System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. We will build on the data analysis we completed last week and create accurate maps that show changes in crime across our London wards. Coordinate Systems and Map Projections Maps, as we saw last week, are representations of reality. But not only are they are designed to represent features, processes and pheonomena in their ‘form’, they also need to represent, with fidelity, their location, shape and spatial arrangement. To be able to locate, integrate and visualise spatial data accurately within a GIS system or digtal map, spatial data needs to have two things: 1. A coordinate reference system (often written as CRS) 2. An associated map projection A CRS is a reference system that is used to represent the locations of the relevant spatial data within a common geographic framework. It enables spatial datasets to use common locations for co-location, integration and visualisation. Each coordinate system is defined by: Its measurement framework Unit of measurement (typically either decimal degrees or feet/metres, depending on the framework) Other measurement system properties such as a spheroid of reference, a datum, and projection parameters Its measurement framework will be one of two types: Geographic: in which spherical coordinates are measured from the earth’s center Planimetric: in which the earth’s coordinates are projected onto a two-dimensional planar surface. For planimetric CRS, a map projection is required. This projection details the mathematical transformation to project the globe’s three-dimensional surface onto a flat map. As a result, there are two common types of coordinate systems that you will come across when using spatial data: 1. Geographic Coordinate Systems (GCS): a global or spherical coordinate system such as latitude-longitude. 2. Projected Coordinate System (PCS): a CRS which has the mechanisms to project maps of the earth’s spherical surface onto a two-dimensional Cartesian coordinate plane. These PCS are sometimes reference to as map projections, although combine both location and the projection in their use. Understanding Coordinate Systems Slides | Video on Stream In summary, a GCS defines where the data is located on the earth’s surface, whereas a a PCS tells the data how to draw on a flat surface, like on a paper map or a computer screen. As a result, a GCS is spherical, and so records locations in angular units (usually degrees). Conversely, a PCS is flat, so it records locations in linear units (usually meters): Visualising the differences between a GCS and a PCS. Image: Esri For a GCS, graticules are used as the referencing system, which are tied directly to the Earth’s ellipsoidal shape. In comparison, within a PCS, a grid is a network of perpendicular lines are used, much like graph paper, which are then superimposed on a flat paper map to provide relative referencing from some fixed point as origin. Your data must have a GCS before it knows where it is on earth. But, whilst theoretically projecting your data is optional, projecting your map is not. Maps are flat, so your map will have a PCS in order accurately draw the data. In most GIS systems, a default projection will be used to draw the map and therefore the system will project your data to match this projection. For example, if you do not specify the projection of the map or data, both ArcGIS and Q-GIS will draw your map and corresponding data using a pseudo Plate Carrée or ‘geographic’ projection. The Plate Carrée Projection This projection is actually just latitude and longitude represented as a simple grid of squares and called pseudo because it is measured in angular units (degrees) rather than linear units (meters). It is easy to understand and easy to compute, but it also distorts all areas, angles, and distances, so it is senseless to use it for analysis and measurement and as a result, before you start your work, you should choose a different PCS! Which CS you will choose will depend on where you are mapping: most often, you will not need to choose a GCS as the data you are using was already collected and/or stored in a pre-selected system. For example, all GPS receivers collect data using only one datum or coordinate system, which is WGS84. Therefore any GPS data you use will be provided in the WGS84 GCS. However, you will often need to choose your PCS: which PCS you use depends on where you are mapping, but also the nature of your map — for example, should you distort area to preserve angles, or vice versa? For example, if you are using GPS data from the U.K, it is likely that you will transform this data into British National Grid (a PCS). Understanding Map Projections Either CS provides a framework for defining real-world locations - however, when it comes to much of GIScience and spatial analysis work, we will use a PCS to help locate, project, analyse and visualise our data in 2D. To locate, project, analyse and visualise our data in 2D, the PCS has, through mathematical transformations known as map projections, transformed the surface of our three-dimensional earth into a two-dimensional map canvas (whether paper or digital). This ability to create a flat surface from a 3D sphere is however not so simple! From a classic geographical metaphor, the easiest way to think about this is to think about peeling an orange - how could you peel an orange to ultimately result in a flat (preferably square/rectangular - computers really like squares!) shape? Well, luckily, you don’t need to think too hard about it - as Esri’s resident cartographer John Nelson (another Twitter recommendation) has done it for us: Trying to flatten an orange - our earth - into a flat map. Images: John Nelson, Esri As he shows, to create just a flat version of our earth from the spheriod itself, it takes some very interesting shapes and direction maniuplation - let alone achieving a rectangle! (You can see the original blog post these images are taken from here.) To create a classic square or rectangular map that we are so used to seeing, we have to use other geometric shapes that can be flattened without stretching their surface to help determine our projection. These shapes are called developable surfaces and consist of three types: Cylindrical Conical Plane The three types of projection families: cyclindrical, conical and plane. Image: QGIS However when any of using these shapes to representing the earth’s surface in two dimensions, there is always some sort of distortion in the shape, area, distance, or direction of the data. This distortion is explained through Vox’s excellent video: Why all world maps are wrong We can actually test out this distortion ourselves. You can head to The True Size (https://thetruesize.com) and see how our use of the Web Mercator has skewed our understanding of the size of countries in respect to one another. In addition, I highly recommend looking through this short (2 minutes!) blog post where a keen mapper got creative with his own orange peel: Blog post: Visualising the distortion of web mercator maps with an orange peel, Chris M. Whong, Online here Different projections can therefore cause different types of distortions. Some projections are designed to minimize the distortion of one or two of the data’s characteristics. A projection could, for example, maintain the area of a feature but alter its shape. Our second short lecture explains how to think through choosing a map projection: Choosing a Map Projection Slides | Video on Stream As explained in our lecture, each map projection therefore has advantages and disadvantages. Ultimately, the best projection for a map depends on the scale of the map, and on the purposes for which it will be used. As the excellent Q-GIS Projection documentation explains: For example, a projection may have unacceptable distortions if used to map the entire African continent, but may be an excellent choice for a large-scale (detailed) map of your country. The properties of a map projection may also influence some of the design features of the map. Some projections are good for small areas, some are good for mapping areas with a large East-West extent, and some are better for mapping areas with a large North-South extent. When it comes to choosing your map projection, think about: Is there a default projection for your area of study (e.g. London and British National Grid)? What analysis are you completing? What properties are important to this analysis? At what scale and direction are you visualising your data? What is critical to remember though, is that map projections are never absolutely accurate representations of our spherical earth. As a result of the map projection process, every map shows distortions of angular conformity, distance and area. Why should we care about projection systems? In summary, the projection system you use can have impact on both analytical aspects of your work, e.g. using measurement tools effectively, such as buffers, alongside visualisation. It is usually impossible to preserve all characteristics at the same time in a map projection. This means that when you want to carry out accurate analytical operations, you will need to use a map projection that provides the best characteristics for your analyses. For example, if you need to measure distances on your map, you should try to use a map projection for your data that provides high accuracy for distances. Furthermore, you need to be aware of the CS that your data is in, particularly when you are using multiple datasets. In order to analyse and visualise data accurately together, they must all be in the same CS. Transforming/Reprojecting Data If you are using datasets that are based on different geographic or projected coordinate systems, you will need transform all your data to one singular system: these are known as transformations. Between any two coordinate systems, there may be zero, one, or many transformations. Some geographic coordinate systems do not have any publicly known transformations because that information is considered to have strategic importance to a government or company. For many GCS, multiple transformations exist. They may differ by areas of use or by accuracies. Accuracies will usually reflect the transformation method. A geographic transformation is always defined in a particular direction, like from NAD 1927 to WGS 1984. Transformation names will reflect this: NAD_1927_To_WGS_1984_1. The name may also include a trailing number, as the above example has _1. This number represents the order in which the transformations were defined. A larger number does not necessarily mean a more accurate transformation. Even though a geographic transformation has a built-in directionality, all transformation methods are inversible. That is, a transformation can be used in either direction. Moving for with CRS in Geocomputation Keep in mind that map projection is a very complex topic. There are hundreds of different projections available that aim to portray a certain portion of the earth’s surface as accurately as possible on a digital screen/flat paper. In reality, the choice of which projection to use will often be made for you. When it comes to geocomputation and spatial analysis, you need to choose your CRS carefully - thinking through what is appropriate for your dataset, incuding what analysis you are completing and at what scale. You will find there are specific recommendations by country and, fortunately for us, most countries have commonly used projections. This is particularly useful when data is shared and exchanged as people will follow the national trend. Often, most countries will utilise the relevant zone within the Universal Transverse Mercator. In addition, a great resource is Esri’s documentation on Choosing a Map Projection. The Tyranny of Web Mercator One thing to watch out for though is the general (over)reliance on what is known as the Pseudo-Mercator projection (EPSG:3857) by web applications such as Google Maps. The projected Pseudo-Mercator coordinate system takes the WGS84 coordinate system and projects it onto a square. (This projection is also called Spherical Mercator or Web Mercator.) This method results in a square-shaped map but there is no way to programmatically represent a coordinate system that relies on two different ellipsoids, which means software programs have to improvise. And when software programs improvise, there is no way to know if the coordinates are consistent across programs. This makes EPSG:3857 great for visualizing on computers but not reliable for data storage or analysis. Luckily for us in Geocomputation, for the majority of our work, we will be using the British National Grid for our mapping and analysis as we are focusing on analysis on London. In this week’s practical, we will look at how we can reproject our spatial data from one a GCS to a PRS (in this case WGS84 to OSGB1936). Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 4: Geo-referencing. Optional: The Power of the Map Maps and map projections have had a long and complicated history with our politics and geopolitics. For example, whilst maps have existed in many forms prior to the periods, we cannot ignore their signficant use for land acquisition and resource exploitation during the “Age of Discovery” and resulting colonialism eras. There is significant power embedded within a map and, even to this day, as we see with the use of the Mercator projection in web technology, a map can be a substantial propaganda tool when it comes to political issues. Google Maps, for example, has found itself at the centre of various border disputes across the world - resulting, in several occasions, with troop mobilisation and threats of war: By misplacing a portion of the border between Costa Rica and Nicaragua, Google effectively moved control of an island from one country to the other and was cited as the justification for troop movements in the region in 2010. The Washington Post, 2020 To further avoid this, Google has created a new techno-political approach within its Google Maps platform in that the world’s borders will look different depending on where you’re viewing them from. You can read more about this a recent article by The Washington Post: Google redraws the borders on maps depending on who’s looking (10 minutes). Maps therefore are never true representations of reality, but will always include some bias - after all, maps are still very much made by humans. Whilst we won’t cover this in any more detail in our lecture or practical content this week, we do hope you enjoy discussing these issues in your Study Group sessions. In addition, there are many excellent books on this power of maps, including Denis Wood’s The Power of Maps and follow-up, Rethinking the Power of Maps and Mark Monmonier’s How to Lie with Maps. These books all outline how both paper and modern digital maps offer opportunities for cartographic mischief, deception, and propaganda. If you’d like to avoid reading for a little longer, I would also highly recommend this excerpt from the “before your time” show, the West Wing, which summarises quite a few of the debates well: Effective Data Visualisation In addition to choosing the correct map projection for your spatial data and map, to visualise your data correctly as a map - for visual analysis and publishing - you need to consider: How you represent your spatial data effectively. How you present this data on a map that communicates your data and analysis accurately. We will first focus on the latter aspect and look at how you can achieve effective data visualisation, including how to make a good map as well as detailing the common cartographic conventions we’d expect you to include in your map. Then we look at common types of spatial data and focus on how we can accurately represent event and survey data that are commonly aggregated to areal units (such as the Administrative Geographies we came across last week) for use within choropleth maps. Cartographic Conventions Making a good map is a highly subjective process - what you think looks good versus what someone else thinks looks good maybe entirely different. That’s why there is a whole discipline out there on cartography - it’s also why good data visualisation skills are becoming essential within data scientist roles. As a result, I can highly recommend taking the Cartography and Visualisation module by Prof James Cheshire next year! At its most fundamental, a map can be composed of many different map elements. They may include: The main map Map graticules A legend (including symbols) A title A scale bar or indicator An orientation indicator, i.e. a North Arrow An inset map (to locate your map within a wider area) Data Source information Any ancillary information These elements are all part of the expected cartographic conventions, i.e. what should be included on/within your map in order to accurately convey all the information contained within your visualisation. Map elements. Image: Manuel Gimond However, not all elements need to be present in a map at all times. In fact, in some cases they may not be appropriate at all. A scale bar, for instance, may not be appropriate if the coordinate system used does not preserve distance across the map’s extent. Knowing why and for whom a map is being made will dictate its layout: If it’s to be included in a paper as a figure, then simplicity and restraint should be the guiding principles. If it’s intended to be a standalone map, then additional map elements may be required, such as customised borders, graphics etc. Knowing the intended audience should also dictate what you will convey and how: If it’s a general audience with little technical expertise then a simpler presentation may be in order. If the audience is well versed in the topic, then the map may be more complex. Ultimately, to make a good map there are several rules you can follow: Visual hierarchy: Making sure the most important elements are the most visible on the map (e.g. size, placement on map, colour scheme). Colour schemes: Keeping colour schemes simple (less than 12 colour at max) and representative of the data you are showing (more on this later) as well as suitable to all audiences (e.g. being aware of mixing colours indetectable to those colourblind/visually impaired) Scale bars and north arrows: Should be used judiciously! They are not needed in every map, nor do they need to be extremely large - just readable. I advise trying to locate the two together and keeping their design as simple as possible. Title and other text elements: Again, less is more! Never use “A map of…” in your title - we know it’s a map! Keep font choices simple and reflective of the topic you are mapping. Titles are not needed on maps with figure captions. Make legends readable - including simplifying their values. Utilise font size effectively to ensure communication of the most important aspects. The following short lecture explains in more detail how to make a good map: Cartographic Conventions and Effective Data Visualisation Slides | Video on Stream Representing Spatial Data The second aspect of creating effective maps is to ensure that you are representing the type of data you are using effectively and accurately. As we saw last week, spatial data itself is only a representation of reality. Some of the types of data we use can be very close representations of reality, such as ‘raw’ geographic data (including satellite imagery or elevation models), whilst other datasets, when used in maps, may be far abstract representations of reality. The different common types of spatial data you might come across in spatial analysis are outlined in the table below: Common Types of Spatial Data Data Type Examples Digital Representation ‘Raw’ Geographic Data Satellite Imagery LIDAR/RADAR imagery Environmental Measurements (e.g. elevation, air quality, water levels) Raster/Grids Coordinates / Point Data, with attributes Processed or Derived Spatial Data Geographic Reference Data (e.g. buildings, roads, rivers, greenspace) Gridded Population (Density) Data Digital Elevation Models Air Quality Maps Points, Lines and Polygons Raster/Grids (Spatial) Event (Count) Data Human Activities ( e.g. crime, phone calls, house sales) Scientific Recordings (e.g. animal and plant sightings) Coordinates / Point Data, with attributes Statistical Survey or Indicator Data Human Characteristics (e.g. demographic, socio-economic &amp; health information) Scientific Recordings (e.g. total animal counts, leaf size measurements) Voting Tabular Data, representative at a specific spatial aggregate scale, i.e. areal unit Whilst we will come across a variety of these types of spatial data on this course, our main focus for the first few weeks are looking at Event and Statistical data - because these are the two types of data that are primarily used within the most common data visualisation map tool: a choropleth map. Choropleth Maps At its most basic, a choropleth map is a type of thematic map in which a set of pre-defined areas is colored or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population density or crime rate. When using either Event Data or Statistical Data, we tend to aggregate these types of data into areal units, such as the Administrative Geographies we came across last week, in order to create these choropleth maps. Because we see choropleth maps in our everyday lives, choropleth maps, I would say, out of any type of map-based data visualisation are the maps most vulnerable to poor use and data representation. We often think it’s a simple case of linking some table data with our areal units and then choosing some pretty colour scheme… An Example Choropleth: London’s Wasted Heat Energy at the MSOA scale. The question is: do you think it looks good? What would you change? Image: Mapping London …However, within a choropleth map, many decisions need to be made in terms of thinking through their classification (categorical or continuous/graduated), the ‘class breaks’ used, as well as the type of colour schemes used. Furthermore, a key challenge to using choropleth maps is that often the areal units we use are not of equal area - as a result, we have to be careful in how we represent our chosen dataset. Showing population as a ‘raw’ geographic fact across London Wards as we did last week, for example, would actually be a big no-no in terms of mapping population. Instead, we would want to show the population density - by normalising our population by the area of each ward. What’s still missing from this map? London Ward Population Density 2019. Data: ONS Without taking these normalisation approaches, we can create incredibly misleading maps. At the most basic, our brain sees the larger areal units within our map as having more of whatever quantity we are representing, irrespective of thinking through the underlying area (and/or population) it is actually representing. This was common amongst the US election maps, for example, where many of the Republican states have a large landmass - but ultimately a low population. Therefore, when representing the results of the election as a categorical choropleth, it presents an overwhelming Republican landslide. However, as we all know, whilst the Party won the Electoral College vote, the Democrats actually won the Popular Vote by 3 million votes. Hence, when mapping by number of votes rather than state outcome, a different message is conveyed, as we see below. Alas, despite this difference in total votes, the US runs an Electoral College System and in the end, the winner is the winner of the Electoral College vote and no map coud or can change that! Different approaches to mapping the 2016 election result in different information communicated (L-&gt;R: Business Insider, Time, xkcd) Despite their various challenges, choropleth maps can be increidbly useful tools. We provide a more detailed introduction to how to create choropleth maps in the following lecture: An Introduction to Choropleth Maps Slides | Video on Stream The Modifiable Areal Unit Problem The final aspect of good map-making we will cover actually focuses on how we process and resultantly analyse our data when we aggregate individual event or statistical data to areal units. When using choropleth maps to represent aggregated data, there are three key analytical challenges you need to be aware of, in order to not fall into the “trap” of the first two, whilst also thinking about ways to address the allter. The are three key challenges: Ecological Fallacy (EF): EF occurs when you try to make inferences about the nature of individuals based on the group to which those individuals belong (e.g. administrative unit). This applies when looking at correlations between two variables when using administrative geographies or looking at averages within these units. Whilst your areal unit may represent the aggregation of individual level data, you can not apply your findings from the analysis of this map to the individuals directly. You can only apply your conclusions to the area that you have aggregated by, e.g. at the Ward scale. The Modifiable Areal Unit Problem (MAUP): Spatial data is scale dependent - when data are tabulated according to different zonal systems at different scales and are then analyzed, it is unlikely that they will provide consistent results - even though the same variables are used and the same areas are ultimately analyzed. As a result, the results from your analysis are only relatable to those precise areal units used. This variability or inconsistency of the analytical results is mainly due to the fact that we can modify areal unit boundaries and thus the problem is known as the MAUP. It is one of the most stubborn problems in spatial analysis when spatially aggregated data are used. Fundamentally, you cannot extrapolate your findings at one scale to another, i.e. any conclusions drawn at the Ward level in London cannot be applied to the Borough level, even though, for example, your wards may “fit” within the Borough scales. Boundary Issues: Spatial data does not have “boundaries” - the use of artifical boundaries such as Administrative Geographies are indiscriminate to the spatial prcoesses that may actually underline the distribution of these phenomena at study. As a result, simply using these boundaries per se can bring about different spatial patterns in geographic phenomena - or simply disregard them in their entirety. We have to use Administrative Boundaries with care and think about the underlying processes we are trying to measure to see if we can account for these discriminatory issues. In summary, whenever you conduct spatial analysis using areal units – you cannot infer about the individuals within those units nor can you assume your findings will apply at coarser scales. You also need to take into account the “decisive and divisive” nature the use of areal units can have on individual level data when aggregating. We will begin to look at MAUP in this week’s practical and Week 4’s seminar and continue accounting for and considering its impact over the next few weeks of our analysis. A more detailed introduction to Administrative Geographies As we read and saw last week, an administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. Generally, they contain 4-5 levels of administrative boundaries, starting at Level 0, with the outline of the country, Level 1, the next regional division, Level 2, the division below that etc. Each country will have a different way of determining their levels and their associated names – and when you start to add in differentiating between urban and rural areas, it becomes a whole new level of complexity. What is important to know is that these geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. For the U.K, we can access the spatial data of our Administrative Geographies from data.gov.uk (and a few other sources). Any country with their own statistics or spatial office should have these datasets available. If not, you can find data (for pretty much all countries) at gadm.org, which allows you to download and use the data for non-commercial purposes. As a note of interest at this point, in the U.K., it is generally understood that for publishable research, we do not analyse data at a smaller scale than something called the Lower Super Output Area (LSOA). There is another administrative unit below the LSOA, known as the Output Area, which (again due to ensure confidentiality of data) has a minimum size of 40 resident households and 100 resident people but for particular types of research, this level of detail can still lead to unintended consequences, such as households being identified within the data. Practical 2: Mapping Crime Across London Wards and Boroughs The first half of this workshop has given you an in-depth introduction into how we can create a successful map, including understanding map projections, cartographic conventions and issues faced with the analysis of aggregated data at areal units. The practical component of the week puts some of these learnings into practice as we analyse crime rates within London at two different scales. The datasets you will create in this practical will be used in the Week 4 practical, so make sure to follow every step and export your data into your working and final folders (respectively) at the end. The practical component introduces you to point-in-polygon counts. You’ll be using these counts throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. Setting the scene: why investigate crime in London? Over the next few weeks, we will look to model driving factors behind crime across London from both a statistical and spatial perspective. As Reid et al (2018) explain: Spatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime. Crime mapping allows researchers and practitioners to explore crime patterns, offender mobility, and serial offenses over time and space. Within the context of local policing, crime mapping provides the visualization of crime clusters by types of crimes, thereby validating the street knowledge of patrol officers. Crime mapping can be used for allocating resources (patrol, specialized enforcement) and also to inform how the concerns of local citizens are being addressed. Mapping crime and its spatial distribution is of significant interest to a variety of stakeholders - it also serves as a relatable and understandable geographical phenomena for learning different types of spatial analysis techniques as well as many of the ‘nuances’ analysts face when using this type of ‘event’ data. As a result, within this practical, we are actually going to answer a very simple question: Does our perception of crime (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the ‘ecological fallacy’ mistake of assuming patterns at the ward level are the same at the borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. To test this, we will use these two administrative geographies (borough and ward) to aggregate crime data for London in 2020. Here we will be looking specifically at a specific type of crime: the theft from a person. Finding our datasets As we saw last week, accessing data within the UK, and specifically for London, is relatively straight-forward - you simply need to know which data portal contains the dataset you want! Crime Data For our crime data, we will use data directly from the Police Data Portal, which you can find at https://data.police.uk/. This Data Portal allows you to access and generate tabular data for crime recorded in the U.K. across different the different Police Forces since 2017. In total, there are 45 territorial police forces (TPF) and 3 special police forces (SPF) of the United Kingdom. Each TPF covers a specific area in the UK (e.g. the \"West Midlands Police Force), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a speciic area, we need to know which Police Force covers the Area of Interest (AOI) for our investigation. When you look to download crime data for London, for example, there are two territorial police forces working within the city and its greater metropolitan area: The Metropolitan Police Force (The Met), which covers nearly the entire London area, including Greater London The City of London (COL) Police, which covers the City of London. The Met has no juridiction in the COL. You therefore need to decide if you want to include an analysis of crime in the City of London or not - we will in our current study. We’ll get to download this dataset in a second! Population Data After what we’ve learnt about above, we know that if we want to study a phenomena like crime (and aggregate it to an areal unit as we will do today!), we will need to normalise this by our population. Luckily, we already have our Ward Population sorted from last week, with our ward_population_2019.shp that should be currently sitting in your final data folder. If it is not, you can download our shapefile here. Remember to unzip it and, for now, store it in your final data folder. In addition to our ward level dataset, we also want to generate the same type of shapefile for our London boroughs, i.e. a borough_ward_population_2019.shp, utilising the same approach as last week, joining our population table data to our borough shape data. To do this, we need to know where to get both our required datasets from - luckily, you’ve already got borough shape data in your raw/boundaires/2011 folder. Therefore, it is just a case of tracking down the same Mid-Year Estimates (MYE) for London Boroughs as we did for the wards, which with the ONS’s central MYE database, this also won’t be too difficult! So let’s get going! Download and process datasets As outlined above, to get going with our analysis, we need to download both the population data for our boroughs and the 2020 crime data for our two police forces in London. Let’s tackle the population data first. 1) Borough Population Through a quick search, we can find our Borough Population table data pretty much in the same place as our Ward data - however it is a separate spreadsheet to download. Navigate to the data here. Download the Mid-2019: April 2020 local authority district codes xls. Open the dataset in your spreadsheet editing software. Navigate to the MYE2-Persons tab. Utilising your preferred approach, extract: Code, Name, Geography and All ages data for all London boroughs. For me, the simplest way is to add a filter to row 5, and from this filter, in the Geography column select only London Boroughs: You should have a total of 33 boroughs. Once you have your 33 boroughs separated from the rest of the data, copy the columns (Code, Name, Geography and All ages) and respective data for each borough into a new csv. Remember to format the field names as well as the number field for the population as we did last week. Save as a new csv in your working population folder: borough_population_2019.csv. 2) Ward Population As mentioned above, you should have a ward_population_2019.shp file within your final data folder. As we’ll be using this dataset in our practical, we would like to make sure that we keep a version of this data in its current state, just in case we make a mistake whilst processing our dataset. As a result, we should create a copy of this dataset within our working folder, that we can use for this practical. Copy and paste over the entire ward_population_2019.shp from your final data folder to your working data folder. Don’t forget to copy over ALL the files. 3) Crime Data We will now head to the Police Data Portal and download our crime data… …or maybe not! As I said at the start of last week’s practical: We’re going to start cleaning (the majority of) our data from the get-go. However, with our crime data, the processing that is required from you right now is exhaustive to do manually - and far (far!) easy to do using programming. Essentially, all of our data that we will download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: For our data processing therefore, you would need to merge all of this crime into a single csv. Now you could do this manually by copying and pasting each csv into a new csv (24 times) - or you can do it through a few lines of code. However, you’ve already read through a lot today, so we’ll save learning Command Line tools for next week, where we’ll find out just how quick it can be to merge csvs! Instead, you can find the pre-merged and pre-filtered spreadsheet here. Note, I filtered the data to only contain data on theft crime, rather than all types of crime in London. There are however a few caveats in our crime data, that we’ll explain below - but these might not become clear until you start using the raw dataset yourself next week. For now, make sure you have downloaded the london_crime_theft_2020 csv linked here. Copy this csv into a new folder in your raw data folder called: crime. Downloading and using crime data from data.police.uk To download data for all of London for 2020, you follow these simple steps: As you can see, it is a simple process of selecting the Police Forces and months for which you want data for - and then a csv for each of these will be generated. 1) Data Structure Once downloaded, you can open up the csv to see what the data contains. Each crime csv contains at least 9 fields: Field(s) Meaning Reported by The force that provided the data about the crime. Falls within At present, also the force that provided the data about the crime. Longitude and Latitude The anonymised coordinates of the crime. LSOA code and LSOA name References to the Lower Layer Super Output Area that the anonymised point falls into, according to the LSOA boundaries provided by the Office for National Statistics. Crime type One of the crime types used to categorise the offence. Last outcome category A reference to whichever of the outcomes associated with the crime occurred most recently. Context A field provided for forces to provide additional human-readable data about individual crimes. For us, the main fields we are interested include: Longitude and Latitude (for plotting as points) LSOA code/name (for aggregating to these units without plotting) Crime Type (to filter crime based on our investigation) 2) Location Anonymisation When mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime — not the exact place that it happened. This displacement occurs to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household. Interestingly enough, the police also convert the data from their recorded BNG eastings and northings into WGS84 latitude and longitude ( hence why we’ll need to re-project our data in this practical). 3) Coding of Crimes into 14 Categories Each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. We can use these crime types to filter our crime specific to our investigation - in our case theft. Now we have all our data ready, let’s get mapping! Using Q-GIS to map our crime data If you do not have access to Q-GIS, please click here to go to the alternative option: Week 3 Practical Alternate: Using AGOL for Crime Mapping Start Q-GIS Click on Project –&gt; New. Save your project into your qgis folder as w3-crime-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. Compared to last week, you should now know what EPSG:27700 means! Shortcut to CRS on Q-GIS To access and set the project CRS quickly in Q-GIS, you can click on the small CRS button in the bottom-right corner in Q-GIS: Now we have our Project CRS set, we’re now ready to start loading and processing our data. Load Ward Population data Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your ward_population_2019.shp in your working folder. Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. Load Borough shape and population data and join! We now need to create our Borough population shapefile - and to do so, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Load the respective Borough dataset as a Vector Layer (found in your raw data folder -&gt; boundaries -&gt; 2011 -&gt; London_Borough_Excluding_MHW.shp). Load the respective Population dataset as a Delimited Text File Layer (Remember the settings, including no geometry! This one is found in your working folder) Join the two datasets together using the Join tool in the Borough dataset Properties box (remember which fields to use, which to add and to remove the prefix - look back at last week’s instructions if you need help). Export your joined dataset into a new dataset within your working folder: borough_population_2019.shp. Make sure this dataset is loaded into your Layers / Added to the map. Remove the original Borough and population data layers. Load and project our crime data We now are ready to load and map our crime data. We will load this data using the Delimited Text File Layer option you would have used just now to load the borough population - but this time, we’ll be adding point coordinates to map our crime data as points. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text File Layer. With File select as your source type, click on the small three dots button and navigate to your all_theft_2019.shp in your raw -&gt; crime folders. Click on the .csv file of this dataset and click Open. In *Record and Fields Options**, ensure it is set to CSV, untick Decimal separator is comma and tick First record has field names, Detect field types and Discard empty fields. In Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84, a.k.a. the GCS of lat and lon! Click Add. But WAIT! We are using the wrong CRS for our project?! Surely, we need everything to be in BNG? As you click Add, you should see that you get a pop-up from Q-GIS asking about transformations - we read about these earlier and they are the mathematical algorithms that convert data from one CRS to another. And this is exactly what Q-GIS is trying to do. Q-GIS knows that the Project CRS is BNG but the Layer you are trying to add has a WGS84 CRS. Q-GIS is asking you what transformation it should use to project the Layer in the Project CRS! This is because one key strength (but also problem!) of Q-GIS is that it can project \"on the fly - what this means is that Q-GIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use. But you must note that this transformation is only temporary in nature and as a result, it is not a full reprojection of our data. Map Projections in Q-GIS The following is taken from the Q-GIS’s user manual section on Working with projections. Every project in QGIS also has an associated Coordinate Reference System. The project CRS determines how data is projected from its underlying raw coordinates to the flat map rendered within your QGIS map canvas. By default, QGIS starts each new project using a global default projection. This default CRS is EPSG:4326 (also known as “WGS 84”), and it is a global latitude/longitude based reference system. This default CRS can be changed both permanently, for example, to British National Grid for all future projects, or for that specific project, as we have done in our two practicals. QGIS supports “on the fly” CRS transformation for both raster and vector data. This means that regardless of the underlying CRS of particular map layers in your project, they will always be automatically transformed into the common CRS defined for your project. Behind the scenes, QGIS transparently reprojects all layers contained within your project into the project’s CRS, so that they will all be rendered in the correct position with respect to each other! These reprojections are only temporary and are not permanently assigned to the dataset it is reprojecting - only to the project. As a result, we should be aware of this when using data across different projects and/or GIS systems and always remember what the data’s original or “true” CRS is! This reprojection is also using computer memory, therefore, if you are to analyse large datasets (such as our crime dataset), it makes sense to reproject our data to have it permanently in the same CRS as our project. For now, let’s use the on-the-fly projection for now and utilise Q-GIS’s recommendation of the +towgs84=446.448.... transformation. This transformation should be built-in to your Q-GIS transformation library, whereas some of the more accurate options would need installation. For now, given the displacement of our data in the first place, this transformation is accurate enough for us! Click to use the +towgs84=446.448.... transformation and click through the OKs to return to the main Q-GIS screen. You should now see your crime dataset displayed on the map: We can test the ‘temporary’ nature of the projection by looking at the CRS of the all_theft_2020 layer: Right-click on the all_theft_2020 layer then select Properties -&gt; Information and then look at the associated CRS. You should see that the CRS of the layer is still WGS84. Yup, Q-GIS is definitely projecting our data on-the-fly! We want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the same CRS as our administrative datasets, i.e. British National Grid. This also means we’ll have the dataset to use in other projects, just in case. Back in the main Q-GIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows: Input Layer: all_theft_2020 Target CRS: Project CRS: EPSG: 27700 Reprojected: Click on the three buttons and Save to File to create a new data file. Save it in your working folder as all_crime_2019_BNG.shp Click Run and then close the tool box. You should now see the new data layer added to your Layers. Q-GIS can be a little bit buggy so when it creates new data layers in your Layers box, it often automates the name, hence you might see your layer added as Reprojected. It does this with other management and analysis tools, so just something to be aware of! For now, let’s tidy up our map a little. Remove the all_theft_2020 original dataset. Rename the Reprojected dataset to all_theft_2020. Now we have an organised Layers and project, we’re ready to start our crime analysis! Counting Points-in-Polygons with Q-GIS The next step of our analysis is incrediby simple - as Q-GIS has an in-built tool for us to use. We will use the Count Points in Polygons in the Analysis toolset for Vector data to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Click on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons. Within the toolbox, select the parameters as follows: Polygons: borough_population_2019 Points: all_theft_2020 (Note how both our data layers state the same CRS!) No weight field or class field Count field names: crimecount Click on the three dot button and Save to file: working -&gt; borough_crime_2020.shp Click Run and Close the box. You should now see a Count layer added to your Layers box. Let’s go investigate. Click the checkbox next to all_theft_2020 to hide the crime points layer for now. Right-click on the Count layer and open the Attribute Table. You should now see a crimecount column next to your POP2019 column. You can look through the column to see the different levels of crime in the each borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software. Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. To get our crime rate statistic, we’re going to do our first bit of table manipulation in Q-GIS, woohoo! With the Attribute Table of your Count layer still open, click on the pencil icon at the start of the row. This pencil actually turns on the Editing mode in Q-GIS. The editing mode allows you to edit both the Attribute Table values and the geometry of your data. E.g. you could actually move the various vertex of your boroughs whilst in this Editing mode if you like! When it comes to the Attribute Table, it means you can directly edit existing values in the table or create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake. Using the Editing mode is the correct approach to editing your table, however, it might not always be the approach you use when generating new fields and, as we all are sometimes, a little lazy. (This may be a simple case of “Do what I say, not what I do!”) Let’s go ahead and add a new field to contain our Crime Rate. Whilst in the Editing mode, click on New Field button (or Ctrl+W/CMD+W) and fill in the Field Parameters as follows: Name: crime_rate Comment: leave blank Type: Decimal number Length: 10 Precision: 0 Click OK. You should now see a new field added to our Attribute Table. What did all this mean? Understanding how to add new fields and their parameters rely on you understanding the different data types we covered last week - and thinking through what sort of data type your field needs to contain. In our case, we will store our data as a decimal to enable our final calculation to produce a decimal (an integer/integer is likely to produce a decimal) but we will set the precision to 0 to have zero places after our decimal place when the data is used. That’s because ultimately, we want our crime rate represented as an integer because, realistically, you can’t have half a crime! Calculating a decimal however will allow us to round-up within our calculations. The empty field has NULL populated for each row - so we need to find a away to give our boroughs some crime rate data. To do this, we will calculate a simple Crime Rate using the Field Calculator tool provided by Q-GIS within the Attribute Table. We will create a crime rate that details the number of crimes per 10,000 people in the borough. In most cases, a crime rate per person will create a decimal result less than 1 which not only will not be stored correctly by our crime_rate field but, for many people, a decimal value is hard to interpret and understand (yes, I know, but we are aiming to make maps that are accessible to everyone…). Therefore going for a 10,000 person approach allows us to calculate and represnt the crime rate using full integers for both our borough and ward scales as we’ll see later. This calculation was determined by a bit of a trial and error by me within this practical, so it is something you’d need to consider and change for future research you might do! Whilst still in the editing mode, click on the Abacus button (Ctrl + I / Cmd + I), which is actaully the Field Calculator. A new pop-up should load up. We can see there are various options we could click at the top - including Create a new field. Ah! So we could in fact create a new field directly from the field calculator which would help us combine these two steps in one and quicken our workflow! For now, in the Field Calculator pop-up: Check the Update existing field box. Use the drop-down to select the crime_ratefield. In the Expression editor, add the following expression: ( “crimecount” / “POP2019” ) * 10000 You can type this in manually or use the Fields and Values selector in the box in the middle to add the fields into the editor. Once done, click OK. You should then return to the Attribute Table and see our newly populated crime_rate field - at the moment, we can see the resulting calculations stored as decimals. Click on the Save button to save these edits - you’ll see the numbers turn to integers. Click again on the Pencil button to exit Editing mode. We now have a crime_rate column to map! Before moving to the next step, if you would like, go ahead and symbolise your boroughs by this crime_rate. Tips for Symbolisation When in the Symbology tab and after selecting Graduated as your symbolisation option, click on the histogram tab and load the values to see the distribution of your data. You can also edit the lines of the borough to a colour of your choice. You should also make sure your new borough crime rate layer has been renamed from the default Count layer name Q-GIS has given it. Rename your borough crime rate layer has been renamed from the default Count layer name to Borough Crime Rate. Great! We now have our Borough crime rate dataset ready for mapping and analysis - we just now need to repeat this process to have our Ward dataset. Repeat the above processes to create a crime_rate column within our Ward dataset ready for mapping and analysis. Tips for Repetition in Q-GIS Remember, you can use the field calculator straight away to shorten the field creation process by selecting to create a new field whilst completing the field calculation (still using the same parameters though!). One additional small tip is that in the middle box in the Field Calculator, you can load Recent field calculations and double-click on your prior calculation to automate the creation of the crime_rate calcuation! Now you have both datasets ready, it’s time to style the maps! Remember to use the Properties box to first symbolise your maps. Think through using the appropriate colour scheme - and perhaps have a look online for some examples, if you don’t want to use the defaults! Once you’re happy with their symbolisation, we’ll turn them into proper publishable maps using Q-GIS’s Print Layout. Making our Crime Rate Maps for analysis in Q-GIS and the Print Layout To create proper publishable maps in Q-GIS, we use what Q-GIS calls its Print Layout window (formely Map Composer). If you’ve ever used ArcMap, this is similar to switch the view of your map canvas to a print layout within the main window - but in Q-GIS’s case, it loads up a new window. From the main Q-GIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime_map_borough_ward A new window should appear. Oh great, another tool I need to learn how to use… well yes, but learning how to use the Print Layout window will help you make great maps. A short introduction to the Print Layout As the Q-GIS documentation on Print Layout explains: The print layout provides growing layout and printing capabilities. It allows you to add elements such as the QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. You can size, group, align, position and rotate each element and adjust their properties to create your layout. The layout can be printed or exported to image formats, PostScript, PDF or to SVG. Initially, when opening the print layout provides you with a blank canvas that represents the paper surface when using the print option. On the left-hand side of the window, you will find buttons beside the canvas to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item. On the right-hand side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. For our practical today, we’re most interested in the bottom panel as Layout will control the overall look of our map, whilst Item properties will allow us to customise the elements, such as Title or Legend, that we may add to our map. In the bottom part of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level, the number of selected items if applicable and, in the case of atlas generation, the number of features. In the upper part of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar. Getting started with creating our map Working with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. To start with creating a map, you use the Add Map tool to draw a box in which a snapshot of the current active map you have displayed in your Q-GIS main window will be loaded. Let’s try this now: Click on the Add Map tool and draw a box in the first half of our map to load our current map, in my case, Ward Crime Rate: Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc. As you can see, the map currently does not look that great - we could really do with zooming in, as we do not need all of the white space. With your map selected, head to the Items Properties panel and look for the Scale parameter. Here we can manually edit the scale of our map to find the right zoom level. Have a go at entering different values and see what level you think suits the size of your map. Keep a note of the scale, as we’ll need this for the second map we’ll add to our map layout - our borough map. Next, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map. In the same panel, note down the size of your map - we want to make sure the next map we add is of the same size. Note, if you need to move the position of the map within the box, look for the Move Item Content tool on the left-hand side toolbar. Once you are done, finally click on the Lock Layers and Lock Style for layers. By locking the Layers (and their symbology) in our map, it means we can change our data/map in our main Q-GIS window without changing the map in the Print Layout - as we’ll see in a minute when adding our Borough Crime Rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main Q-GIS window. Now we’ve added our first map to our Map Layout, we want to add a Legend for this specific map. Click on the Add Legend tool and again, draw a box on your map in which your legend will appear. As you’ll see, your Legend auto-generates an entry for every layer in our Layers box in the main Q-GIS application: In Item Properties, uncheck auto-update - this stops Q-GIS automatically populating your legend as it has done current, and enables you to customise your legend. First, let’s rename our Layer in the legend to: Ward Crime Rate (per 10,000 people). Next, we want to remove all othe Layers, using the - button We can also customise the Legend further, including type, size and alignment of font - go ahead and style your legend as you would prefer. Move the Legend to an appropriate part of the layout near your Ward Crime Rate map - resize if necessary. Now we are finished with the Ward map, we want to make sure we don’t change any aspect of its layout. To do so, we need to lock both the Map and Legend in the Items panel - this prevents us accidently moving items in our layout. Note, this is different to locking your layers in the Items Properties as we did earlier. In the Items panel, click the Lock checkbox for both our map and legend. Once locked, we can now start to add our Borough map. In the main Q-GIS window, uncheck your Ward_crime_rate layer and make sure your Borough_crime_rate layer is now visble. Return to the Print Layout window. Repeat the process above of adding a map to the window - this time, you should now see your Borough map loaded in the box (and you should see no changes to your Ward map). Place your Borough map next to your Ward map - use the snap grids to help. Set your Borough map to the same zoom level as your Ward map. Make sure your Borough map is the same size as your Ward map. Set your Borough to the same extent as your Ward map (extra neatness!). Add a frame if you want. Lock your layer and its symbology in the Items Properties once ready and the lock your layer in the Items panel. We now just need to add a second legend for our Borough map. If we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there is a difference in the values, we need to have two legends. The Extension Activity to this practical, on the other hand, may result in a single legend - as a clue to help complete the activity! Repeat the process as above to add a Legend for our Borough map. Remember to re-title the Legend to make it more legible/informative. Match the same formatting for a clean look. Once complete, lock these two items in the Items panel as well. Locking/Unlocking and Visibility If you hadn’t noticed in the Items panel, you have the ability not only to lock/unlock different items (unlock to edit any items again), but also turn on/off the visibility of your layers: Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on the left-hand tool bar: Add a scale bar: use the Item Properties to adjust the Style, number of segments, font etc.. Add a north arrow: draw a box to generate the arrow and then use the Item Properties to adjust. I typically place the two side by side - and you can select both, right-click and group so you can then treat them as a single item when moving them around the page. Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Once you have added these properties in, you should have something that looks a little like this: Export map We are finally ready to export our map! To export your map to a file go: Layout -&gt; Export as Image and then save in your maps folder as London_2020_Crime_Rate.png. You can also export your map as a PDF. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in Q-GIS. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. This is all possible using the field calculator in Q-GIS, but will require some thinking about the right expression. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "],["programming-for-data-analysis-using-r-and-r-studio.html", "4 Programming (for Data Analysis) using R and R-Studio", " 4 Programming (for Data Analysis) using R and R-Studio Welcome to Week 4 in Geocomputation! Well done on making it through Week 3 - and welcome to our introduction to using programming, in the form of R and R-Studio, for data analysis. This week is heavily practical oriented - with many aspects of your practical integrated at various points in the workshop - as well as, of course, a data analysis section towards the end. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. Week 4 in Geocomp Video on Stream This week’s content introduces you to the foundational concepts associated with Programming for Data Analysis, where we have three areas of work to focus on: General principles of programming How to use R and R-Studio effectively for programmatical data analysis The ‘Tidyverse’ philosophy This week’s content is split into 4 parts: An Introduction to Programming (40 minutes) Using R and R-Studio for Data Analysis (60 minutes) The Tidyverse Philosophy and Principles (40 minutes) Practical 3: Analysing Crime in 2020 in London (30 minutes) This week, we have a slightly different approach to our workflow structure, with a mixture of short lectures, instructional videos and activities to complete throughout each part. A single Key Reading is found towards the end of the workshop. This week, you have 1 assignment, which will be highlighted in the workbook. Part 4 is the main part of analysis for our Practical for this week, but you will find aspects of programming in Parts 1-3 that you will need to do in order to prepare our data for the final part. If you have been unable to download R-Studio Desktop or cannot access it via Desktop@UCL Anywhere, you will have access to our R-Studio Server website instead. Instructions on how to access this are provided below. Learning Objectives By the end of this week, you should be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R and produce a bar chart We will build on the data analysis we completed last week and look to further understand crime in London by looking at its prevalence on a month-by-month basis. An Introduction to Programming Programming is our most fundamental way of interacting with a computer - it was how computers were first built and operated - and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking - and not typing. However, programming and computer code underpin every single application that we use on our computers… or really any technological device. After all, programming is used for so many purposes and applications, that, we as users take for granted - from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages (and ways of using programming languages!) have developed - Wikipedia, for example, has a list of 50 different types of languages, although there is some overlap between many of these and some are used for incredibly niche activties. In general, the main programming languages that people focus on learning at the moment include: Top 10 programming languages and their applications according to DZone in 2017. Some can be used for a range of purposes – others are more specific, e.g. HTML for website building. There are also different ways in which programming languages work, which give some advantages over others. This is to do with how the code is written and ‘talks to the computer’ - behind our main programming languages there is something called a compiler that takes the code we write in our various programming languages and translates it into machine code, a.k.a. the code our computers know how to understand. This code is written purely in binary and, as a result, looks a lot different to the code we’ll be writing in our practicals (think a lot of 1s and 0s!). For some languages, this translation is completed when your code is compiled before it is run, i.e. the ‘compiler’ will look through all your code, translate to machine code and then execute the machine code according. These languages are known as compiled, or low-level languages and can, at first, be slow to write but are incredibly efficient when executing huge amounts of code (e.g. when creating software). They however require an understanding of things called registers, memory addresses, and call stacks and are, as a result, a lot more complicated to learn and use (and no, I personally do not know how to code in any low-level languages…nor do I particularly want to!). For other languages, such as R and Python, these fall into the interpreted language category. Here, each line of code is executed without a pre-runtime translation. In this case, a program called an interpreter reads each program statement and, following the program flow, then decides what to do, and does it. The issue with these high-level programming languages is that this approach can be costly in computational resources (i.e. processing time and memory space). As there is no pre-run time compilation, bugs are not found before the code is run but instead as the code is run - as a result (and what you might see happen in your own code), your computer can get stuck trying to execute code which is either completely unfeasible for your computer to execute (e.g. your computer cannot handle the size of data you are feeding it) or it ends up in a loop with no way out - except for you stopping the code. However, the advantage of using these languages is that their main focus is on things like variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts - all of which we’ll use within our module, believe it or not! These languages have a primary focus on usability over optimal program efficiency, which, when we’re just learning to code, are ideal for us in Geocomputation! Don’t worry if you don’t understand what any of this means, you will do by the end of this module! As we’re not taking a Computer Science degree here, we won’t go into any more detail about this, but suffice to say, there is a lot more to programming then what we’ll cover in Geocomputation. But what is important to recognise is that a lot of work went into creating the programming environments that we get to use today - and I, for one, am extremely glad I never had to learn how to write a compiler! If understanding a little more about compilers and machine code is of interest to you, the below video provides an accessible explanation - although you might want to come back to it at the end of the practical: How do computers read code? The Command Line Interface The most basic of programming we can use without installing anything on our computers is using the Command Line Interface(CLI) already built in, known as the shell. The shell is a simple program that lets you interact with your computer using text commands (Command Line Interface) instead of a point &amp; click Graphical User Interface (GLI). The shell simply takes your commands and provides them to the operating system of your computer. Each operating system has its own shell program: Mac / Linux = zsh (new) / bash (Bourne-again Shell) (previous) Microsoft = PowerShell (and a few others). For most operating systems, you can access the shell using a window-based program, known as a terminal emulator (TE). The default TE for Mac &amp; Linux users is: Terminal. The default TE for Windows users use Command Prompt (old) or Terminal (new). (If you remember my introductory lecture, this is how I used to have to interact with my first computer at a very young age of probably 5 when attempting to load a computer game!) The shell is an incredibly useful program - and if you take programming further in the future (e.g. analysis of big datsets through servers, running multiple scripts, dealing with version control, software engineering), it will become a tool that you’ll become incredibly familiar with. But for now, we just want to illustrate how cool it can be for us to be able to tell our computer to do things in a few lines of code - rather than having to click and point - particularly once you know how to use the CLI-shell and can remember specific commands. Let’s take a look by completing a simple task with our shell to tell our computer to do something - let’s do some file management for our practical today. Using the Command Line Interface On Your Computer As you may remember from last week’s practical, I provided you with our crime data in a single processed csv file. However, when you download the data from data.police.uk, the data is not so nicely formatted! Essentially, all of our data that we download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: To be able process this data easily in R, we want to move all of our files into a single folder. And believe it or not, it only takes a couple of lines of code to do so. Let’s take a look: Using the Command Line to copy and move files. Video on Stream As we’ve seen in the video above, it can really useful - and quick - to use our Shell to organise our data files prior to loading them in R-Studio. Just to proove this, the first piece of programming we will do today is use your built-in shell on your computer to repeat the same process and copy our crime data into a single folder. File Management using the Command Line Now you’ve watched how I copy the files over, let’s go ahead and do this ourselves. Head to data.police.uk and download all available crime data for London in 2020 (this may only include up until November) and for both Police Forces. The below video shows you how to do this. You should now have your data downloaded in a single main folder in your downloads (it may be given a ridiculously long coded filename). Copy this folder into your GEOG0030 -&gt; data -&gt; raw -&gt; crime folder using copy and paste / drag-drop (whatever your preference). Next, open up your shell on your computer. On mac, hold CMD and hit space to open up your search bar and type terminal, and a terminal window should appear. On windows, press the windows button and do the same. Alternatively search for shell or command prompt. With your shell open, we will navigate to your raw data folder and copy over the crime data into a single folder. To do so, we’ll use six main terminal commands: Common Shell/Terminal Commands pwd (both Mac and Windows): Print Working Directory - this will show us where we are in our computer’s file system. By default, you should be located at the “base” of your user file. dir (Windows) or ls (Mac): Lists all files in the directory you are currently located. cd (both Mac and Windows): Change into a new directory (i.e. folder) - this can be a single path ‘step’ or several steps to get to the directory you want. md (Windows) or mkdir (both Mac and Windows): Make a new directory (i.e. folder) where you are currently located. cp (both Mac and Windows): Copy content of a directory or specific file to a new location. This command can take wildcards to help search multiple folders at once, as we’ll see in our query. move (Windows) or mv (Mac): Move directory to a new destination. Let’s get going. In your shell, type the command pwd and press return. You should now see a file path appear under your command - the computer is telling you where you are currently located in your computer system. Next, type dir OR ls (OS-dependent) and press return. The computer now lists all of the folders in your directory that you can move into as a next step from your current folder. You now need to identify what folder your GEOG0030 work is contained in - and what your “path” is to get there as we now need to continue changing directories to get to our GEOG0030 folder. Next, type cd followed by the folder(s) you need to change into in order to get to your main GEOG0030 folder. Remember to press return to execute your code. In my case, the command is: cd Code. Each time you change folder, this folder is added to the file path next to your Prompt - have a quick look at this now. Keep going changing folders until you are in the folder that contains your downloaded crime data. Auto-Population of File Paths A tip is that your terminal can auto-populate your folder names for you when there is enough information for them to determine the unique folder. To do this, press tab on your keyboard. E.g. your crime folder is likely to be a long list of numbers and letters if you haven’t renamed it whilst copying it over to your raw folder. Therefore I recommend using this approach will save you time entering all these numbers. You can also change into this folder in one command, simply keep adding to your folder path as so: Type in (and use tab) cd GEOG0030/data/raw/crime/52c6b758bceaf2244fc1b6f93e85d7f00f234ccf/ and then press return. Note, if you are using a WINDOWS PC, you need to use a backslash (\\) in your file path, not a forward-slash. Note, you do not need a slash at the start of your file paths.* Once you are in the correct folder, we first want to make a new folder to contain all the crime csvs (without their current subfolder system): Type in mkdir all_crime and press return. If you now type dir or ls, you should now see the new folder listed within your current folder. Let’s go ahead and copy all of our csvs into this single folder. Type cp **/*.csv all_crime and press return. Again use a backslash ()) if on a Windows PC. This command uses the wildcard * to search for any file in any folder, as long as it has a .csv file type. Using wildcards is a very common programming tool and we are likely to use them in our practicals moving forward. You can also use them in searches on search engines such as Google! We can now change into our new folder and then list its contents to check that our files have moved across. Type cd all_crime and press return. Then type dir or ls. Check that you have all your files (either 22 or 24, depending on when you are completing this practical!). Great, we have our files all in a single folder which will make using them in R much easier. We’ll do one final thing - and that is move this folder out of this original and into the main crime folder. Still in the terminal, type: cd .. to take a step back into our police crime data folder. Next, to move our all_crime folder: (WINDOWS) move all_crime .. or (MAC): mv all_crime .. Finally, type cd .. and press return. You should now find yourself one step back in your file system in the main crime folder. We can check that our move worked by again listing the contents of the folder. Type dir or ls and press return. Check that your all_crime folder is now listed. ::: Great! You’ve just done your first bit of command line programming! Think about how quick was it to type those commands and get the files moved - rather than have to do all of that by hand. Of course it helped that I told you what commands to write - but the more time you spend with programming, the quicker (and more familiar) you will get at (with) coding and executing these commands. Command Line Paths Note, the use of .. in our two commands above means to take a step back in your file system path, as we did in both of these cases here (i.e. the ‘parent’ folder. In addition, two further commands to be aware of include: ~ (Mac) to denote the root or home directory, e.g. cd ~. In Windows, there is not really a shortcut for this. .: a single full-stop means “this folder”. The command line is just one aspect of programming - but we also want to have the ability to create and run scripts. Scripts are incredibily important for us when completing data anlaysis and, as such, we’ll look at the differences between the two as we start to use R/R-Studio for our data analysis today. One cool thing about the terminal is that we actually have the ability to create and run scripts just within the terminal itself. We can do this by opening up a text editor in our terminal to write a script in any programming language and then execute our script within the terminal. We execute our script by actually setting the terminal to that programming language and then calling the script. This all sounds extremely complicated - but it really is not once you’ve spent a bit of time working with the CLI. We can have a quick look here: Using the Command Line to create and run scripts (in Python). Video on Stream Whilst we could use this type of approach for data analysis for conducting actual spatial analysis, we won’t be doing so in Geocomptuation (you are probably happy to read!). This is because, quite frankly, the terminal is pretty limited in its display of maps and charts, a key output of our work here in Geocomputation, and general user-friendly functionality. In fact, we’d need to save our outputs to a file each time to go view them, which would end up being a pretty clunky workflow… Instead, what’s great is that we have several different types of software and Integrated Develoement Environments that bring the functionality of running scripts together with the visualisation capacity we like in our GIS software. For us in Geocomputation, our tool of choice for this is R-Studio. Using R and R-Studio for Data Analysis Before we go any further, what I want to make clear from this workshop - and the remainder of the module - is that programming using R and R-Studio is ultimately a tool we will use to complete specific tasks we need to do for our data analysis. There are a lot of different tools out there that you can use to achieve the same outcomes (as you’ve seen with Q-GIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools - more on this next week. With this tool though, there is a lot to learn about the principles and the theory behind programming languages. As evident above, whilst we could look at this in a lot of detail (there is a lot of theory behind programming which we just won’t cover - that’s for computer scientists), we will instead focus on the aspects most important to our use, which is covered in our main lecture video below: Principles of Programming for Data Analysis in the Programming for Data Analysis section. The second thing to make clear is that R and R-Studio are two different things: R is our programming language, which we need to understand in terms of general principles, syntax and structure. R-Studio is our Integrated Development Environment, which we need to understand in terms of functionality and workflow. Integrated Development Environment An Integrated Development Environment (IDE) is simply a complicated way of saying “a place where I write and build scripts and execute my code”. Nowadays, we have some really fancy IDEs that, when they know what language you are coding in, will highlight different types of code according to what they represent (e.g. a variable, a function) as well as try to proof-read/de-bug your code “on-the-fly” before you’ve even run it. R-Studio is definitely a very fancy IDE - as it offers a lot of functionality beyond just writing scripts and execute code as we’ll see over the coming weeks. As you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the R-Studio IDE (or really software now!) to execute this statisitcal programming. Together, R and R-Studio has grown into an incredibly success partnership of analytical programming language and analysis software - and is widely used for academic research as well as in the commercial sector. One of R’s great strength is that it is open-source, can be used on all major computer operating systems and is free for anyone to use. It, as a result, has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis. Believe it or not, the entire workbook you are reading right now has been created in R-Studio, utilising a mixture of programming languages, including R, HTML, CSS and Markdown. R-Studio has the flexibility to understand programming languages other than R (including Python!), whilst R can be deployed outside of the R-Studio environment in standalone scripts and other IDEs. However, for us, the partnership between R and R-Studio works pretty well for what we want to achieve - so this is what we’ll be using for the remainder of the Geocomputation module. How do I use R-Studio? Unlike traditional statistical analysis programmes you may have used such as Microsoft Excel or even SPSS, within the R-Studio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script (or similar document, more on this in Week 10), that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks. We can also use the R Console to execute simple instructions that do not need repeating - such as installing libraries or quickly viewing data (we’ll get to this in a second). In addition, R, its various graphic-oriented “packages” and R-Studio are capable of making graphs, charts and maps through just a few lines of code (you might notice a Plots window to your right in your R-Studio window) - which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted. Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and R-Studio. These include: It is broadly intuitive with a strong focus on publishable-quality graphics. It is ‘intelligent’ and offers in-built good practice – it tends to stick to statistical conventions and present data in sensible ways. It is free, cross-platform, customisable and extendable with a whole swathe of packages/libraries (‘add ons’) including those for discrete choice, multilevel and longitudinal regression, and mapping, spatial statistics, spatial regression, and geostatistics. It is well respected and used at the world’s largest technology companies (including Google, Microsoft and Facebook, and at hundreds of other companies). It offers a transferable skill that shows to potential employers experience both of statistics and of computing. The intention of the practical elements of this week is to provide a thorough introduction to R-Studio to get you started: 1. The basic programming principles behind R. 2. Loading in data from csv files, filtering and subsetting it into smaller chunks and joining them together. 3. Calculating a number of statistics for data exploration and checking. 4. Creating basic and more complex plots in order to visualise the distributions values within a dataset. What you should remember is that R/R-Studio has a steep learning curve, but the benefits of using it are well worth the effort. I highly recommend you take your time and think through every piece of code you type in - and also remember to comment your code (we’ll get to this in a bit!) . The best way to learn R is to take the basic code provided in tutorials and experiment with changing parameters - such as the colour of points in a graph - to really get ‘under the hood’ of the software. Take lots of notes as you go along and if you are getting really frustrated take a break! This week, we focus solely on using R and R-Studio (from now on, this may be simply denoted as R) for basic statistical data analysis. Next week, we will introduce using R for spatial (data) analysis - but there’s lots to get on with today to understand the fundamental principles of using R (and programming in general). Accessing R-Studio for Geocomputation You have two options for using R-Studio in this module. Using R-Studio Desktop: You should have installed this in Week 1 as per the software installation instructions. Using R-Studio Server: First sign in to the UCL VPN or UCL China Connect. To use R-Studio Server, open a web browser and navigate to: https://rstudio.data-science.rc.ucl.ac.uk/ Log in with your usual UCL username and password. You should see the RStudio interface appear. If it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the start a new session button. You can use either approach - but do recognise their may be some differences oh how our code appears. The code below has been created on an R-Studio Desktop Version 1.2.5033 and tested on the R-Studio Server. Note RStudio server will only work with an active VPN connection that links your personal computer into UCL’s network. Students in mainland China may want to use UCL China Connect. Students that use a Mac computer that is running on the latest version of MacOS (MacOS Big Sur), are advised to use Desktop@UCL as the Cisco AnyConnect VPN application may not work. If you are completely unable to access the server (e.g. your browser displays a This site can’t be reached message), it means that your VPN connection is not working correctly. Please ensure that your VPN is working correctly or use Desktop@UCL Anywhere instead. An Introduction to R-Studio and its interface Let’s go ahead and open R-Studio (Desktop or Server) and we’ll first take a quick tour of the various components of the R-Studio environment interface and how and when to use them: Introducing the R-Studio Interface Video on Stream As you’ve heard, R-Studio has various windows that you use for different purposes - and you can customise its layout dependent on your preference. When you first open R-Studio, it should look a little something like this: The main windows (panel/pane) to keep focused on for now are: Console: where we write “one-off” code, such as installing libraries/packages, as well as running quick views or plots of our data. Files: where our files are stored on our computer system - can help with checking file paths as well as file names, and general file management. Environment: where our variables are recorded - we can find out a lot about our variables by looking at the Environment window, including data structure, data type(s) and the fields and ‘attributes’ of our variables. Plots: the outputs of our graphs, charts and maps are shown here. Help: where you an search for help, e.g. by typing in a function to find out its parameters. You may also have your Script Window open, which is where we build up and write code, to a) keep a record of our work, b) enable us to repeat and re-run code again, often with different parameters. We will not use this window until we get to the final practical instructions. We’ll see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as ‘attributes’ (do not get confused here with the Attribute Table for Q-GIS) and data structures. Programming for Data Analysis Before we get started with using R-Studio, we first need to take a few steps back and address the bigger learning curve in the room, that is: How do I program?. As stated earlier, R/R-Studio is just a tool - but to use it, you need to understand how to write code in R effectively and, of course, accurately to get your analysis to work. This means we need to learn about and understand: Basic Syntax Data Structures and Types Functions and Libraries/Packages Object-Oriented Programming Here, we provide a short introduction to the basic principles of programming, with a focus on Object Oriented Programming. This is a video you might want to re-watch after completing today’s practical. Principles of Programming for Data Analysis Slides | Video on Stream In the above lecture, you heard about the different including: Syntax using variables and functions Importance of data types and data structures The role of packages/libraries in expanding R’s functionality And a brief introduction to Object-Oriented Programin (OOP) We can put some of these principles into action by testing some of R-Studio’s capability with some short pieces of coding now. Using the Console in R-Studio We’ll first start off with using R-Studio’s console to test out some of R’s in-built functionality by creating a few variables as well as a dummy dataset that we’ll be able to analyse - and to get familiar with writing code. Note, you might need to click on the console window to get it to expand - you can then drag it to take up a larger space in your R-Studio window. The video below provides an overview of the short tutorial with additional explanations, so if you’re already a bit stuck, you can watch this as you start to complete the following instructions. Using the Console in R-Studio for programming Video on Stream In your R-Studio console, you should see a prompt sign - &gt; to the left - this means we’re ready to start writing code (a bit like earlier in the shell). Error Warnings in the Console Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you see a &gt; on the left it means you can type in your next line, a + means that you haven’t finished the previous line of code. As will become clear, + signs often appear if you don’t close brackets or you did not properly finish your command in a way that R expected. In your console, let’s go ahead and conduct some quick maths - at their most basic, all programming langauges can work like calculators! Command Input Type in 10 * 12 into the console. # Conduct some maths 10 * 12 ## [1] 120 Once you press return, you should see the answer of 120 returned below. Great, you’ve now learnt how to enter code into the R-Studio console! Pretty similar to your computer’s CLI right?! Storing Variables But rather than use ‘raw’ or ‘standalone’ numbers and values, we primarily want to use variables that stores these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called creating an object and this object becomes stored as a variable. We do this by using the &lt;- symbol is used to assign the value to the variable name you have given. Let’s go ahead and try this. Let’s create two variables for experimenting with: 2.Type in ten &lt;- 10 into the console and execute. # Store our ten variable ten &lt;- 10 You have just created your first variable. You will see nothing is returned in the console - but if you check your Environment window, it has now appeared as a new variable that contains the associated value. Type in twelve &lt;- 12 into the console and execute. # Store our ten variable twelve &lt;- 12 Once again, you’ll see nothing returned to the console but do check your Environment window for your variable. We’ve now stored two numbers into our environment - and given them pretty good variable names for easy reference. R stores these objects as variables in your computer’s RAM so they can be processed quickly. Without saving your environment (we will come onto this below), these variables would be lost if you close R (or it crashes). Now we have our variables, let’s go ahead and do the same simple multiplication maths: Type in ten * twelve into the console and execute. # Conduct some maths again using our variables ten * twelve ## [1] 120 You should see the output in the console of 120 (of course..!). Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these variables can be treated in the same way as the values they contain. Next, type in ten * twelve * 8 into the console and execute. # Conduct some more maths with variables and raw values ten * twelve * 8 ## [1] 960 You should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable. Type output &lt;- ten * twelve * 8 into the console and execute. # Conduct some maths and store it as output output &lt;- ten * twelve * 8 As we’re storing the output of our maths to a new variable, the answer won’t be returned to the screen. Accessing and returning variables We can ask our computer to return this output by simply typing it into the console. Ask the computer to return the variable output. You should see we get the same value as the earlier equation. # Return the variable, output output ## [1] 960 Variables of different data types We can also store variables of different data types, not just numbers but text as well. Type in str_variable &lt;- \"This is our first string variable\" into the console and execute. # Store a variable str_variable &lt;- &quot;This is our 1st string variable&quot; We have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores “words” (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of the \" \". Let’s access our variable to see what is now stored by our computer. Type in str_variable into the console and execute. # Return our str_variable str_variable ## [1] &quot;This is our 1st string variable&quot; You should see our entire sentence returned - and enclosed in \"\". Again, by simply entering our variable into the console, we have asked R to return our variable to us. Calling functions on our variables We can also call a function on our variable. This use of call is a very specific programming term and generally what you use to say \"use\" a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to print our variable, which will give us the same output as accessing it directly via the console: Type in print(str_variable) into the console and execute. # Print str_variable to the screen print(str_variable) ## [1] &quot;This is our 1st string variable&quot; We have just used our first function: print(). This function actively finds the variable and then returns this to our screen. You can type ?print into the console to find out more about the print() function. # Gain access to the documentation for our print function ?print This can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output. In many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one argument. Returning functions When a function provides an output, such as this, it is known as returning. Not all functions will return an output to your screen - they’ll simply just do what you access them to do, so often we’ll use a print() statement or another type of returning function to check whether the function was successful or not - more on this later in the workshop. Examining our variables using functions Within the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is: Type in typeof(str_variable) into the console and execute. # Call the typeof() function on str_variable to return the data type of our variable. typeof(str_variable) ## [1] &quot;character&quot; You should see the answer: \"character\". As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables. Type in typeof(ten) into the console and execute. # Call the typeof() function on ten variable to return the data type of our variable. typeof(ten) ## [1] &quot;double&quot; You should see the answer: \"double\". As evident, our ten is a double data type. For high-level objects that involve (more complicated) data structures, such as when we load a csv into R as a data frame, we are also able to check what class our object is, as follows: Type in class(str_variable) into the console and execute. # Call the class() function on str_variable to return the object of our variable. class(str_variable) ## [1] &quot;character&quot; In this case, you’ll get the same answer - “character” - because, in R, both its class and type are the same: a character. In other programming languages, you might have had \"string\" returned instead, but this effectively means the same thing. Let’s try testing our ten variable: Type in class(ten) into the console and execute. # Call the class() function on ten to return the object of our variable. class(ten) ## [1] &quot;numeric&quot; In this case, you’ll get a different answer - \"numeric\" - because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our ten variable is an integer or not. Type in is.integer(ten) into the console and execute. # Test our ten variable by asking if it is an integer is.integer(ten) ## [1] FALSE You should see we get the answer FALSE - as we know from our earlier typeof() function, our variable ten is stored as a double and therefore cannot be an integer. Whilst knowing this might not seem important now, but when it comes to our data analysis, the difference of a decimal number vs. a whole number can quite easily add bugs into our code! We can incorporate these tests into our code when we need to evaluate an output of a process and do some quality assurance testing of our data analysis. We can also ask how long our variable is - in this case, we’ll find out how many different sets of characters (strings) are stored in our variable, str_variable. Type in length(str_variable) into the console and execute. # Call the length() function on str_variable to return the length of our variable. length(str_variable) ## [1] 1 You should get the answer 1 - as we only have one set of characters. We can also ask how long each set of characters is within our variable, i.e. ask how long the string contained by our variable is. Type in nchar(str_variable) into the console and execute. # Call the nchar() function on str_variable to return the length of each of our elements within our variable. nchar(str_variable) ## [1] 31 You should get an answer of 31. Creating a two-element object Let’s go ahead and test these two functions a little further by creating a new variable to store two string sets within our object, i.e. our variable will hold two elements. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute. # Store a new variable with two items using the c() function two_str_variable &lt;- c(&quot;This is our second string variable&quot;, &quot;It has two parts to it&quot;) In this piece of code, we’ve created a new variable using the c function in R, that stands for \"combine values into a vector or list. We’ve provided that function with two sets of strings, using a comma to separate our two strings - all contained within the function’s (). You should now see a new variable in your Environment window which tells us it’s a) chr: characters, b) contains 2 items, and c) lists those items. Let’s now try both our length() and nchar() on our new variable and see what the results are. # Call the length() function and nchar() function on our new variable length(two_str_variable) ## [1] 2 nchar(two_str_variable) ## [1] 34 22 Did you see a difference? You should have seen that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22. There is one final function that we often want to use with our variables when we are first exploring them, which is attributes() - as our variables are very simple, they currently do not have any attributes (you are welcome to type in the code and try) but it is a really useful function, which we’ll come across later on. # Call the attributes() function on our new variable attributes(two_str_variable) ## NULL We’ve had fun experimenting with simple variables in our console - and learnt about many new functions we can use with our code. In fact, we’ve learnt 7 functions - can you name/remember them all without scrolling up? If you can’t, I highly recommend taking notes on each of the functions - even if it is just a short list of the functions and what they do. We’re now going to move on to creating and analysing our dummy dataset - so fingers crossed you’ll remember these as we move forward. Using comments in our code In addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code I have written, I have provided an additional comment to explain what the code does. Comments are denoted using the hash symbol #. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code - as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments as you code and not leave it to do retrospectively - because I can tell you from experience - you most certainly will not. Whilst we are using the console, using comments is not necessary - but as we start to build up a script in our full practical, you’ll find them essential to help understand your workflow in the future! Analysing dummy data in R-Studio using the Console The objects we created and played with above are very simple: we have stored either simple strings or numeric values - but the real power of R comes when we can begin to execute functions on more complex objects. As we heard in our lecture, R accepts four main types of data structures: vectors, matrices, data frames, and lists. So far, we have dabbled with a single item or a dual item vector - for the latter, we used the c() function to allow us to combine our two strings together within a single vector. We can use this same function to create and build more complex objects - which we can then use with some common statistical functions. We’re going to try this out by using a simple set of dummy data: we’re going to use the total number of pages and publication dates of the various editions of Geographic Information Systems and Science (GISS) for our brief dummy analysis: Book Edition Year Total Number of Pages 1st 2001 454 2nd 2005 517 3rd 2011 560 4th 2015 477 As we can see, we will ultimately want to store the data in a table as above (and we could easily copy this to a csv to load into R if we wanted). But we want to learn a little more about data structures in R, therefore, we’re going to go ahead and build this table “manually”. Let’s get going. Clearing our Environemnt workspace First, let’s clear up our workspace and remove our current variables: Type rm(ten, twelve, output, str_variable, two_str_variable) into the console and execute. # Clear our workspace rm(ten, twelve, output, str_variable, two_str_variable) Note, of course you can either copy and paste this code - or try out using the tab function to autocomplete your variable names in the console as you start typing them in, just as we did when using the Command Line. You should now see we no longer have any variables in our window - we just used the rm() function to remove these variables from our environment. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later - but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment. Creating an atomic vector of multiple elements The first complex data object we will create is a vector. A vector is the most common and basic data structure in R and is pretty much the workhorse of R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types: Atomic vectors (all elements are of the same data type) Lists (elements can be of different data types) Although in practice the term “vector” most commonly refers to the atomic types and not to lists. The variables we created above are actually vectors - however they are made of only one or two elements. We want to make complex vectors with more elements to them. Let’s create our first official “complex” vector, detailing the different total page numbers for GISS: Type giss_page_no &lt;- c(454, 517, 560, 477) into the console and execute. # store our total number of pages, in chronological order, as a variable giss_page_no &lt;- c(454, 517, 560, 477) Let’s check the results. Type print(giss_page_no) into the console and execute. # print our giss... variable print(giss_page_no) ## [1] 454 517 560 477 We can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object: Type our various statistical functions (detailed below) into the console and execute. # calculate the arithmetic mean on our variable mean(giss_page_no) ## [1] 502 # calculate the median on our variable median(giss_page_no) ## [1] 497 # calculate the range numbers of our variable range(giss_page_no) ## [1] 454 560 We have now completed our first set of descriptive statistics in R! We now know that the average number of pages the GISS book has contain is 497 pages - this is of course truly thrilling stuff, but hopefully an easy example to get onboard with. But let’s see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note, I entered the total number of pages in a specific order to correspond to these publishing dates (i.e. chronological), as outlined by the table above. As a result, I’ll enter the publication year in the same order. Type giss_year &lt;- c(2001, 2005, 2011, 2015) into the console and execute. # store our publication years, in chronological order, as a variable giss_year &lt;- c(2001, 2005, 2011, 2015) Let’s check the results. Type print(giss_year) into the console and execute. #print our giss_year variable print(giss_year) ## [1] 2001 2005 2011 2015 Again, truly exciting stuff. Of course, on their own, the two vectors do not mean much - but we can use the same c() function to combine the two together to create a matrix. Creating a matrix from two vectors In R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both rows and columns. As with atomic vectors, the elements of a matrix must be of the same data type. As both our page numbers and our years are numeric (we can check this using which function?), we can add them together to create a matrix using the matrix() function: Type giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) into the console and execute. # create a new matrix from our two vectors with two columns giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) # note the inclusion of a new argument to our matrix: ncol=2 #this stands for &quot;number of columns&quot; and we want two. Again, let’s check the results. Type print(giss_year_nos) into the console and execute. print(giss_year_nos) ## [,1] [,2] ## [1,] 2001 454 ## [2,] 2005 517 ## [3,] 2011 560 ## [4,] 2015 477 The thing about matrices - as you might see above - is that, for us, they don’t have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we wouldn’t really know what to do with it. As a result, we tend to primarily use Data Frames in R as they offer the opportunity to add field names to our columns to help with their intepretation. Arguments/Parameters in Functions The function we just used above, ‘matrix()’, was the first function that we used that took more than one argument. In this case, the arguments the matrix needed to run were: What data or dataset should be stored in the matrix. How many columns (ncol=) do we need to store our data in. The function can actually accept several more arguments - but these were not of use for us in this scenario, so we did not include them. For almost any R package, the documentation will contain a list of the arguments that the function will takes, as well as in which format the functions expects these arguments and a set of usage examples. Understanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function - and whether we will need to transform our variable into a different data structure to be used for that specific function. For any function, there will be mandatory arguments (i.e. it will not run without these) or optional arguments (i.e. it will run without these, as the default to this argument has been set usually to FALSE, 0 or NULL). Creating a Data Frame from our matrix A data frame is an extremely important data type in R. It is pretty much the de-facto data structure for most tabular data and what we use for statistics. It also is the underlying structure to the table data (what we would call the attribute table in Q-GIS) that we associate with spatial data - more on this next week. A data frame is a special type of list where every element of the list will have the same length (i.e. data frame is a “rectangular” list), Essentially, a data frame is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply NULL. Data frames can have additional attributes such as rownames(), which can be useful for annotating data, like subject_id or sample_id or even UID. In statistics, they are often not used - but in spatial analysis, these IDs can be very useful. Some additional information on data frames: They are usually created by read.csv() and read.table(), i.e. when importing the data into R. Assuming all columns in a data frame are of same type, a data frame can be converted to a matrix with data.matrix() (preferred) oras.matrix(). You can also create a new data frame with data.frame() function, e.g. a matrix can be converted to a data frame, as we’ll see below. You can find out the number of rows and columns with nrow() and ncol(), respectively. Rownames are often automatically generated and look like 1, 2, …, n. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset. Let’s go ahead and create a new data frame from our matrix: Type giss_df &lt;- data.frame(giss_year_nos) into the console and execute. # Create a new dataframe from our matrix giss_df &lt;- data.frame(giss_year_nos) We now have a data frame, we can finally use the View() function in R. Still in your console, type: View(giss_df) # View our data frame View(giss_df) You should now see a table pop-up as a new tab on your script window. It’s now starting to look like our original table - but we’re not exactly going to be happy with X1 and X2 as our field names - they’re not very informative. Renaming our column field names Instead, what we can do is rename our data frame column field names by using the names() function. Before we do this, have a read of what the names() function does. Still in your console, type: ?names # Get the help documentation for the names function ?names As you can see, the function will get or set the names of an object, with renaming occuring by using the following syntax: names(x) &lt;- value The value itself needs to be a character vector of up to the same length as x, or NULL. This is one of the cool aspects of OOP, in that we can access specific parts of our object and change it without changing the object as a whole or having to create a new object/variable to enact our changes. We have two columns in our data frame, so we need to parse our names() function with a character vector with two elements. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, new_names &lt;- c(\"year\", \"page_nos\"), and then the names() function containing this vector for renaming, names(giss_df) &lt;- new_names: # Create a vector with our new column names new_names &lt;- c(&quot;year&quot;, &quot;page_nos&quot;) #Rename our columns with our next names names(giss_df) &lt;- new_names You can go and check your data frame again and see the new names using either View() function or by clicking on the tab at the top. Adding a column to our data frame We are still missing one final column from our data frame - that is our edition of the textbook. As this is a character data type, we would not have been able to add this directly to our matrix - and instead have waited until we have our data frame to do so. This is because data frames can take different data types, unlike matrices - so let’s go ahead and add the edition as a new column. To do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our data frame by storing this vector as a new column in our data frame. We use the $ sign with our code that gives us “access” to the data frame’s column - we then specify the column edition, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column. This $ is another feature of OOP. Let’s take a look. Create a edition vector variable containing our textbook edition numbers - type and execute edition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\"). We then store this as a new column in our data frame under the column name edition by typing and executing giss_df$edition &lt;- edition: # Create a vector with our editions edition &lt;- c(&quot;1st&quot;, &quot;2nd&quot;, &quot;3rd&quot;, &quot;4th&quot;) # Add this vector as a new column to our data frame giss_df$edition &lt;- edition Again, you can go and check your data frame and see the new column using either View() function or by clicking on the tab at the top. You should now have a data frame that looks like: Now we have our data frame, let’s find out a little about it. We can first return the dimensions (the size) of our data frame by using the dim() function (dim simply stands for dimensions in this case.). In your console, type dim(giss_df) and execute: # Check our data frame dimensions dim(giss_df) ## [1] 4 3 We can see we have four rows and three columns. And we can finally use our attributes() function to get the attributes of our data frame. In your console, type attributes(giss_df) and execute: # Check our data frame attributes attributes(giss_df) ## $names ## [1] &quot;year&quot; &quot;page_nos&quot; &quot;edition&quot; ## ## $row.names ## [1] 1 2 3 4 ## ## $class ## [1] &quot;data.frame&quot; You should see that we now get a list of the column and row names, plus the class of the data frame. There is a lot more we could now do with our data frame but we simply do not have time - and we’d much rather implement some of these functions or data management techniques with a much more exciting dataset than the details of the GISS textbook. Hopefully though, this has served as a good introduction to the different data structures you’ll be coming across over the next 6 weeks as we use R - and provided you with some simple code you can return to time and time again for reminders, such as how to create a new column in your data frame. Before we leave the console (and to be honest, we won’t exactly leave it behind), we’ll enter one last line of code for now: Type in install.packages(\"tidyverse\") into the console and execute. # Install the tidyverse library install.packages(&quot;tidyverse&quot;) Leave this code to run - it might take some time but you won’t need to worry about this until you’ve moved onto the practical section. Now we’re ready to move onto our next section, but first - after reading the Tips and recap below - I recommend you take a long break from this workbook! Tips &amp; Tricks R is case-sensitive so you need to make sure that you capitalise everything correctly if required. The spaces between the words don’t matter but the positions of the commas and brackets do. Remember, if you find the prompt, &gt;, is replaced with a + it is because the command is incomplete. If necessary, hit the escape (esc) key and try again. It is important to come up with good names for your objects. In the case of the majority of our variables, we used a underscore _ to separate the words. It is good practice to keep the object names as short as posssible but they still need to be easy to read and clear what they are referring to. Be aware: you cannot start an object name with a number! If you press the up arrow in the command line you will be able to edit the previous lines of code you have inputted. Coding Breakthroughs In this section you have: Entered your first commands into the R command line interface. Created objects in R. Created a vector of values. Executed some simple R functions. Created a data frame. Now, please, make sure you go ahead and take a break! The Tidyverse Philosophy and Principles Over the past weeks a lot of information has come your way, diving deep into the world of GIScience…and now programing. However, whilst you are slowly becoming proficient in using spatial data and hopefully enjoying learning about how to code, we also need to learn about how our data is structured aand organised. I told you there were quite a few learning curves in this module! This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be what’s becoming colloquially called: dirty data. What we mean by dirty data is data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Let’s think back to the Ward and Borough Population data that you downloaded from the ONS - we could not use the Excel Spreadsheet straight away as it was within a large workbook with a) many tabs and b) lots of additional formatting (e.g. empty rows, “whitespace”). Instead, we extracted the data we wanted and formatted it into a very simple table, consisting of only fields that contained individual records for each of our Wards or Boroughs. This table would fit what is understood as the tidy data approach, which is a general perspective on how to structure your data in R to ease its analysis. Tidy data was formalised by R Wizard Hadley Wickham in his contribution to the Journal of Statistical Software as a way of consistently structuring your data. In the words, of the Wizard: Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This tidy data approach is very much at the core of the tidyverse R package that we just installed - and for us as soon-to-be connoisseurs of secondary data, is also of significant importance when organising your data for future projects. So what do tidy data look like? Tidy Data In Practice Believe it or not, you can often represent the same underlying data in multiple ways. The example below, taken from the the tidyverse package and described in the R for Data Science book, shows that the same data can organised in four different ways. The data shows the population and cases (of something, e.g. malaria) for each country, for 1999 and 2000: None of these representations are wrong per se, however, not are equally easy to use. Only Table 1 can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In the case of Table 4 - we even have two tables! These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a table/data frame/tibble. Put each variable in a column. Figure 4.1: A visual representation of tidy data by Hadley Wickham. Why ensure that your data is tidy? Well, there are two main advantages (according to Hadley Wickham): There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in mutate and summary functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. We’ll see all of this in action over the coming weeks - but if you’d like, you can hear this explanation directly from one of R-Studio’s very own resident Data Scientists below. You don’t need to listen to the whole video, but from the beginning until approximately 7:40mins. Tidy Data in R* {-} The R-Studio Youtube channel is also generally a great resource for you to be aware of as you pursue your learning in R. So what is the Tidyverse? The tidyverse is a collection of packages that are specifically designed for these data sciencey tasks of data wrangling, management, cleaning, analysis and visualisation within R-Studio. Our earlier lecture introduced you to the concept of a package - but they are explained in more detail below. Whilst in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures as we’ll see over the coming weeks. The tidyverse itself is treated and loaded as a single package, but this means if you load the tidyverse package within your script (through library(tidyverse)), you will directly have access to all the functions that are part of each of the packages that are within the overall tidyverse. This means you do not have to load each package seperately - saving us lines of code, sweet! We’ve already gone ahead and executed the code to install the tidyverse within our various versions of R - but because the tidyverse consists of multiple packages, it may take a little while before everything is installed so be patient! For more information on tidyverse, have a look at https://www.tidyverse.org/. How does the tidyverse help with tidying data? There are some specific functions in tidyverse suite of packages that will help us cleaning and preparing our datasets to create a tidy dataset. The most important and useful functions, from the tidyr and dplyr packages, are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgropus for further processing tidyr pivot_longer convert data from wide format to long format tidyr pivot_wider convert long format dataset to wide format These functions all complete very fundamental tasks that we need to manipualte and wrangle our data. We will get to use these over the coming weeks, so do not panic about trying to remember them all right now. Installing and using Libraries/Packages for Data Analysis As you will have heard in our earlier lecture, our common Data Analysis languages, including Python and R, have developed large community bases and as a result there are significant amount of help and support resources for those working in data science. Beyond help and support, these large community bases have been essential for expanding the utility of a programming language for specific types of data analysis. This is because of how programming languages work – they have a core library of functions to do certain things, such as calculate the mean of a dataset as we did earlier. But to do more specific or specialized analysis, such as create a buffer around a point, a function needs to be written to enable this. You either need to write the function yourself – or hope that someone else has written it – plus you need to know that there is the supporting functions around it. E.g. your code can “read” your spatial data and know a) its spatial and b) the projection system its in to calculate a distance. Without this, you won’t be able to run your function or do your analysis. These community bases have identified these gaps, such as for spatial data reading and analysis, and spent considerable amount of time writing these functions and supporting functions to add to the core library. These functions often get packaged into an additional library (or can be called a package) that you add to your own core library by installing this library to your computer AND then importing it to your work through your script. The tidyr and dplyr packages with the tidyverse are just two examples of these additional libraries created by the wider R community. The code you just ran asked R-Studio to fetch and install the tidyverse into your R-Studio - so this means we’ll be able to use these libraries in our practical below simply by using the library(tidyverse) code at the top of our script. One thing we need to be aware of when it comes to using functions in these additional libraries, is that sometimes these functions are called the same thing as the base R package, or even, in some cases, another additional library. We therefore often need to specify which library we want to use this function from, and this can be done with a simple command (library::function) in our code - as we’ll see in practice over the next few weeks, so just make a mental note of this for now. Whilst we’ve gone ahead and installed the tidyverse, each time we start a new script, we’ll need to load the tidyverse. We are going to show all of this in our next Prcatical, which gets you to analyse crime in London whilst putting into place everything we’ve been dicussing today. Practical 3: Analysing Crime in 2020 in London Wow, we’ve got through a lot today - and barely even started our practical! But, what I can say, is that there is not a substantial more to learn in terms of principles and practicalities of programming beyond building up your “dictionary/vocabulary” of programming libraries and respective commands. There are some more complicated coding things we can do, such as for and while loops and if statements, but, for now, consider yourself a solid beginner programmer. As a result, we’re ready to put this all into practice in our practical today, which will be relatively short in comparison to everything you’ve been through above. What we’ll be doing today is running an exploratory data analysis, using basic statistics, of crime in London over a monthly basis. Let’s get started. Setting Up R-Studio for GEOG0030 In the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this isn’t routinely the case. In RStudio, we can use scripts to build up our code that we can run repeatedly - and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by creating a new project. To put it succintly, projects in R-Studio keep all the files associated with a project together — input data, R scripts, analytical results, figures. This means we can easily keep track of - and access - inputs and outputs from different weeks across our module, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with directories and paths a whole lot easier - particularly if you have followed the folder structure I advised at the start of the module. Let’s go ahead and make a New Project directly within our GEOG0030 folder. Click on File -&gt; New Project –&gt; Existing Directory and browse to your GEOG0030 folder. Click on Create Project. You should now see your main window switch to this new project - and if you check your Files window, you should now see a new R Project called GEOG0030: We are now “in” the GEOG0030 project - and any folders within the GEOG0030 project can be easily accessed by our code. Furthermore, any scripts we create will be saved in this project. Note, there is not a “script” folder per se, but rather your scripts will simply exist in this project. You can test this change of directly by selecting the Terminal window (next to your Console window) to access the Terminal in R-Studio and type our pwd command. You should see that our current directory is your GEOG0030 folder. R for Data Science by Hadley Wickham and Garret Grolemund Your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. I’d stick with these sections for now, but have a quick glance at what’s available in the book. Setting up our script In our shorter practical sessions above, we’ve had a bit of fun playing with the R code within the R console and seeing how we can store variables and access information about them. Furthermore, we’ve had a look at the different data structures we may use moving forward. But ultimately this really doesn’t offer the functionality that we want for our work - or even the reality of what we need to do with spatial analysis. What we really want to do is to start building scripts and add start analysing some data! Therefore, for the majority of our analysis work, we will type our code within a script and not the console. Let’s create our first script: Click on File -&gt; New File –&gt; R Script. You can also use the plus symbol over a white square as a shortcut or even Ctrl/CMD + Shift + N. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date. Let’s go ahead and save this script straight away. Save your script as: wk4-csv-processing.r. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with csv processing. This will help us a lot in the future when we come to find code that we need for other projects. I personally tend to use one script per type of processing or analysis that I’m completing. For example, if you are doing a lot of data cleaning to create a final dataset that you’ll then analyse, its best practice to separate this from your analysis script so you do not continually clean your raw datasets when you run your script. Giving our script some metadata The first bit of code you will want to add to any script is to add a TITLE. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (#) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run. Let’s go ahead and give our script a TITLE - and maybe some additional information: Add the following to your script (substitute accordingly): # Combining Police Data csvs from 2020 into a single csv # Followed by analysis of data on monthly basis # Script started January 2021 # NAME Save your script. Load Our Libraries Now we have our title, the second bit of code we want to include in our script is to load our libraries (i.e. the installed packages we’ll need in our script): Type the following into the script: # Libraries used in this script: # Load the tidyverse library library(tidyverse) By loading simply the tidyverse, we have a pretty good estimate that we’ll be able to access all the functions that we’re going to need today. However, often when developing a script, you’ll realise that you’ll need to add libraries as you go along in order to use a specific function etc. When you do this, always add your library to the top of your script - if you ever share your script, it helps the person you are sharing with recognise quickly if they need to install any additional packages prior to trying to run the script. It also means your libraries do not get lost in the multiple lines of code you are writing. Setting Up Our Directory Understanding and accessing our directory path used to be the worst part of programming. And if you do not use the Project approach advocated above, it certainly will continue to be. If, for example, we did not use a project approach, we would need to set our working directory directly within our script using the command: setwd(\"file/path/to/GEOG0030) We’d therefore need to know what our path is and hope we do not make any mistakes. There are some automated shortcuts to doing this in R using the Files window, but ultimately, having to set a working directory is becoming a thing of the past. Because we are using a project approach - we do not need to set a working directory - because we’re already in it! Therefore, when looking for data in our folders, we know pretty much the path we’ll need to take. However, we still might need to access data from another folder outside of our GEOG0030 folder - so we need to know how to do this. To help with this, we’re going ot add one more library to our library list, called the here library. We won’t go into too much detail what this library does per se, but essentially it alows you to direct R to a specific area on your computer and a specific file with relative ease. We actually won’t use it in this practical, but I wanted to get you into the habit of adding it to your scripts by default. First, you’ll need to install this library to your computer. In your Console window, type and execute: # Install the here library via your console install.packages(&quot;here&quot;) Once installed, we can go ahead and load this after our tidyverse library - your script should look like so: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) One thing to note, not only does installing and loading libraries need to occur in two different parts of R-Studio, but when installing, your library needs to be in \"\" but when loading, it does not. File and folder names best practice Please ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; &amp; $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ if you like. Remember to save your script. We’re now ready to run these first two lines of code. Running a script in R-Studio There are two main ways to run a script in R-Studio - all at once or by line/chunk by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code iteratively. To run line-by-line: By clicking: Select the line or chunk of code you want to run, then click on Code and choose Run selected lines. By key commands: Select the line or chunk of code you want to run and then hold Ctl or Cmd and press Return. To run the whole script By clicking: Click on Run on the top-right of the scripting window and choose Run All. By key commands: Hold Option plus Ctl or Cmd and R. Stopping a script from running If you are running a script that seems to be stuck (for whatever reason) or you notice some of your code is wrong, you will need to interrupt R. To do so, click on Session -&gt; Interrupt R. If this does not work, you may end up needing to Terminate R but this may lose any unsaved progress. Run your code line-by-line In this practical, I recommend running each line (or set of lines) of code you enter as you go - rather than wait til the end and execute the whole script. This way you will be able to find any bugs as you go along. Don’t forget to scroll to the top of your script and execute your library loads! Data Import and Processing We’re now ready to get started with using the crime data csvs currently sat in our all_crime folder. To do so, we need to first figure out how to import the csv and understand the data structure it will be in after importing. Importing and loading Data To read in a csv into R requires the use of a very simple function: read_csv(). We can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single csv, we’ll go ahead and just use the function with a simple parameter. # Read in a single csv from our crime data crime_csv &lt;- read_csv(&quot;data/raw/crime/all_crime/2020-11-metropolitan-street.csv&quot;) We can explore the csv we have just loaded as our new crime_csv variable and understand the class, attributes and dimensions of our variable. # Check class and dimensions of our data frame # you can also check the attributes if you would like - this will load up a huge list of every row though! class(crime_csv) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; dim(crime_csv) ## [1] 96914 12 We’ve found out our variable is a data frame, containing 96914 rows and 12 columns. We can also tell it’s a big file - so best not load it up right now. We however do not want just the single csv and instead what to combine all our csvs in our all_crime folder into a single dataframe - so how do we do this? Joining all of our csvs files together into a single data frame This will be the most complicated section of code you’ll come across today, and we’ll use some functions that you’ve not seen before - we also need to install and load an additional library to use something known as a pipe function which I’ll explain in more detail next week. In your console, install the magrittr package: # Install the magrittr library via your console install.packages(&quot;magrittr&quot;) And in your # Load libraries section of your script, add the magrittr library. Your library section should look like this: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) library(magrittr) Remember to execute the loading of the magrittr library by selecting the line and running the code. Now we’re ready to add and run the following code: # Read in all csvs and append all rows to a single data frame all_crime_df &lt;- list.files(path=&quot;data/raw/crime/all_crime&quot;, full.names=TRUE) %&gt;% lapply(read_csv) %&gt;% bind_rows This might take a little time to process (or might not), as we have a lot of data to get through. You should see a new datafarme appear in your global environment called all_crime, for which we now have 1,099,507 observations! Explaining the above code It is a little difficult to explain the code above without going into a detail explanation of what a pipe is (next week) but essentially what these three lines of code does is: List of of the files found in the data path: \"data/raw/crime/all_crime Read each of these as a csv (this is the lapply() function) in as a dataframe And then bind the rows of these dataframes to a single dataframe called all_crime_df We’ll go into more detail about pipes next week. We can now have a look at our large dataframe in more detail. Let’s have a look # Understand our all_crime_df cols, rows and print the first five rows ncol(all_crime_df) ## [1] 12 nrow(all_crime_df) ## [1] 1099507 head(all_crime_df) ## # A tibble: 6 x 12 ## `Crime ID` Month `Reported by` `Falls within` Longitude Latitude Location ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 37c663d86… 2020… City of Lond… City of Londo… -0.106 51.5 On or n… ## 2 5b89923fa… 2020… City of Lond… City of Londo… -0.118 51.5 On or n… ## 3 fb3350ce8… 2020… City of Lond… City of Londo… -0.113 51.5 On or n… ## 4 07172682a… 2020… City of Lond… City of Londo… -0.112 51.5 On or n… ## 5 14e02a604… 2020… City of Lond… City of Londo… -0.111 51.5 On or n… ## 6 &lt;NA&gt; 2020… City of Lond… City of Londo… -0.0980 51.5 On or n… ## # … with 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime ## # type` &lt;chr&gt;, `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt; You should now see with have the same number of columns as our previous single csv, but 1,099,507 rows! You can also see that the head() function provides us with the first five rows of our dataframe. You can conversely use tail() to provide the last five rows. Filtering our data to a new variable For now in our analysis, we only want to extract the theft crime in our dataframe - so we will want to filter our data based on the Crime type column. However, as we can see, we have a space in our field name for Crime type and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding (or else our code will break!), we need to rename our fields. To do so, we’ll first get all of the names of our fields so we can copy and paste these over into our code: # Get the field names of our all_crime_df names(all_crime_df) ## [1] &quot;Crime ID&quot; &quot;Month&quot; &quot;Reported by&quot; ## [4] &quot;Falls within&quot; &quot;Longitude&quot; &quot;Latitude&quot; ## [7] &quot;Location&quot; &quot;LSOA code&quot; &quot;LSOA name&quot; ## [10] &quot;Crime type&quot; &quot;Last outcome category&quot; &quot;Context&quot; We can now copy over these values into our code to create a new vector variable that contains these field names, updated without spaces. We can then rename the field names in our dataset - just as we did with our GISS table earlier: # # Create a new vector containing updated no space / no capital field names no_space_names &lt;- c(&quot;crime_id&quot;, &quot;month&quot;, &quot;reported_by&quot;, &quot;falls_within&quot;, &quot;longitude&quot;,&quot;latitude&quot;, &quot;location&quot;, &quot;lsoa_code&quot;, &quot;lsoa_name&quot;, &quot;crime_type&quot;, &quot;last_outcome_category&quot;, &quot;context&quot;) # Rename our df field names using these new names names(all_crime_df) &lt;- no_space_names Note, we could have cleaned our data further and so would only needed to rename a few columns using slicing - but we’ll save data frame slicing for next week!. We now have our dataframe ready for filtering - and to do so, we’ll use the filter() function for the dplyr library. This function is really easy to use - but there is also a filter() function in the R base library - that does something different to the function in dplyr. As a result, we need to use a specific type of syntax - library::function - to tell R to look for and use the the filter function from the dplyr library rather than the default base library. We then also need to populate our filter() function wsith the necessary paramteres to extract only the “Theft from the person” crime type. This includes providing the function with our main dataframe plus the filter query, as outliend below: # Filter all_crime_df to contain only theft, store as a new variable: all_theft_df all_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == &#39;Theft from the person&#39;) You should now see the new variable appear in your Environment with 28,476 observations. Great, you’ve completed your first ever filter using programming. We now want to follow the tidy data philosophy and create one final dataframe to allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London - and luckily for us dplyr has another function that will do this for us, known simply as count(). You perhaps can see already that dplyr is likely to become well-used library by us in Geocomputation…! Go ahead and search the help to understand the count() function - you’ll also see that there is only one function called count() so far, i.e. the one in the dplyr library, so we do not need to use the additional syntax we used above. Let’s go ahead and count the number of thefts in London by month. The code for this is quite simple: # Count in the all_theft_df the number of crimes by month and store as a new dataframe theft_month_df &lt;- count(all_theft_df, month) We’ve stored the output of our count() function to a new dataframe: theft_month_df. Go ahead and look at the dataframe to see the output - it’s a very simple table containing simply the month and n, i.e. the number of crimes occuring per month. We can and should go ahead and rename this column to help with our interpretation of the dataframe. We’ll use a quick approach to do this, that uses selection of the precise column to rename only the second column: # Rename the second column of our new data frame to crime_totals names(theft_month_df)[2] &lt;- &quot;crime_totals&quot; This selection is made through the [2] element of code added after the names() function we have used earlier. We’ll look more at selection, slicing and indexing in next week’s practical. Data Analysis: Distribution of Crime in London by month in 2020 We now have our final dataset ready for our simple analysis for our Assignment. Assignment 1: Analysis of crime per month in London in 2020 For your assignment this week, I would like you to complete two tasks. 1. Basic statistics First what I would like you to do, using the code you’ve written previously is to find out: What is the mean average crime per month? What is the median average crime per month? You may also automate the collection of the max, min and range of our crime per month dataframe. 2. Monthly graph The second thing I would like you to do is present our data on a simple bar chart. The basic code to generate a bar chart is provide here: # Read in a single csv from our crime data barplot(theft_month_df$crime_totals, main=&quot;Crime distribution in London by month in 2020&quot;, names.arg = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;)) As you’ll see, we have added a Title to our graph (main= argument), whilst used the names.arg parameter to add the months of the year in along the x-axis. Using the barplot() documentation, I would like you to figure out how to change the bar chart fill and borders another colour from grey and black respectively. You may also look to customise your chart further, but primarily I’d like you to work out how to change these colours to something more aesthetically appealing! You do not need to submit your bar chart this week, but have it ready for your work in Week 5. Beyond the assignment, just take a look at your bar chart and how the distribution of crime changed last year… Well that’s pretty cool, huh?! I wonder what happened in March to make theft from a person decrease so substantially? We’ve managed to take a dataset of over 1 million records and clean and filter it to provide a chart that actually shows the potential impact of the COVID-19 lockdown on theft crime in London. Of course, there is a lot more research and exploratory data analysis we’d need to complete before we could really substantiate our findings, but this first chart is certainly a step in the right direction! Next week, we’ll be doing a lot more with our dataset - including a lot more data wrangling and of course spatial anlaysis, but hopefully this week has shown you want you can achieve with just a few lines of code. Now, make sure to save your script, so we can return to it next week. You do not need to save your workspace - but can do so if you’d like. Saving the workspace will keep any variables generated during your current session saved and available in a future session. Extension Activity: Mapping Other Crime Type Distributions Across London If you’ve whizzed through this workshop and would like an additional challenge, you are more than welcome to deploy the code you’ve used above on one or more other crime types in London. If you remember, each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. You can conduct the same analysis on one or more of these categories in addition to theft, to see if you can find a similar pattern in their prevalance/distribution over the same months. R for Data Science by Hadley Wickham and Garret Grolemund As highlighted earlier, your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. In addition, you can look at Sections: 1, 2, 4, 11 and 12. I’d stick with these sections for now, but have a quick glance at what else is available in the book. We’ll be looking at Data Transformation (5) and Visualisation (3) next week, plus more on Exploratory Data Analysis (7) in Week 6. Don’t worry about completing the exercises - unless you would like to! Recap In this section you have learnt how to: Create an R script. Load a csv into R, perform some analysis, and write out a new csv file to your working directory. Subset R data frames by name and also column and/or row number. Created a simple graph to plot crime in London by month. Learning Objectives You should now hopefully be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R Acknowledgements Part of this page is adapted from POLS0008: Understanding Data and GEOG0114: Exploratory spatial data analysis by Justin Van Dijk at UCL as well as Software Carpentry’s Introduction to R Data Types and Data Structures Copyright © Software Carpentry. The examples and datasets used in the workbook are original to GEOG0030. "],["programming-for-giscience-and-spatial-analysis.html", "5 Programming for GIScience and Spatial Analysis", " 5 Programming for GIScience and Spatial Analysis Welcome to Week 5 in Geocomputation! This week is, again, heavily practical oriented - with our practical taking up the majority of our time this week. You’ll find in this practical, many additional explanations of key programming concepts - such as selection, slicing and pipes - integrated within it. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. Week 5 in Geocomp To be added on Thursday - always nice to say hi to you all! Video on Stream This week’s content introduces you to the foundational concepts associated with Programming for Spatial Data Analysis, where we have three new areas of work to focus on: Data wrangling in programming (using indexing, selection and slicing) Using spatial libraries in R to store and manage spatial data Using visualisation libraries in R to map spatial data This week’s content is split into 4 parts: Spatial Analysis for Data Science Research (20 minutes) Spatial Analysis Software and Programming (20 minutes) Spatial Analysis in R-Studio (40 minutes) Practical 4: Analysing Crime in 2020 in London from a spatial perspective (90 minutes) This week, we have 2 lectures (15 mins and 40 mins), and an additional instructional video to help you with the completion of this week’s practical. A single Key Reading is found towards the end of the workshop. After promising to set a Mini-Project during Reading Week, I appreciate the delivery of this material is late, so I will not be setting the Project as promised. Instead, I would like you to spend time going through the practical and experimenting with the visualisation code at the end. There is also an extension that I would like you to complete, if possible over Reading Week. Part 4 is, as usual, the main part of analysis for our Practical for this week - all programming this week is within Part 4, which is a little longer than usual to account for this. If you have been unable to download R-Studio Desktop or cannot access it via Desktop@UCL Anywhere, you will have access to our R-Studio Server website instead. Instructions on how to access this are provided in the previous week’s workshop. Learning Objectives By the end of this week, you should be able to: Understand how spatial analysis is being used within data science applications Recognise the differences and uses of GUI GIS software versus CLI GIS software Understand which libraries are required for spatial analysis in R/R-Studio Conduct basic data wrangling in the form of selection and slicing Create a map using the tmap visualisation library We will continue to build on the data analysis we completed last week and look to further understand crime in London by looking at its prevalence on a month-by-month basis but this time, from a spatial perspective. Spatial Analysis for Data Science Research Over a decade ago, when I first became involved in the GIScience world, the term “data science” barely existed - fast-forward to today, and it doesn’t go a day without hearing the phrase and the hubris surrounding its potential to help solve the many grand challenges the modern world faces. Whilst there is much hubris (and not a huge amount of evidence) of data science’s ability to “save the world”, on a more fundamental level, data science, and the community of practice associated with it, is having a transformational impact on how we think about and “do” data-focused (and primarily quantitative) research. For us geographers and geographically-minded analysts, our traditional use of GIScience and spatial analysis is most certainly not immune to this transformation - many of the datasets assicated with data science do have a locational component and thus we have seen an increasing interest in and entry into the spatial analyis field from more “generalised” data analysts or data scientists. Furthermore, the increasing popularity of data science amongst ourselves as geographers is also having a signficant impact on how we “do” spatial anaysis. We have, as a result, seen a greater focus on the use of programming as a primary tool within spatial analysis, concomitant to a new prioritisation of openness and reproducibility in our research and documentation of our results. Hence why, a decade later, an Undergraduate module on GIScience now focues on “Geocomputation”, a precursor to spatial data science, rather than a more generalised understanding of the GIS industry and the traditional applications of GIS and spatial analysis, such as: Transportation Logistics Supply Chain Management Generalised Urban Planning Insurance Environmental modelling Whilst these traditional applications and industries still utilise GIS software (and there is substantial potential to build careers in these areas, particularly through the various Graduate Schemes offered by related companies such as Arup, Mott MacDonald, Esri, to name a few), with data science emerging as a dominant area of growth in spatial analysis, it is important to prioritise the skills you will need to complete in the relevant sectors that are hiring “spatial data scientists”, i.e. learning to code effectively and efficiently. Once you have acquired these skills, the outstanding question becomes: how will I apply them in my future career? Whilst the majority of spatial analysis using programming is not exactly too different from spatial analysis using GIS software, the addition of programming skills have opened up spatial analysis to many different applications and, of course, novel datasets. Within academia and research itself, we see the use of spatial analysis within data science research for: 1. Analysis of distributions, patterns, trends and relationships within novel datasets The most basic application of spatial analysis - but one that now utilises large-scale novel datasets, such as mobile phone data, social media posts and other human ‘sensor data’. To get a better understanding of the various applications, a key recommendation is to look at Carto’s introduction video to their Spatial Data Science conference held this year, where they highlighted how spatial data science has been used for various applications within COVID-19. As a commerical firm, they seem to have a bit of cash to make great videos, but I’d also recommend looking at the various talks held at the conference this year that show the diversity of applications using spatial data science from the various participants. Carto’s take on the use of spatial data science for COVID-19 2. Supplementing the analysis of traditional datasets for augmented information Adding a ‘spatial data science’ edge to traditional analysis, supplementing “small” datasets with big data (or vice versa) to provide new insights into both datasets. An example of this is the recent combination of geodemographic classification (Week 9) with big data information on mobility (e.g. mobile phone data, travel card data) to understand different types of commuter flows and thinking through how this can inform better urban planning policy. A recent paper that did just such is from Liu and Cheng (2020), with the following abstract: Plentiful studies have discussed the potential applications of contactless smart card from understanding interchange patterns to transit network analysis and user classifications. However, the incomplete and anonymous nature of the smart card data inherently limit the interpretations and understanding of the findings, which further limit planning implementations. Geodemographics, as ‘an analysis of people by where they live’, can be utilised as a promising supplement to provide contextual information to transport planning. This paper develops a methodological framework that conjointly integrates personalised smart card data with open geodemographics so as to pursue a better understanding of the traveller’s behaviours. It adopts a text mining technology, latent Dirichlet allocation modelling, to extract the transit patterns from the personalised smart card data and then use the open geodemographics derived from census data to enhance the interpretation of the patterns. Moreover, it presents night tube as an example to illustrate its potential usefulness in public transport planning. (Yunzhe Liu &amp; Tao Cheng (2020) Understanding public transit patterns with open geodemographics to facilitate public transport planning, Transportmetrica A: Transport Science, 16:1, 76-103, DOI: 10.1080/23249935.2018.1493549) We’ll be looking at this in a little more detail in Week 9. 3. Creation of new datasets from both traditional and novel datasets Opening up spatial analysis to novel datasets has enabled many researchers to identify opportunities in the creation of new datasets that can ‘proxy’ certain human behaviours and characteristics that we currently do not either have data for, or the data is old/insufficient/not at the right scale. A good example of this is my previous research group at the University of Southampton: Worldpop. Worldpop create population and socio-economic datasets for every country across the world utilising (primarily) bayesian modelling approaches alongside both census data and more innovative datasets, such as mobile phone data or tweets. You can watch this incredibly cheesey but informative video made by Microsoft about the group below: What does Worldpop do? There are plenty of examples in recent GIS and spatial analysis research where new datasets are/have been created for use in similar applications. Another example is Facebook, who is using a lot of their socila network data to create mobility and social connectivity datasets with their ‘Data For Good’ platform (see more here). 4. Creation of new methods and datasets Finally, the intersection of data science and spatial analysis has also seen geographers adapt data science techniques to create new methods and analytical algorithims to puruse the creation of more new datasets and/or new insight. An example of this is the increased use and adaptation of the DB-Scan algorithm (Week 7) within urban analytics, seen within the various papers: Xinyi Liu, Qunying Huang &amp; Song Gao (2019) Exploring the uncertainty of activity zone detection using digital footprints with multi-scaled DBSCAN, International Journal of Geographical Information Science, 33:6, 1196-1223, DOI: 10.1080/13658816.2018.1563301 Arribas-Bel, D., Garcia-López, M. À., &amp; Viladecans-Marsal, E. (2019). Building (s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics, 103217. Jochem, W. C., Leasure, D. R., Pannell, O., Chamberlain, H. R., Jones, P., &amp; Tatem, A. J. (2020). Classifying settlement types from multi-scale spatial patterns of building footprints. Environment and Planning B: Urban Analytics and City Science. https://doi.org/10.1177/2399808320921208 Beyond these research-oriented applications, we can also think of many ‘data sciencey’ applications that we use in our day to day lives that use spatial analysis as a key component. From the network analysis behind route-planning within mapping applications to searching travel apps for a new cafe or restaurant to try, not only does spatial analysis underline much of the distance and location-based metrics these applications rely on, it also helps to integrate many of the novel datasets - such as traffic estimations or social media posts - that augment these distance metrics and become invaluable to our own decision-making. Applications of Spatial Analysis with ‘Data Science’ Applications A short blog piece by Esri on the insight that can be derived from spatial analysis can be found here. Spatial Analysis Software and Programming This week - and the previous - is your first introduction in our module to using R-Studio for the management and and analysis of spatial data. Prior to this, we’ve been using traditional GIS software in the form of QGIS. As we’ve suggested above, the increasing popularity of data science is having a signficant impact on how we “do” spatial anaysis, with a shift in focus to using programming as our primary tool rather than traditional GIS-GUI software. GUI-GIS software still has its place and purpose, particularly in the wider GIScience and GIS industry - but when we come to think of data science, the command line has become the default. Behind this shift in focus, alongside the need to have a tool that is capable of handling large datasets, has been a focus on improving openness and reproducibility within spatial analysis research. As Brunsdon and Comber (2020) propose: Notions of scientific openness (open data, open code and open disclosure of methodology), collective working (sharing, collaboration, peer review) and reproducibility (methodological and inferential transparency) have been identified as important considerations for critical data science and for critical spatial data science within the GIScience domains. (Brunsdon, C., Comber, A. Opening practice: supporting reproducibility and critical spatial data science. J Geogr Syst (2020). https://doi.org/10.1007/s10109-020-00334-2) As part of this move towards openness and reproducibility within spatial data science, we can look to the emerging key principles of data science research to explain why programming is becoming the primary tool for spatial analysis research. Key principles of data science research When thinking about spatial analysis, we can identify the key principles of data science as: 1. Repeatability: the idea that a given process will produce the same (or nearly the same) output given similar inputs. Instruments and procedures need to be consistent. 2. Reproducibility: There are three types of reproducibility when we think of data science research. Statistical reproducibility: an analysis is statistically reproducible when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. Empirical reproducibility: an analysis is empirically reproducible when detailed information is provided about non-computational empirical scientific experiments and observations. In practice, this is enabled by making data freely available, as well as details of how the data was collected. Computational reproducibility: an analysis is computationally reproducible if there is a specific set of computational functions/analyses (in data science, almost always specified in terms of source code) that exactly reproduce all of the results in an analysis. 3. Collaboration: an analysis workflow that is easy to share work with others and collaborate, preferably in real-time, alongside easy integration with version control. 4. Scalability: at its most basic, an analysis that can re-run the same processing easily, with simple adjustment of variables and parameters to include additional data; at an intermediate level, the analysis and workflow can be easily expanded to include larger datasets (which require more processing requirements); at the most advanced, the workflow is suitable for distributed/multiple core computing. We can use these principles to review the different tools/software available to us for spatial analysis, in order to be confident moving forward, that we use the appropriate tools for the tasks we have at hand. A Review of Spatial Analysis Software Slides | Video on Stream Spatial Analysis in R-Studio We have now seen that for us, to work towards completing spatial analysis research that adheres to these data science pricinples, we need to focus on using programming tools, such as R and R-Studio, rather than the traditional GIS GUI software. But the question is, how do we use R and R-Studio as a piece of GIS software? As you’ll already have seen, there are quite a few aesthetic differences between R-Studio and Q-GIS - for one, there is no \"map canvas area where we’ll see our data as we load it. There are also quite a few other differences in terms of how we: Load Manage Process Analyse Visualise Disseminate spatial data and our spatial analysis outputs. To help you understand these differences, the following longer lecture (approximately 40 minutes) provides you with a thorough introduction into how we use R-Studio as a GIS software: Using spatial data in R/R-Studio Slides | Video on Stream Practical 4: Analysing Crime in 2020 in London from a spatial perspective Now we’ve had our introduction to using R-Sutdio as a GIS software, it’s time to get started using it ourselves for spatial analysis. As outlined earlier, we’ll be completing an analysis of our crime dataset in London, but rather than solely looking at crime change of time - we’re going to add in a spatial component to our analysis, and understanding how crime has changed across our wards over the year. To do this, what we’ll first do is head back to our script from last week, run our script - and then write our all_theft_df to a csv file. If you had saved your environment from last week, keeping your variables in the memory, theoretically you won’t need to export the dataframe as you should have access to this variable within your new script - but it would be good practice to write out the data - and then load it back in. We’re going to be adding in and using a few additional libraries into our script today - but we’ll explain them as and when we use them; for now, just add them into our library section of our script when instructed to below. Overall, our workflow will be: Take our all_theft_df and wrangle it to produce a dataframe with a ward per row with a crime count for each month in our fields. Join this dataframe to our ward_population_2019 shapefile (in your working folder) and then produce a crime rate for each month, for each ward. Create a map for January 2020 using the tmap library. Extension: Create a new dataframe that represents crime from a quarterly perspective and create four maps ready for export. Let’s get started! Write out / export our dataframe from last week Open up R-Studio (Server or Desktop), and make sure you open up your GEOG0030 project. Next open your script from Week 4 - it should be saved as: wk4-csv-processing.r and should be visible in your files from your GEOG0030 project. First check your Environment box - if you have a variable in your Global Environment with the name all_theft_df then you do not need to run your script. If you do not have a variable saved, go ahead and run your script to and including the code that filters our large all_crime_df to only the all_theft_df: # Filter all_crime_df to contain only theft, store as a new variable: all_theft_df all_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == &#39;Theft from the person&#39;) We should all now have an all_theft_df variable in our environment that we’re ready to export to a csv. Scroll to the bottom of your Week 4 script and enter the following code and execute: # Write out the theft_crime_df to a csv within our raw crime data folder write.csv(all_theft_df,&quot;data/raw/crime/all_crime_2020.csv&quot;, row.names = FALSE) Remember, if using a Windows machine, you’ll need to submit your forward-slashes (/) with backslashes, and in this case, within R, it will need to be two backslashes (\\\\). You should now see a new csv within your raw crime data folder (data -&gt; raw -&gt; crime). Save your wk4-csv-processing.r script and then close the script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk5-crime-spatial-processing.r. At the top of your script, add the following metdata (substitute accordinlgy): # Analysing crime in 2020 by month and ward # Script started February 2021 # NAME Now let’s add all of the libraries we’ll be using today: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) As you’ll have heard in our lecture, we’ll be using sf to read and load our spatial data, use the tidyverse libraries to complete our data wrangling and then use the tmap library to visualise our final maps. The here library enables easy reference to our , whilst magrittr allows us to use the pipe function (%&gt;%) which we’ll explain in a bit more detail below. Loading our datasets We’re going to load both of the datasets we need today straight away: 1) the all_theft_2020.csv we have just exported and 2) the ward_population_2019.shp we created in Week 3. First, let’s load our all_theft_2020.csv into a dataframe called all_theft_df. You should see we use the same read_csv code as last week. For those of use with the variable still stored in your Environment, you can still add this code to your script - it will simply overwrite your current variable (which essentially stores the same data that is contained in the csv). # Read in our all_theft_2020 csv from our raw crime data folder all_theft_df &lt;- read_csv(&quot;data/raw/crime/all_crime_2020.csv&quot;) We can double-check what our csv looks like by either viewing our data or simply calling the head() function on our dataframe. + Call the `View()` function in the console or the `head()` function in the script. Call the head() function on our data to check the first five rows: # Check the first five rows of our all_theft_2020 dataframe head(all_theft_df) ## # A tibble: 6 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37c663d… 2020… City of Lo… City of Lon… -0.106 51.5 On or n… E01000916 ## 2 dcfa16f… 2020… City of Lo… City of Lon… -0.0941 51.5 On or n… E01000002 ## 3 be9310e… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 4 0cbb0c5… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 5 85df9c1… 2020… City of Lo… City of Lon… -0.0761 51.5 On or n… E01000005 ## 6 8249cc1… 2020… City of Lo… City of Lon… -0.0750 51.5 On or n… E01000005 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You should see these rows display in your console. Great, the dataset looks as we remember, with the different fields, including, importantly for this week, the LSOA_code which we’ll use to process and join our data together (you’ll see this in a second!). Next, let’s load our first ever spatial dataset into R-Studio - our ward_population_2019.shp. We’ll store this as a variable called ward_population and use the sf library to load the data: # Read in our ward_population_2019 shp from our working data folder # Note the st_read function here - keep a record of this function as it is your main function to read in shape data # Do not worry about the stringsAsFactors paratmer this week ward_population &lt;- st_read(&quot;data/working/ward_population_2019.shp&quot;, stringsAsFactors = FALSE) ## Reading layer `ward_population_2019&#39; from data source `/Users/Jo/Code/GEOG0030/data/working/ward_population_2019.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 657 features and 7 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## CRS: 27700 You should now see the ward_population variable appear in your Environment window. As this is the first time we’ve loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial data frame. Interacting with spatial data The first thing we want to do when we load spatial data is, of course, map it to see its ‘spatiality’ (I’m going to keep going with that word..) or rather how the data looks from a spatial perspective. To do this, we can use a really simple command from R’s base library: plot(). As we won’t necessarily want to plot this data everytime we run this script in the future, we’ll type this command into the console as a “one-off”. In your console, plot our new spatial data: # Plot our ward_population data plot(ward_population) You should see your ward_population plot appear in your Plots window - as you’ll see, your ward dataset is plotted ‘thematically’ by each of the fields within the dataset, including our POP2019 field we created last week. Note, this plot() function is not to be used to make maps - but simply as a quick way of viewing our spatial data. We can also find out more information about our ward_population data. Let’s next check out our class of our data. Again, in the console type: # Find out the class of our ward_population data class(ward_population) ## [1] &quot;sf&quot; &quot;data.frame&quot; We should see our data is an sf dataframe, which is great as it means we can utilise our tidyverse libraries with our ward_population. We can also use the attributes() function we looked at last week to find out a little more about our “spatial” data frame. Again, in the console type: # Find out the attributes of our ward_population data attributes(ward_population) ## $names ## [1] &quot;NAME&quot; &quot;GSS_CODE&quot; &quot;DISTRICT&quot; &quot;LAGSSCODE&quot; &quot;HECTARES&quot; ## [6] &quot;NONLD_AREA&quot; &quot;POP2019&quot; &quot;geometry&quot; ## ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 ## [253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 ## [271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 ## [289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 ## [307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 ## [325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 ## [343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 ## [361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 ## [379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 ## [397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 ## [415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 ## [433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 ## [451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 ## [469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 ## [487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 ## [505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 ## [523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 ## [541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 ## [559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 ## [577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 ## [595] 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 ## [613] 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 ## [631] 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 ## [649] 649 650 651 652 653 654 655 656 657 ## ## $class ## [1] &quot;sf&quot; &quot;data.frame&quot; ## ## $sf_column ## [1] &quot;geometry&quot; ## ## $agr ## NAME GSS_CODE DISTRICT LAGSSCODE HECTARES NONLD_AREA POP2019 ## &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## Levels: constant aggregate identity We can see how many rows we have, the names of our rows and a few more pieces of information about our ward_population data - for example, we can see that the specific $sf_column i.e. our spatial information) in our dataset is called geometry. We can investigate this column a little more by selecting this column within our console to return. In the console type: # Get info about the geometry of our ward_population data ward_population$geometry ## Geometry set for 657 features ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## CRS: 27700 ## First 5 geometries: You should see new information about our geometry column display in your console. From this selection we can find out the dataset’s: geometry type dimension bbox (bounding box) CRS (coordinate reference system) And also the first five geometries of our dataset. This is really useful as one of the first things we want to know about our spatial data is what coordinate system it is projected with. As we should know, our ward_population data was created and exported within British National Grid, therefore seeing the EPSG code of British National Grid - 27700 - as our CRS confirms to us that R has read in our dataset correctly! We could also actually find out this information using the st_crs() function from the sf library. # Find out the CRS of our ward_population data st_crs(ward_population) ## Coordinate Reference System: ## User input: 27700 ## wkt: ## PROJCS[&quot;OSGB 1936 / British National Grid&quot;, ## GEOGCS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB_1936&quot;, ## SPHEROID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## AUTHORITY[&quot;EPSG&quot;,&quot;7001&quot;]], ## TOWGS84[446.448,-125.157,542.06,0.15,0.247,0.842,-20.489], ## AUTHORITY[&quot;EPSG&quot;,&quot;6277&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4277&quot;]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;latitude_of_origin&quot;,49], ## PARAMETER[&quot;central_meridian&quot;,-2], ## PARAMETER[&quot;scale_factor&quot;,0.9996012717], ## PARAMETER[&quot;false_easting&quot;,400000], ## PARAMETER[&quot;false_northing&quot;,-100000], ## UNIT[&quot;metre&quot;,1, ## AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], ## AXIS[&quot;Easting&quot;,EAST], ## AXIS[&quot;Northing&quot;,NORTH], ## AUTHORITY[&quot;EPSG&quot;,&quot;27700&quot;]] You’ll see we actually get a lot more information about our CRS beyond simply the code using this function. This function is really important to us as users of spatial data as it allows us to retrieve and set the CRS of our spatial data (the latter is used in the case the data does not come with a .proj file but we do know what projection system should be used). To reproject data, we actually use the st_transform() function - but we’ll take a look at this in more detail in Week 7. The final thing we might want to do before we get started with our data analysis is to simply look at the data table part of our dataset, i.e. what we’d call the Attribute Table in Q-GIS, but here it’s simply the table part of our data frame. To do so, you can either use the View() function in the console or click on the ward_population variable within our enviroment. Processing our crime data to create our required output data frame Now we have our data loaded, our next step is to process our data to create what we need as our final output for analysis: a spatial dataframe that contains a theft crime rate for each ward for each month (of available data) in 2020. But wait - if we look at our all_theft_df, we do not have a field that contains the wards our crimes have occured in. We only have two types of spatial or spatially-relevant data in our all_theft_df: The approximate (“snap point”) latitude and longitude of our crime in WGS84. The Lower Super Output Area (LSOA) in which it occured. From Week 3’s practical, we know we can map our points using the coordinates and then provide a count by using a point-in-polygon (PIP) operation. However to do this for each month, we would need to filter our dataset for each month and repeat the PIP operation - when we know a little more advanced code, this might end up being quite simple, but for now, when all we’re trying to do is some basic table manipulation, surely there must be a quicker way? Adding Ward Information to our all_theft_df dataframe Yes, there is! All we need to do is figure our which Ward our LSOAs fall within and then we can add this as an additional attribute or rather column to our all_theft_df - so how do we do this? From a GIScience perspective, there are many ways to do this - but the most straight forward is to use something called a look-up table. Look-up tables are an extremely common tool in database management and programming, providing a very simple approach to storing additional information about a feature (such as a row within a dataframe) in a separate table that can quite literally be “looked up” when needed for a specific application. In our case, we will actually join our look-up table to our current all_theft_df to get this information “hard-coded” to our dataframe for ease of use. To be able to do this, we therefore need to find a look-up table that contains a list of LSOAs in London and the Wards in which they are contained. Lucky for us, after a quick search of the internet, we can find out that the Office for National Statisitcs provides this for us in their Open Geography Portal. They have a table that contains exactly what we’re looking for: Lower Layer Super Output Area (2011) to Ward (2018) Lookup in England and Wales v3. As the description on the website tells us, \"this file is a best-fit lookup between 2011 lower layer super output areas, electoral wards/divisions and local authority districts in England and Wales as at 31 December 2018. As we know we are usig - but the LSOAs are still from 2011 within the police data, we know this is the file we’ll need to complete our look-up. In addition, the description tells us what field names are included in our table, which we can have a good guess at which we’ll need before we even open the data: LSOA11CD, LSOA11NM, WD18CD, WD18NM, WD18NMW, LAD18CD, LAD18NM. (Hint, it’s the ones beginning with LSOA and WD!) We therefore have one more dataset to download and then load into R. Download the look-up table at the ONS: https://geoportal.statistics.gov.uk/datasets/8c05b84af48f4d25a2be35f1d984b883_0 Move this file in your data -&gt; raw -&gt; boundaries folder and rename to “data/raw/boundaries/lsoa_ward_lookup.csv”. Load the dataset using the read_csv() function: Do not worry if you have a few parsing failures, the table should still work fine. # Read in our lsoa_ward_lookup csv from our raw boundaries data folder lsoa_ward_lookup &lt;- read_csv(&quot;data/raw/boundaries/lsoa_ward_lookup.csv&quot;) ## Warning: 1909 parsing failures. ## row col expected actual file ## 32801 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Broncoed &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32802 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Dwyrain &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32803 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - De &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32804 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - De &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32805 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Gorllewin &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## ..... ....... .................. ....................... .......................................... ## See problems(...) for more details. Now we have our lookup table, all we are going to do is extract the relevant ward name and code for each of the LSOAs in our all_theft_df. To do so, we’re going to use one of the join functions from the dplyr library. Joining data by fields in programming We’ve already learnt how to complete Attribute Joins in Q-GIS via the Joins tab in the Propeties window - so it should come of no surprise that we can do exactly the same process within R. To conduct a Join between two dataframes (spatial or non-spatial, it does not matter), we use the same principles of selecting a unique but matching field within our dataframes to join them together. As we have seen from the list of fields above - and with our knowledge of our all_theft_df dataframe - we know that we have at least two fields that should match across the datasets: our lsoa codes and lsoa names. We of course need to identify the precise fields that contain these values in each of our data frames, i.e. LSOA11CD and LSOA11NM in our lsoa_ward_lookup dataframe and lsoa_code and lsoa_name in our all_theft_df dataframe, but once we know what fields we can use, we can go ahead and join our two data frames together. But how do we go about join them in R? Within R, you have two options to complete a data frame join: The first is to use the Base R library and its merge() function: By default the data frames are merged on the columns with names they both have, but you can also provide the columns to match separate by using the parameters: by.x and by.y. E.g. your code would look like: merge(x, y, by.x = \"xColName\", by.y = \"yColName\"), with x and y each representing a dataframe. The rows in the two data frames that match on the specified columns are extracted, and joined together. If there is more than one match, all possible matches contribute one row each, but you can also tell merge whether you want all rows, including ones without a match, or just rows that match, with the arguments all.x and all. The second option is to use the Dplyr library and one of their mutate()-based join() functions: dplyr uses SQL database syntax for its join functions. There are four types of joins possible (using this SQL syntax) with the dplyr library. inner_join(): includes all rows in x and y. left_join(): includes all rows in x. right_join(): includes all rows in y. full_join(): includes all rows in x or y. Figuring out which one you need will be on a case by case basis. Again, if the join columns have the same name, all you need is left_join(x, y). If they don’t have the same name, you need a by argument, such as left_join(x, y, by = c(“xName” = “yName”)) . Note the syntax for the by parameter in the dplyr: you submit only the column name you’re interested in (e.g. LSOACD) but within qutation marks (i.e. “LSOACD”). Left of the equals is the column for the first data frame, right of the equals is rthe name of the column for the second data frame. So which approach should I choose? In all cases moving forward, we will use the one of the dplyr join approaches. There are three reasons for using the dplyr approach: The base merge() function does not always work well with data frames and can create errors in your joining. With the dplyr code built on SQL, joins run substantially faster and very well on dataframes. All tidyverse functions use NAs as a part of data, because it should explain some aspects of information that can’t be explained by “identified” data and will not drop NAs during processing, which, if this happens without your realisation, can affect your data and its reliability quite substantially. When using the tidyverse, we often need to use a specific function to drop NA values, e.g. na.omit() or find ways of replacing NAs, as we’ll see later. One thing to note is that there is a new package entering the “game” of data wrangling in R, called data.table. We won’t look into this package now, because its best suited for really large datasets but one to quickly make a note about if you end up dealing with datasets for your dissertations that have millions of entries. Joining our two tables using the left_join() function from dplyr Now we know what set of functions we can use to join our data, let’s go ahead and join our lsoa_ward_lookup dataframe to our all_theft_df dataframe so we can get our ward information. We’re going to need to make multiple joins between our tables as we have multiple entries of crime for the same LSOA - as a result, we’re going to need to use a function that allows us to keep all rows in our all_theft_df dataframe, but we do not need to keep all rows in our lsoa_ward_lookup if those wards are not within our dataset. Let’s have a look in detail at how the four different types of joins from dplyr work: It looks like we’re going to need to use our left_join() function as we want to join matching rows from our lsoa_ward_lookup dataframe to our all_theft_df dataframe but make sure to keep all rows in the latter. Within your script, create a join between our two dataframes and store as a new variable: # Join lsoa_ward_lookup rows to the all_theft_df on our two lsoa code fields # Note again how we state to the two fields we&#39;ll use in the join in the by parameter all_theft_ward_df &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;)) Run the code. Let’s go ahead and check our join - we want to check that our LSOA codes and names match across our new dataframe. In your console, check the first five rows of our new data frame: # Check our join via the first five rows head(all_theft_ward_df) ## # A tibble: 6 x 19 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37c663d… 2020… City of Lo… City of Lon… -0.106 51.5 On or n… E01000916 ## 2 dcfa16f… 2020… City of Lo… City of Lon… -0.0941 51.5 On or n… E01000002 ## 3 be9310e… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 4 0cbb0c5… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 5 85df9c1… 2020… City of Lo… City of Lon… -0.0761 51.5 On or n… E01000005 ## 6 8249cc1… 2020… City of Lo… City of Lon… -0.0750 51.5 On or n… E01000005 ## # … with 11 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;, LSOA11NM &lt;chr&gt;, WD18CD &lt;chr&gt;, ## # WD18NM &lt;chr&gt;, WD18NMW &lt;lgl&gt;, LAD18CD &lt;chr&gt;, LAD18NM &lt;chr&gt;, FID &lt;dbl&gt; You should now see that you have with 19 variables: 12 from all_theft_df, plus 7 from lsoa_ward_lookup. Note, the join does not keep the ‘join key’ fields from both dataframes by default. It keeps only the field from the “left” dataframe - hence we are now missing LSOA11CD. To keep both fields in future, we would need to add the keep parameter into our code, and set this to TRUE as so: Do not add this to your script, it is just provided as an example! # Join lsoa_ward_lookup rows to the all_theft_df on our two lsoa code fields # Set keep to TRUE to keep both join key fields all_theft_ward_df &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;), keep = TRUE ) Now we have our joined dataset, we can move forward with some more data wrangling. The thing is, our data frame is getting quite busy - we have duplicate fields and some fields we just won’t need. It would be good if we could trim down our dataframe to only the relevant data that we need moving forward, particularly, for example, if we wanted to go ahead and write out a hard copy of our theft data that now contains the associated ward. To be able to “trim” our data frame, we have two choices in terms of the code we might want to run. First, we could look to drop certain columns from our data frame. Alternatively, we could create a subset of the columns we want to keep from our data frame and store this as a new variable or simply overwrite the currently stored variable. To do either of these types of data transformation, we need to know more about how we can interact with a data frame in terms of indexing, selection and slicing. Data Wrangling: Introducing Indexing, Selection and Slicing Everything we will be doing today as we progress with our data frame cleaning and processing relies on us understanding how to interact with and transform our data frame - this interaction itself relies on knowing about how indexing works in R as well as how to select and slice your data frame to extract the relevant cells, rows or columns and then manipulate them - as we’ll be doing in this practical. Whilst there are traditional programming approaches to this using the base R library, dplyr is making this type of data wrangling easier by the year! If you’ve not used R before - or have but not familiar with how to index, select and slice, I would highly recommend watching this following video that explains the process from both a base R perspective and using the dplyr library - it also includes a good explanation about what our pipe function , %&gt;% , does. I’d love to have time to make this video for you all myself, but this is currently not possible - and this video provides a very accessible explanation. I’ll add some detailed notes as and when we use these functions in the next section of the practical, but for an audio/visual explanation, I’d highly recommend watching this video. Selection and slicing in R As you can see from the video, there are two common approaches to selection and slicing in R, which rely on indexing and/or field names in different ways. The following summarises the above video, for ease of reference during the practical: Base R approach to selection and slicing (common programming approach) The most basic approach to selecting and slicing within programming relies on the principle of using indexes within our data structures. Indexes actually apply to any type of data structure, from single atomic vectors to complicated data frames as we use here. Indexing is the numbering associated with each element of a data structure. For example, if we create a simple vector that stores three strings: # Store a simple vector of three strings simple_vector &lt;- c(&quot;Aa&quot;, &quot;Bb&quot;, &quot;Cc&quot;, &quot;Dd&quot;, &quot;Ee&quot;, &quot;Ff&quot;, &quot;Gg&quot;) R will assign each element (i.e. string) within this simple vector with a number: Aa = 1, Bb = 2, Cc = 3 and so on. Now we can go ahead and select each element by using the base selection syntax which is using square brackets after your element’s variable name, as so: # Select the first element of our variable ss simple_vector[1] ## [1] &quot;Aa&quot; Which should return the first element, our first string containing “Aa”. You could change the number in the square brackets to any number up to 7 and you would return each specific element in our vector. However, say you don’t want the first element of our vector but the second to fifth elements. To achieve this, we conduct what is known in programming as a slicing operation, where, using the [] syntax, we add a : (colon) to tell R where to start and where to end in creating a selection, known as a slice: # Select the second to fifth element of our vector, creating a &#39;slice&#39; of our vector simple_vector[2:5] ## [1] &quot;Bb&quot; &quot;Cc&quot; &quot;Dd&quot; &quot;Ee&quot; You should now see our 2nd to 5th elements returned. You’ve created a slice! Now what is super cool about selection and slicing is that we can add in a simple - (minus) sign to essentially reverse our selection. So for example, we want to return everything but the 3rd element: # Select everything but the third element of our vector simple_vector[-3] ## [1] &quot;Aa&quot; &quot;Bb&quot; &quot;Dd&quot; &quot;Ee&quot; &quot;Ff&quot; &quot;Gg&quot; And with a slice, we can use the minus to slice out parts of our vector, for example, remove the 2nd to the 5th elements (note the use of a minus sign for both): # Select the second to fifth element of our vector, creating a &#39;slice&#39; of our vector simple_vector[-2:-5] ## [1] &quot;Aa&quot; &quot;Ff&quot; &quot;Gg&quot; This use of square brackets for selection syntax is common across many programming languages, including Python, but there are often some differences you’ll need to be aware of if you pursue other languages. For example: Python always starts its index from 0! Whereas we can see here with R, our index starts at 1 R is unable to index the characters within strings - this is something you can do in Python, but in R, we’ll need to use a function such as substring() - more on this another week. But ultimately, this is all there is to selection and slicing - and it can be applied to more complex data structures, such as data frames. Let’s take a look. Selection and slicing in data frames We apply these selection techniques to data frames, but we will have a little more functionality as our data frames are made from both rows and columns. This means when it comes to selections, we can utilise an amended selection syntax that follows a specific format to select individual rows, columns, slices of each, or just a single cell: [ rows, columns] There are many ways we can use this syntax, which we’ll example below using our lsoa_ward_lookup data frame. First, before looking through and executing these examples (in your console) familiarise yourself with the lsoa_ward_lookup data frame: # View lsoa_ward_lookup dataframe, execute this code in your console View(lsoa_ward_lookup) To select a single column from your data frame, you can use one of two approaches. First we can follow the syntax above carefully and simply set our column parameter in our syntax above to the number 2: # Select the 2nd column from the data frame, Opt 1 lsoa_ward_lookup[,2] You should see your second column display in your console. Second, we can actually select our column by only typing in the number (no need for the comma). By default, when there is only one argument present in the selection brackets, R will select the column from the data frame, not the row: # Select the 2nd column from the data frame, Opt 2 lsoa_ward_lookup[2] Note, this is different to when we “accessed” the properties of the column last week using the $ syntax - we’ll look at how we use this in later practicals. To select a specific row, we need to add in a comma after our number - this will tell R to select the relevant row instead: # Select the 2nd row from the data frame lsoa_ward_lookup[2,] You should see your second row appear. Now, to select a specific cell in our data frame, we simply provide both arguments in our selection parameters: # Select the value at the 2nd row and 2nd column in the data frame lsoa_ward_lookup[2,2] What is also helpful in R is that we can select our columns by their field names by passing these field names to our selection brackets as a string. For a single column: # Select the LSOA11NM column (2nd column) by name lsoa_ward_lookup[&quot;LSOA11NM&quot;] Or for many columns, we can supply a combined vector: # Select the LSOA11CD (1st column) and LSOA11NM column (2nd column) by name lsoa_ward_lookup[c(&quot;LSOA11CD&quot;, &quot;LSOA11NM&quot;)] This approach to selecting multiple columns is also possible using the indexing, but in this case we use the slicing approach we saw earlier (note, you cannot slice using field names but need to provide each individual field name within a vector as above). To retrieve our 2nd - 4th columns in our data frame, we can use either approach: # Select the 2nd to 4th columns from our data frame lsoa_ward_lookup[2:4] # Does the same thing: # lsoa_ward_lookup[,2:4] We can also apply the negative # Select everything but the 2nd to 4th columns from our data frame lsoa_ward_lookup[-2:-4] If you do not want a slide, we can also provide a combined list of the columns we want to extract: # Select the 2nd, 3rd, 4th and 7th columns from our data frame lsoa_ward_lookup[c(2, 3, 4, 7)] We can apply this slicing approach to our rows: # Select the 2nd to 4th rows from our data frame lsoa_ward_lookup[2:4,] As well as a negative selection: # Select everything but the 2nd to 4th rows from our data frame lsoa_ward_lookup[-2:-4,] (Note we have fewer rows than we should in the original data frame!) And if it’s not a slice you want to achieve, you can also provide a list of the rows (akin to our approach with the columns above)! And of course, for all of these, we can store the output of our selection as a new variable or pipe it to another function. That’s obviously what makes selection and slicing so useful - however it can be at times a little confusing. Dplyr approach to selection and slicing (making our lives easy!) We’re quite lucky, therefore, as potential data wranglers that the dplyr library has really tried to make this more user-friendly. Instead of using the square brackets [] syntax, we now have functions that we can use to select or slice our data frames accordingly: For columns, we use the select() function that enables us to select a (or more) column(s) using their column names or a range of “helpers” such as ends_with() to select specific columns from our data frame. For rows, we use the slice() function that enables us to select a (or more) row(s) using their position (i.e. similar to the proess above) For both functions, we can also use the negative / - approach we saw in the base R approach to “reverse a selection”, e.g.: # A few Dplyr examples in one! # Select column 2 select(lsoa_ward_lookup, 2) # Select everything but column 2 select(lsoa_ward_lookup, -2) # Select LSOA11CD column, note no &quot;&quot; select(lsoa_ward_lookup, LSOA11CD) # Select everything but column 2 select(lsoa_ward_lookup, -LSOA11CD) # Select LSOA... columns select(lsoa_ward_lookup, starts_with(&quot;LSOA&quot;)) # Select everything but column 2 select(lsoa_ward_lookup, -LSOA11CD) We’ll be using these functions throughout our module, so we’ll leave our examples there for now! In addition to these index-based functions, within dplyr, we also have: filter() that enables us to easily filter rows within our data frame based on specific conditions (such as being a City of London ward). This function requires a little bit of SQL knowledge, which we’ll pick up on throughout the module - but look further at in Week 6. Image: Allison Horst In addition, dplyr provides lots of functions that we can use directly with these selections to apply certain data wrangling processes to only specific parts of our data frame, such as mutate() or count(). We’ll be using quite a few of these functions in the remaining data wrangling section below - plus throughout our module, so I highly recommend downloaded (and even printing off!) the dplyr cheat sheet to keep track of what functions we’re using and why! One thing to note is that in either the base R or dplyr approach, we can use the magrittr pipe - %&gt;% - to ‘pipe’ the outputs of our selection into another function. This is explained in more detail in another section. As we have seen above, whilst there are two approaches to selection using either base R library or the dplyr library, we will continue to focus on using functions directly from the dplyr library to ensure efficiently and compatibility within our code. Within dplyr, as you also saw, whether we want to keep or drop columns, we always use the same function: select. To use this function, we provide our function with a single or “list” (not a programmatical list, just a list) of the columns we want to keep - or if we want to drop them, we use the same approach, but add a - before our selection. (We’ll use this drop approach in a litte bit). Let’s see how we can extract just the relevant columns we will need for our future analysis - note, in this case we’ll overwrite our all_theft_ward_df variable. In your script, add the following code to extract only the relevant columns we need for our future analysis: # Reduce our data frame using the select function all_theft_ward_df &lt;- select(all_theft_ward_df, crime_id, month, longitude, latitude, lsoa_name, lsoa_code, crime_type, WD18CD, WD18NM) You should now see that your all_theft_ward_df data frame should only contain 9 variables - you can go and view this data frame or call the head() function on the data in the console if you’d like to check out this new formatting. Improving efficiency in our code Our current workflow looks good - we now have our data frame ready for use in wrangling… but wait, could we not have made this whole process a little more efficient? Well, yes! There is a quicker way - particularly if I’m not writing out explanations to you to read through - but generally, yes, we coud make our code way more “speedy” by using the pipe function, %&gt;%, introduced above, which for those of you that remember, we used in our work last week. As explained above, a pipe is used to pipe the results of one function/process into another - when “piped”, we do not need to include the first “data frame” (or which data structure you are using) in the next function. The pipe “automates” this and pipes the results of the previous function straigt into this function. It might sound a little confusing at first, but once you start using it, it really can make your code quicker and easier to write and run - and it stops us having to create lots of additional variables to store outputs along the way. It also enabled the code we used last week to load/read all the csvs at once - without the pipe, that code breaks! Let’s have a think through what we’ve just achieved through our code, and how we might want to re-write our code. In our workflow, we have: Joined our two data frames together Remove the columns not needed for our future analysis Let’s see how we can combine this process into a single line of code: Option 1: Original code, added pipe # Join dataframes, then select only relevant columns all_theft_ward_df_speedy_1 &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;)) %&gt;% select(crime_id, month, longitude, latitude, lsoa_name, lsoa_code, crime_type, WD18CD, WD18NM) You should see that we now end up with a data frame akin to our final output above - the same number of observations and variables, all from one line of course. We could also take another approach in writing code, by completing our selection prior to our join, which would mean having to write out fewer field names when piping this output into our join. Let’s have a look: Option 2: New code - remove columns first # Select the relevant fields from the lookup table, then join to dataframe all_theft_ward_df_speedy_2 &lt;- select(lsoa_ward_lookup, LSOA11CD, WD18CD, WD18NM) %&gt;% right_join(all_theft_df, by = c( &quot;LSOA11CD&quot; = &quot;lsoa_code&quot;)) You’ll see in this approach, we now have 14 variables instead of the 9 as we haven’t really “cleaned” up the fields from the original all_theft_df - we could drop these fields by piping our output into another select() function, but we may end up creating even more coding work for ourselves this way. What these two options do show is that there are multiple ways to achieve the same output, using similar code - we just need to always think through what outputs we want to use. Pipes help us improve the efficiency of our code - the one thing however to note in our current case is that by adding the pipe, we would not be able to check our join prior to the selection - so sometimes, it’s better to add in this efficiency, once you’re certain that your code has run correctly. For now, **we’ll keep our original all_theft_ward_df data frame that you would have created prior to this info box - but from now on, we’ll use pipes in our code when applicable. Go ahead and remove the speedy variables from your environment: rm(all_theft_ward_df_speedy_1, all_theft_ward_df_speedy_2). We now FINALLY have our dataset for starting our last bit of data wrangling: aggregating our crime by ward for each month in 2020. Aggregate crime by ward and by month To aggregate our crime by ward for each month in 2020, we need to use a combination of dplyr functions. First, we need to group our crime by each ward and then count - by month - the number of thefts occuring in each ward. To do so, we’ll use the group_by() function and the count() function. The group_by() function creates a “grouped” copy of the table (in memory) - then any dplyr function used on this grouped table will manipulate each group separately (i.e. our count by month manipulation) and then combine the results to a single output. If we solely run the group_by() function, we won’t really see this effect on its own - instead we need to add our summarising function -in our case the count() function, which \"counts the number of rows in each group defined by the variables provided within the function, in our case, month. Let’s see this in action: Pipe our grouped table into the count function to return a count of theft per month for each Ward in our all_theft_ward_df data frame: # Group our crimes by ward, then count the number of thefts occuring in each month theft_count_month_ward &lt;- group_by(all_theft_ward_df, WD18CD) %&gt;% count(month) To understand our output, go ahead and View() the variable. We have 3 fields - with 4490 rows. You should see that we’ve ended up with a new table that lists each ward (by the WD18CD column) eleven times, to detail the number of thefts for each month - with the months represented as a single field. But does this table adhere to the Tidyverse principles we read about this and last week? Not really - whilst it is just about usable for a statistical analysis - if we think about joining this data to our ward_population dataset, we are really going to struggle to add each monthly count of crime in this format. What we would really prefer is to have our **crime count detailed as one field for each individual month, i.e. 2020-01 as a single field, then 2020-02, etc. To do this, we need to figure out how to transform our data to present our months as fields - and yes, before you even have a chance to guess it, the Tidyverse DOES have a function for that! Do you see why using the Tidyverse is an excellent choice to our R-Studio learning… ;) This time, we look to the tidyr library which has been written to quite literally: “help to create tidy data, where each column is a variable, each row is an observation, and each cell contains a single value. ‘tidyr’ contains tools for changing the shape (pivoting) and hierarchy (nesting and ‘unnesting’) of a dataset, turning deeply nested lists into rectangular data frames (‘rectangling’), and extracting values out of string columns. It also includes tools for working with missing values (both implicit and explicit).” tidyr documentation And even in our explanation of the tidyr library, we may have found our solution in tools for changing the shape (pivoting). To change the shape of our data, we’re going to need to use tidyr’s pivot functions. Note, do not get confused here with the traditional sense of pivot in data processing in terms of pivot tables. If you’ve never use a pivot table before in a spreadsheet document (or R-Studio for that matter), they are primarily used to summarizes the data of a more extensive table. This summary might include sums, averages, or other statistics, which the pivot table groups together in a meaningful way. In our case, the application of the word pivot is not quite the same - here, our pivot() functions will change just the shape of our data (and not the values). In the tidyr library, we have the choice of two pivot() functions: pivot_longer() or pivot_wider(). pivot_wider() “widens” data, increasing the number of columns and decreasing the number of rows. pivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns. Well, our data is already pretty long - and we know we want to create new fields representing our months, so I think we can make a pretty comfortable guess that pivot_wider() is the right choice for us. We just need to first read through the documentation to figure out what parameters we need to use and how. Type ?pivot_wider into the console. You should now see the documentation for the function. We have a long list of parameters we may need to use with the function - but we need to figure out what we need to use to end up with the data frame we’re looking for from our data: If we read through the documentation, we can figure our that our two parameters of interest are the names_from and values_from fields. We use the names_from parameter to set our month column as the column from which to derive ouput fields from, and the values_from field as our n field (count field) to set our values. As we do not have a field that uniquely identifies each of our rows, we can not use the id_cols parameter. We will therefore need to state the parameters in our code to make sure the function reads in our fields for the right parameter. Pivot our data “wider” to create a theft by ward by month data frame: # Read in our lsoa_ward_lookup csv from our raw boundaries data folder theft_by_ward_month_df &lt;- pivot_wider(theft_count_month_ward, names_from = month, values_from = n) Have a look at the resulting data frame - does it look like you expect? Trial and error your code When you come across a new function you’re not quite sure how to use, I can highly recommend just trialling different inputs for your parameters until you get the output right. To do this, just make sure you don’t overwrite any variables until you’re confident the code work. In addition, always make sure to check your output against what you’re expecting. In our case here, we can check our original theft_count_month_ward data frame values against the resulting theft_by_ward_month_df dataframe - for example, do we see 30 thefts in January for ward E05000026? As long as you do, we’re ready to move forward with our processing. One final thing we want to do is clean up the names of our fields to mean a little more to us. Let’s transform our numeric dates to text dates (and change our WD18CD in the process). Rename our field names for our theft_by_ward_month_df data frame: # Read in our lsoa_ward_lookup csv from our raw boundaries data folder names(theft_by_ward_month_df) &lt;- c(&#39;ward_code&#39;, &#39;jan_2020&#39;, &#39;feb_2020&#39;, &#39;mar_2020&#39;, &#39;apr_2020&#39;, &#39;may_2020&#39;, &#39;jun_2020&#39;, &#39;jul_2020&#39;, &#39;aug_2020&#39;, &#39;sept_2020&#39;, &#39;oct_2020&#39;, &#39;nov_2020&#39;) And we’re now done! We have our final data frame to join to our ward_population spatial data frame. Excellent! Let’s just do one final bit of data management and write out this completed theft by ward by month table to a new csv for easy reference/use in the future. Write out completed theft table to a new csv file for future reference: # Write out the theft_crime_df to a csv within our working data folder write.csv(theft_by_ward_month_df, &quot;data/working/theft_by_ward_per_month_2020.csv&quot;, row.names = FALSE) Join our theft data frame to our ward population data frame We’re now getting to the final stages of our data processing - we just need to join our completed theft table, theft_by_ward_month_df to our ward_population spatial data frame and then compute a theft crime rate. This will then allow us to map our theft rates per month by ward - exactly what we wanted to achieve within this practical. Luckily for us, the join approach we used earlier between our all_theft_df and our lsoa_ward_lookup is the exact same approach we need for this, even when dealing with spatial data. Let’s go ahead and use the same left_join function to join our two data frames together - in this case, we want to keep all rows in our ward_population spatial data frame, so this will be our x data frame, whilst the theft_by_ward_month_df will be our y. Join our two data frames together, using our respective ward code fields to join the data: # Join theft by month to the correct wards in our ward_population data frame all_theft_ward_sdf &lt;- left_join(ward_population, theft_by_ward_month_df, by = c(&quot;GSS_CODE&quot; = &quot;ward_code&quot;)) To double-check our join, we want to do one extra step of “quality assurance” - we’re going to check that each of our wards has at least one occurence of crime over the eleven months. We do this by computing a new column that totals the number of thefts over the 11 month period. By identifying any wards that have zero entries (i.e. NAs for each month), we can double-check with our original theft_by_ward_month_df to see if this is the correct “data” for that ward or if there has been an errors in our join. We should actually remember from last week, that only those wards in the City of London (that are to be omitted from the analysis) should have a total of zero. We can compute a new column by using the mutate() function from the dplyr library. We use the rowsums() function from the base library to compute the sum of rows, which we use the across() function from the dplyr library to parse. This code is actually a little complicated - and not wholly straight-foward to identify from reading through dplyr documentation alone. And believe it or not, I do not know every single function available within our various R libraries - so how did I figure this out? Well, just through simple searching - it might take a few attempts to find the right solution, but the great thing about programming is that you can try things out easily and take steps back. You can find the original post where I found this code on Stack Overflow and what you’ll notice is that there is a variety of answers to try - and believe me, I certainly did! Luckily the final answer provided a good solution to what we needed. Summarise all thefts for each ward by computing a new totals column using the mutate() and rowsums() functions: # Total number of thefts for each ward, create new column all_theft_ward_sdf &lt;- all_theft_ward_sdf %&gt;% mutate(theft_total = rowSums(across(8:18), na.rm = T)) You can now View() our updated all_theft_ward_sdf spatial data frame - and sort out columns to see those with a theft_total of 0. What you should see is that we have approximately 20 City of London wards without data, whilst we do indeed have 10 additional wards without data. The question is why? Do we have errors in our processing that we need to investigate? Or do these areas simply have no theft? If we had not complete this analysis in Q-GIS prior to this week’s practical, we would need to conduct a mini-investigation into the original theft dataset and search for these individual wards within the dataset to confirm to ourselves that they are not present within this original dataset. Luckily, having done the practical two weeks before, I can very much confirm that these wards do not have any records of thefts within them. We can therefore move forward with our dataset as it is, but what we’ll need to do is adjust the values present within these wards prior to our visualisation analysis - these should not have “NA” as their value but rather 0. In comparison our City of London wards should only contain “NAs”. To make sure our data is as correct as possible prior to visualisation, we will remove our City of London wards that do not have any data (crime or population), and then convert the NAs in our theft counts to 0. Filter out the City of London wards with a theft count of 0 and then replace the NAs in our theft columns with 0. # Filter out City of London wards with a crime count of 0 or a population of 0 # Note the logic is a little complicated here to achieve the above filter all_theft_ward_sdf &lt;- filter(all_theft_ward_sdf, theft_total &gt; 0 | DISTRICT != &quot;City and County of the City of London&quot;) # We&#39;re also going to remove the ward of Vintry, which whilst it has a positive crime count, it does not contain a population # I only realise this at the end of the practical, therefore it&#39;s added as a single line of code here! all_theft_ward_sdf &lt;- filter(all_theft_ward_sdf, NAME != &quot;Vintry&quot;) # Replace all NAs in our data frame with 0 all_theft_ward_sdf[is.na(all_theft_ward_sdf)] = 0 The final thing we need to do before we can map our theft data is, of course, compute a crime rate per month for our all_theft_ward_sdf data frame. We have our POP2019 column within our all_theft_ward_sdf data frame - we just need to figure out the code that allows us to apply our calculation that we’ve used in our previous practicals (i.e. using the Attribute/Field Calculator in Q-GIS: value/POP2019 * 10000) to each of our datasets. Once again, after a bit of searching, we can find out that the mutate() function comes in handy - and we can follow a specific approach in our code that allows us to apply the above equation to all of our columns within our data frame. Now this is certainly a big jump in terms of complexity of our code - below, we are going to store within our crime_rate variable our own function that calculates crime rate on a given value, currently called x, but will be (through our second line our code) each individual cell within our all_theft_ward_sdf spatial data frame contained within our month columns (using the mutate_at() function). How this code works - for now - is not something you need to worry about too much, but it shows you that a simple task that we completed easily in Q-GIS can, actually, be quite complicated when it comes to writing code. What is great is that you now have this code that you’ll be able to refer to in the future if and when you need it - and you can of course trial and error different calculations to include with the function. For now, let’s get on with calculating our theft crime rate. We’re going to create a new dataframe to store our crime rate as when we apply our calculation to our current data frame, we are actually transforming the original values for each month and not creating a new column per se for each month. Create a new data frame to store the crime rate for each ward for each month. # Create a new function called crime rate, which takes an argument, x, and the following calculation # The calculation to pass x through is equal to ( x / POP2019) * 10 000) crime_rate = function(x, na.rm = FALSE) ((x/all_theft_ward_sdf$POP2019)*10000) # Apply this calculation to all columns between 8 and 18 within the all_theft_ward_sdf and transform the values theft_crime_rate_sdf &lt;- mutate_at(all_theft_ward_sdf, vars(8:18), crime_rate) Have a look at your new theft_crime_rate_sdf spatial data frame - does it look as you would expect? Complexities of coding These last few chunks of code are the most complicated pieces of code we have come across so far in this module - and not something I would expect you to be able to write on your own. And to be honest, neither have I. Much of programming is figuring out what you need to do - trying out different approaches and if you get stuck, searching online for solutions - and then copy and pasting! You then use trial and error to see if these solutions work - and if not, find a new option. What is important is to recognise what inputs and outputs you need for the functions you are using - and starting from there. This is only knowledge you’ll gain from programming more, so do not worry at this stage if this feels a little overwhelming, because it will. Just keep going and you’ll find in six weeks time, you’ll be able to re-read the code above and make a lot more sense out of it! Now we have our final data frame, we can go ahead and make our maps. Making Maps in R-Studio: Grammar of Graphics Phew - we are so nearly there - as we now have our dataset ready, we can start mapping. This week, we’re going to focus on using only one of the two visualisation libraries mentioned in the lecture - and we’ll start with the easiest: tmap. tmap is a library written around thematic map visualisation. The package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. It is also based on the grammar of graphics, and resembles the syntax of ggplot2 and so provides a reasonable introduction into understanding how to make maps in R. What is really great about tmap is that it comes with one quick plotting method for a map called: qtm - it quite literally stands for quick thematic map. We can use this function to plot the theft crime rate for one of our months really really quickly. Let’s create a crime rate map for January 2020. Within your script, use the qtm function to create a map of theft crime rate in London in January 2020. # Use qtm, pass our theft_crime_rate_sdf as our data frame, and then the jan-2020 column as our fill argument qtm(theft_crime_rate_sdf, fill=&quot;jan_2020&quot;) In this case, the fill argument is how we tell tmap to create a choropleth map based on the values in the column we provide it with - if we simply set it to NULL, we would only draw the borders of our polygon (you can try this out in your console if you’d like). Within our qtm function, we can pass quite a few different parameters that would enable us to change specific aesthetics of our map - if you go ahead and search for the function in the Help window, you’ll see a list of these parameters. We can, for example, set the lines of our ward polygons to white by adding the borders parameter. Update our map to contain white borders for our ward polygons: # Use qtm, pass our theft_crime_rate_sdf as our data frame, and then the jan-2020 column as our fill argument # Add the borders parameter and set to white # Note colour based parameters can take words or HEX codes qtm(theft_crime_rate_sdf, fill=&quot;jan_2020&quot;, borders = &quot;white&quot;) Yikes - that doesn’t look great! But at least we tried to change our map a little bit. Setting colours in R Note, when it comes to setting colours within a map or any graphic (using ANY visualisation library), we can either pass through a colour word, a HEX code or a pre-defined palette when it comes to graphs and maps. You can find out more here, which is a great quick reference to just some of the possible colours and palettes you’ll be able to use in R but we’ll look at this in more detail in the second half our module. For now, you can use the options I’ve chosen within my maps - or if you’d like, experiment a little bit and see what works! We can continue to add and change parameters in our qtm function to create a map we are satisfied (we just need to read the documentation to figure out what parameters do what). The issue with the qtm function is that it is extremely limited in its functionality to: Change the classification breaks used within the Fill parameter Add additional data layers, such as an underlying ward polygon layer to show our City of London wards that are missing. Instead, when we want to develop more complex maps using the tmap library, we want to use their main plotting method which uses a function called tm_shape(), which we build on using the ‘layered grammar of graphics’ approach. Using the `tm_shape() function and the “grammar of graphics” The main approach to creating maps in tmap is to use the “grammar of graphics” to build up a map based on what is called the tm_shape() function. Essentially this function, when populated with a spatial data frame, takes the spatial information of our data (including the projection and geometry/“shapes” of our data) and creates a spatial “object”. This object contains some information about our original spatial data frame that we can override (such as the projection) within this function’s parameters, but ultimately, by using this function, you are instructing R that this is the object from which to “draw my shape”. But to actually draw the shape, we next need to add a layer to specify the type of shape we want R to draw from this information - in our case, our polygon data. We need to add a function therefore that tells R to “draw my spatial object as X” and within this “layer”, you can also specific additional information to tell R how to draw your layer. You can then add in additional layers, including other spatial objects (and their related shapes) that you want drawn on your map, plus a specify your layout options through a layout layer. Hence the “layered” approach of making maps mentioned in the lecture. This all sounds a little confusing - and certainly not as straight-forward as using the Print Layout on Q-GIS. However, as with Everything In Programming, the more times you do something, the clearer and more intuitive it becomes. For now, let’s see how we can build up our first map in tmap. Building a map: theft in January 2020 To get started with making a map, we first need to specify the spatial object we want to map. In our case, this is our theft_crime_rate_sdf spatial data frame, so we set this to our tm_shape() function. However, on it’s own, if you try, you’ll see that we have “no layer elements defined after tm_shape”. For the following lines of code, I want you to build on the first line by adding the extra pieces of code I’ve added at each step. DO NOT duplicate the entire code at each step (i.e. copy and paste below one another!). In the end you only want ONE CHUNK of code that plots our map. Set our tm_shape() equal to our theft_crime_rate_sdf spatial data frame. Execute the line of code and see what happens: # Set our tm_shape equal to our spatial data frame tm_shape(theft_crime_rate_sdf) We therefore need to tell R that we want to map this object as polygons. To do so we use the tm_polygons() function and add this function as a layer to our spatial object by using a + sign: Add our + tm_polygons() layer and execute: # Draw our spatial object as polygons tm_shape(theft_crime_rate_sdf) + tm_polygons() As you should now see, we have now mapped the spatial polygons of our theft_crime_rate_sdf spatial data frame - great! A step in the right direction. However, this is not the map we want - we want to have our polygons represented by a choropleth map where the colours reflect the theft crime rate in January, rather than the default grey polygons we see before us. To do so, we use the col= parameter that is within our tm_polygons() shape. The col= parameter is used to “fill” our polygons with a specific fill type, of either: a single color value (e.g. “red”) the name of a data variable that is contained in shp. Either the data variable contains color values, or values (numeric or categorical) that will be depicted by a specific color palette. In the latter case, a choropleth is drawn. “MAP_COLORS”. In this case polygons will be colored such that adjacent polygons do not get the same color. (This information is all extracted from the `tm_polygons() documentation - see how important the Help window is in R!) Let’s go ahead and pass our jan_2020 column within the col= parameter and see what we get. Add col = \"jan_2020\" to our tm_polygons() function as a parameter: # Map our theft_crime_rate_sdf using the Jan 2020 column tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;) Awesome! It looks like we have a choropleth map. We are slowly getting there. But there are two things we can notice straight away that do not look right about our data. The first is that our classification breaks do not really reflect the variation in our dataset - this is because tmap has defaulted to its favourite break type: pretty breaks, whereas, as we know, using an approach such as natural breaks, aka jenks, may reveal better variation in our data. So how do we state our classification breaks in tmap? To figure this out, once again, we need to visit the documentation for tm_polygons() and read through the various parameters to find out what we might need… ..Hmm, if we scroll through the parameters the three that stick out are: n, style, and breaks. It seems like these will help us create the right classfication for our map: n: state the number of classification breaks you want style: state the style of breaks you want, e.g. “cat”, “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, and “log10_pretty”. breaks: state the numeric breaks you want to use when using the fixed style approach. There are some additional parameters in there that we might also want to consider, but for now we’ll focus on these three and specifically the first two today. Let’s say we want to change our choropleth map to have 5 classes, determined via the jenks method - we simply need to add the n and style parameters into our tm_polygons() layer. Add the n and style parameters into our tm_polygons() layer. Note we pass the jenks style as a string. # Map our theft_crime_rate_sdf using Jan 2020 column, with 5 breaks and using the jenks style tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;jenks&quot;) We now have a choropleth that reflects better the distribution of our data - but I’m not that happy with the classification breaks used by the “jenks” approach - they’re not exactly as readable as our pretty breaks. Therefore, I’m going to use these breaks, but round them down a little to get a compromise between my two classification schemes. To do so, I need to change the style of my map to fixed and then supply a new argument from breaks that contains these rounded classification breaks. Change the style parameter and add the breaks parameter: # Map our theft_crime_rate_sdf using the Jan 2020 column, but fixed breaks as defined tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) That looks a little better from the classification side of things. We still have one final “data-related” challenge to solve, before we start to style our map - and that is showing the polygons for City of London wards, even though we have no data for them. We always want to create a map that contains as much information as possible and leave no room in interpretation error - by leaving our CoL wards as a white space within our current map, we are not telling our map readers anything other than there is a mistake in our map that they’ll question! We therefore want to include our CoL wards in our map, but we’ll symbolise them differently so we will be able to explain why we do not have data for the CoL wards when, for example, we’d write up our analysis or present the map on a poster. This explanation is something you’d add into a figure caption or footnotes, for example, depending on how long it needs to be! For now, we want to add these polygons to our map - and the easiest way to do so is to simply add another spatial object to our map that symbolises our polygons as grey (/“gray”, alas US-centric programming here ;) wards. Let’s go ahead and try this out using the “layered” approach of graphic making, where we simply rinse and repeat and add our layer (and spatial object) as another addition to our map. This time, we’ll use our original spatial data frame that we loaded into our script, ward_population, so we do not get confused between our layers. As per out tm_polygons() layer, we simply add this using the + sign. Create a new tm_shape(), equal to our ward_population spatial data frame and draw as grey polygons. # Map our theft_crime_rate_sdf as above and our CoL wards (from ward_population) tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) + tm_shape(ward_population) + tm_polygons(&quot;gray&quot;) Hmmm! Interesting! We have our grey polygons of our ward - and what appears to be the legend for our choropleth - but no choropleth map? What could have gone wrong here? Maybe this is to do with the LAYERED approach - ah! As we have added our new shape-layer, we have simply added this on top of our original map. So it seems how we order our tmap code is really important! As we build our maps, we need to be conscious of the order in which we layer our objects and polygons. Whatever comes first in our code is drawn first, and then the next layer is drawn on top of that and so on! This should be simple fix - and requires just a little rearranging of our code. Re-arrange our code to have our grey CoL wards first, and then our Jan 2020 theft crime rate choropleth map: # Re-order map to display grey polygons under our choropleth map tm_shape(ward_population) + tm_polygons(&quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) Ahah, that’s much better! Now we have our data displayed, we want to go ahead and start styling our map. Styling maps using tmap And now things start to get even more complicated… As you’ve seen, getting to the point where we have a choropleth map in R takes a lot of knowledge about how to use the tmap library successfully. Whilst ultimately it is only four functions so far, it is still A LOT to learn and understand to make a good map, compared to, for example, the Q-GIS Print Layout. To style our map takes even more understanding and familiarity with our tmap library - and it is only something you’ll only really learn by having to make your own maps. As a result, I won’t go into explaining exactly every aspect of map styling - but I will provide you with some example code that you can use as well as experiment with to try to see how you can adjust aspects of the map to your preferences. Fundamentally, the key functions to be aware of: tm_layout(): Contains parameters to style titles, fonts, the legend etc tm_compass(): Contains parameters to create and style a North arrow or compass tm_scale_bar(): Contains parameters to create and style a scale bar To be able to start styling our map, we need to interrogate each of these functions and their parameters to trial and error options to ultimately create a map we’re happy with. Here, for example, is a first pass at styling our above map to contain a title, change the colour palette of our map, plus change the position of the legend, add a north arrow and a scale bar - whilst also formatting the font: Example code: feel free to implement/adjust: # Creating a map template for us to use tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = FALSE, legend.position = c(&quot;left&quot;,&quot;top&quot;), legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Well this is starting to look a lot better, I’m still not happy with certain aspects. For example, I think moving the legend outside of the map might look better - plus I’d prefer that the legend also has a different title that is more informatives. Let’s see what small adjustments we can make. Here I’ve added: a title = argument into the tm_polygons() layer for the theft_crime_rate_sdf whilst adding legend.outside = TRUE, legend.outside.position = \"right\" to the tm_layout() layer. Let’s see: More example code - feel free to add and implement: # Creating a map template for us to use, legend outside tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), title=&quot;Theft Crime Rate per 10,000 people&quot;, style=&quot;jenks&quot;, palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Well I’m pretty happy with that! There’s only a few more things I’d want to do - and that would be to add an additional legend property to state why the City of London wards are grey as well as our data source information. Remember - all our maps contain data from the Ordnance Survey and Office for National Statistics and this needs to be credited as such (I’ve put this for now in our Acknowledgements section of the workshop). This could all be added in an additional text box within our map using the tm_credits() function - but I’m not happy with the display that R creates (feel free to experiment with this if you’d like!). I haven’t quite figured out how to get the tm_credits() box to appear outside the main plotting area! For now, I would add this in post-production or take the my next step in my own R map-making learning curve is to figure out how to make an additional box outside the map area. Let’s see what we get up to in the second half of term! Exporting our final map to a PNG Once we’re finished making our map, we can go ahead and export it to our maps folder. To do so, we need to save our map-making code to a function and then use the tmap_save() function to save the output of this code to a picture within our maps folder. Once you’re happy with your map, export it using the code below. # Store map as a variable jan2020_map &lt;- tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), title=&quot;Crime Rate per 10,000 people&quot;, style=&quot;jenks&quot;, palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) ## save an image (&quot;plot&quot; mode) tmap_save(jan2020_map, filename = &quot;maps/jan2020_theft_crime_map.png&quot;) We also want to export the rest of our hard work in terms of data wrangling that we’ve completed for this practical - so let’s go ahead and export our data frames so we can use them in future projects, where during GEOG0030 or beyond. What we’ll do is export both the all_theft_ward_sdf spatial data frame and theft_crime_rate_sdf as shapefiles. This means we’ll have both datasets to use in the future - you can, if you want, also export the all_theft_ward_sdf spatial data frame as a csv if you like. Export our final spatial data frames as shapefiles: # Write out the theft_crime_df to a csv within our raw crime data folder st_write(theft_crime_rate_sdf,&quot;data/working/theft_rate_by_ward_per_month_2020.shp&quot;, row.names = FALSE) # Write out the all_theft_ward_sdf to a shapefile within our working data folder st_write(all_theft_ward_sdf,&quot;data/working/theft_count_by_ward_per_month_2020.shp&quot;, row.names = FALSE) # Write out the all_theft_ward_sdf to a csv within our raw crime data folder write.csv(all_theft_ward_sdf,&quot;data/working/theft_count_by_ward_per_month_2020.csv&quot;, row.names = FALSE) And that’s it - we’ve made it through our entire practical - awesome work and well persevered! You will have learnt a lot going through this practical that we’ll keep putting into action as we move forward in Geocomputation. Therefore, as I always say, do not worry if you didn’t understand everything we’ve covered as we’ll revisit this over the next five weeks - and you’ll of course always have this page to look back on. To consolidate our learnings, I have a small task for you to complete - as I’ve said earlier, I won’t set the mini-project I had planned, but what I would like you to do is complete a small assignment in time for our seminar in Week 6. Assignment: Making maps for another month! For your assignment for this week, what I’d like you to do is to simply make a map for a different month of 2020 and export this to submit within your respective seminar folder. If you navigate to your folder from here, you’ll see I’ve added folders within each seminar for the different maps we’re making within our practicals. What I’d like you to do is check this folder to see what months are already covered within the folder - and then make a map for the month that isn’t yet made! To help, when you export your map, make sure to use the name of the month at the start of your title (i.e. as prescribed above!). You’ll of course see that January 2020 is already taken - by me! But it’d be great to get maps for every single month of the year (until November, or December if the data is now available) within each seminar folder. But what if all the months are now done? Please go ahead and make a duplicate map (not of January, of course!) - the more the merrier, and if you can look into different colour palettes and styling effects, even better! Remember, you’ll need to really think about your classification breaks when you change to map a different map from January as my breaks are based on January’s distribution! We won’t worry about standardising our breaks across our maps for now - just make sure you represent the distribution of your data well! If you have any issues with this, please get in touch! Recap Wow - that’s been a lot to get through, but over the last two weeks, you really have had a crash-course in how to use programming for statistical and spatial analysis. In this week’s workshop, you’ve learnt about why we use programming for spatial anlaysis, including how the four key principles of data science have affected how we “do” spatial analysis. You’ve then had a thorough introduction into how we use R and R-Studio as a GIS - and as we can see through our practical, in comparison to Q-GIS, there is a lot more to learn, as we need to know a lot about programming, particuarly to “wrangle” our data - before we even get to map-making. Furthermore, when it comes to map-making in R, this isn’t even as straight-forward! We need to know all about this “grammar of graphics” and how to layer our data and what parameters do what, which, compared to drawing a few boxes etc. in Q-GIS, is a whole lot more complicated! You can therefore see that Geocomputation really requires a combination of foundational concepts in GIScience, Cartography and Programming in order to understand precisely what you’re doing - and even when you’ve had this foundational introduction, it can still feel overwhelming and a lot to learn - and that’s because it is! I do not expect you to “get this” all at once, but this workbook is here for you to refer to as and when you need to get your “Aha” moments, that you’ll get a) on this course and b) as you, for example, complete your own independent research projects, such as your dissertations. Take this all in good time, and we’ll get there in the end - and I will revisit lots of the concepts we’ve looked at over the last two weeks time and time again! What you should realise however is that once you have this code written - you can just come back to it and copy and paste from your scripts, to use in other scripts, for example, changing variables, data files and, of course, parameter settings. And that’s how you end up building up a) your scripts in the first place but b) your understanding of what this code does! If you’re concerned that need to know and understand every function – I can whole-heartedly say - no, you don’t. It takes time, experimenting and research to learn R. For example, last week I had you clean the names of our crime dataset clean names manually - I found out this week there is a great package called janitor that has a function called clean_names() would do that all for us. We’ll use this in Week 6 for some data cleaning, so we won’t deviate now. Ultimately programming - and increasing your “vocabulary” of packages and functions - is an iterative learning process and only one you’ll build upon by writing more and more scripts! To help with all of this new learning, I recommend only one key reading for now: Key Reading Geocomputation with R (2020) by Robin Lovelace, Jakub Nowosad and Jannes Muenchow, which is found online here. I’d recommend reading through Chapters 1, 2, 3 and 8. We’ll continue to build on everything we’ve learnt over the last five weeks as we move into the second half of the module, where we focus more on spatial analysis techniques. You’ll be probably happy to know we will focus less on programming concepts and more on spatial analysis concepts - and use what we know so far with programming to conduct the spatial analysis. This should mean that our practicals will be a little shorter in terms of reading - and even more active in terms of doing! Extension: Facet Mapping So you’ve got this far and really want more work? Really? Are you serious? Ok, well here we go! (And for those of you that don’t, do not worry, as we’ll be looking at this in more detail at Week 10!) So how cool would it be if we could make a map for all 11 (12) months of data in an instant using code…? Well that’s exactly what faceting is for! According to Lovelace et al (2020): Faceted maps, also referred to as ‘small multiples’, are composed of many maps arranged side-by-side, and sometimes stacked vertically (Meulemans et al. 2017). Facets enable the visualization of how spatial relationships change with respect to another variable, such as time. The changing populations of settlements, for example, can be represented in a faceted map with each panel representing the population at a particular moment in time. The time dimension could be represented via another aesthetic such as color. However, this risks cluttering the map because it will involve multiple overlapping points (cities do not tend to move over time!). Typically all individual facets in a faceted map contain the same geometry data repeated multiple times, once for each column in the attribute data. However, facets can also represent shifting geometries such as the evolution of a point pattern over time. In our case, we want to create facet maps that show our theft rate over the 11 months and to do so, we need to add two bits of code to our original tmap approach. First, in our tm_polygons() shape, we add all our months as a combined vector. + *We make this easy for ourselves by creating a month variable that stores these values from a selection of the names() function on our spatial data frame. Second, we add a tm_facets() function that tells tmap to facet our maps, with a specific number of columns. The code below shows how to create a basic facet map using this code. What I’d like you to do is figure out how to make this facet map more aesthetically pleasing - including changing the location of the legend (or removing it?) as well as altering the colours etc. If you manage to create a facet map you are happy with, please export this and upload it to your relevant seminar folder! # Store our months as a variable month &lt;- names(theft_crime_rate_sdf)[8:18] # Map all our months at once tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = month) + tm_facets( ncol=3) Learning Objectives You should now hopefully be able to: Understand how spatial analysis is being used within data science applications Recognise the differences and uses of GUI GIS software versus CLI GIS software Understand which libraries are required for spatial analysis in R/R-Studio Conduct basic data wrangling in the form of selection and slicing Create a map using the tmap visualisation library Acknowledgements This page is entirely original to GEOG0030. The datasets used in this workshop (and resulting maps): Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). "],["analysing-spatial-patterns-i-spatial-auto-correlation-regression.html", "6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression", " 6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Welcome to Week 6 in Geocomputation! This week marks a big change in our Geocomputation adventure as we begin to fully focus on spatial analysis using R-Studio and R programming simply as a tool to conduct this analysis. Everything we do over the coming weeks could be completed in Q-GIS (or many other GIS software), but to adhere to our data science principles - and to ultimately make our lives easier - we focus on learning how to excecute spatial analysis in R. We will now slightly change the structure of our workshops to reflect this new focus. As you’ll find, the content of the workshops will be more practical-led, with short lectures provided where necessary. We will continue to on one variable (and thus dataset) throughout and build our script to reflect our investigation - whilst also still using the terminal, when applicable. As a result, all workshops will begin from now on with a bit of “script housekeeping” - we will always start our content by first creating a new script, adding the dependencies (i.e. libraries) you’ll need for this week’s content and then download, clean and load the data ready to begin the main workshop content. We will not cover the programming principles used in our code if we have covered it in previous practicals but if we come across something new, we will of course provide an explanation. We have a small bit of data cleaning to do this week, so we best get started! Week 6 in Geocomp Video on Stream This week’s content introduces you to two spatial analysis techniques: spatial autocorrelation and regression with spatial data. We have three areas of work to focus on: What is spatial autocorrelation How to define neighbours both theoretically and programmatically for use in spatial autocorrelation and regression How to factor spatial autocorrelation into regression models This week’s content is split into 4 parts: Workshop Housekeeping (30 minutes) Analysing Spatial Patterns (10 minutes) Analysing Distributions (90 minutes) Analysing Relationships (25 minutes) This week, we have 1 lecture and 2 assignments within this week’s workshop. Learning Objectives By the end of this week, you should be able to: Understand how to analyse distributions of (areal) spatial data through visual and statistical analysis Explain the different approaches to defining neighbours within spatial autocorrelation Run different types of spatial autocorrelation techniques and understand their differences Understand the basics of incorporating the issue of spatial autocorrelation into regression This week, we are using a completely new dataset and investigating a different phenomena: childhood obesity. We’ll be investigating its distribution across London at the ward scale and then start to look into the different factors that might contribute to obesity in children, which will be resumed in one of the optional practicals in Week 10. To complete this analysis, we’ll be using a single data download from the London Datastore, which we’ll need to clean, wrangle and then join to one of our Ward shapefiles in order to spatially investigate the distribution of childhood obesity - and then analyse factors that might contribute toward it. Workshop Housekeeping As stated above, we’ll start each week with a workshop housekeeping section where we’ll outline the datasets (and cleaning) and libraries you’ll need for your analysis. It’s up to you if you want to do this before listening to the lectures in the relevant sections - but don’t forget to do this, or you won’t have any data to analyse. We provide instructions on how to clean your data and then the reminder to load it within your R script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk6-obesity-spatial-analysis.r. At the top of your script, add the following metdata (substitute accordingly): # Analysing childhood obesity and its factors # Script started February 2021 # NAME Dependencies (aka libraries) Now we’ll install the libraries we need for this week. spdep contains the relevent functions to run our various spatial autocorrelation tests, whilst RColorBrewer enables us to use colour palettes from ColorBrewer family within our maps - more on this later. You’ll need to install these two libraries, plus janitor that will be used to clean our file names, using the install.packages(c(\"janitor\", \"spdep\", \"RColorBrewer\")) command in your console. Within your script, add the following libraries for loading: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) library(janitor) library(spdep) library(RColorBrewer) Remember to select the lines of code you want to run and press CMD (Mac)/CTRL(Windows) + Enter/Return - we won’t remind you to run each line of code in the remainder of the practical sessions. Datasets for this week We are going to only need two datasets for this week - our London Ward boundaries from 2011 and the Greater London Authority (GLA) Ward Atlas and Profiles. The GLA Ward Atlas and Profiles provide a range of demographic and related data for each ward in Greater London and were specifically designed to provide an overview of the ward’s population by collating and presenting a range of data on the population, diversity, households, life expectancy, housing, crime, benefits, land use, deprivation, and employment (GLA, 2014). Indicators in the Atlas/Profile include: Age and sex Land area, projections and population density Household composition, religion, ethnicity Birth rates (general fertility rate), death rates (standardised mortality ratio), life expectancy Average house prices, properties sold, housing by council tax band, tenure, property size (bedrooms), dwelling build period and type, mortgage and landlord home repossession Employment and economic activity, Incapacity Benefit, Housing Benefit, Household income, Income Support and JobSeekers Allowance claimant rates, dependent children receiving child-tax credits by lone parents and out-of-work families, child poverty GCSE results, A-level / Level 3 results (average point scores), pupil absence, Child obesity Crime rates (by type of crime), fires, ambulance call outs, road casualties Happiness and well-being, land use, public transport accessibility (PTALs), access to public greenspace, access to nature, air emissions / quality, car use, bicycle travel Indices of Deprivation Election turnout i.e. a lot of data! The main dataset utilises the 2011 Ward Boundaries as its spatial representation, therefore we need to use the 2011 boundaries. The data is collected from a range of data sources, which you can read about on the website prior to downloading. The Atlas dataset combines data from 2011 - approximately 2015. There is an additional excel workbook that contains the data referenced to the 2014 boundaries, but this requires some manual cleaning to use that we don’t want to do this week! Data Currency and Spatial/Temporal Compatibility 2015 certainly feels like a long time ago - and 2011 is even longer! Currency of data is a big issue when we are looking to compare data - for example, as many of you are finding with your own dissertation research, the UK’s last census was in 2011. This means our main source of detailed small area data is quite outdated and we’re a year or two away from this year’s census (data to be collected soon!) being available for academic research. But, what you might have seen via the London Data Store and some of the Data Portals I’ve mentioned, is that there are many more recent datasets out there to analyse - from Mean Income estimations readily available for 2019, to our Population Estimates, as well as other data such as the 2019 Index of Multiple Deprivation. However, when you do find more recent datasets like these, you can often run into spatial incompability issues - particuarly in our situation when using the Ward spatial unit, which has changed THREE times since the last census. What you might find is that the data you download can only be used with boundaries from specific years - so if you want to analyse data across different iterations of the Ward boundaries, you’ll need to find a way of “translating” the data into one version of the boundaries. Now this in itself comes with many complications - particularly when it comes to thinking through the implications this may have due to the statistical collection and representation of the sampled population within the dataset! There is no easy way to solve this but follow the best practice advice that comes with the datasets. In our case, we have been able to find this Ward Atlas that provides the data we need for our analysis - but you might not always be so lucky. One more thing in terms of temporal compatibility - it is ok to mix data from different years (within reason!), as long as you show consideration to the different currency of your datasets. A good rule of thumb is with 3-5 years. Downloading our data We already have our 2011 London Ward boundaries within our raw data folder, so we only need to download our Ward Atlas. Navigate to the Ward Atlas dataset in the London Data Store here. Download the ward-atlas-data.csv. You might find that instead of downloading the file, your browser will open up a new window. You have two options: Copy and paste all contents of the page into a text editor such as Notepad and save your pasted contents as ward-atlas-data.csv in your raw data folder - make sure to add the .csv to the end of your file name to save your text file as a `csv. Click back to the dataset page, right-click on the ward-atlas-data.csv name and select Download Linked File from your computer’s options. Move this into your raw data folder. This is the easier approach of the two. Make sure you store this in your raw folder. Loading our data Let’s first load our London Ward shapefile from our raw -&gt; boundaries -&gt; 2011 folder. Load the 2011 London Ward boundaries. # Read in our London Ward boundaries london_ward_shp &lt;- read_sf(&quot;data/raw/boundaries/2011/London_Ward_CityMerged.shp&quot;) We can both View() and plot() the data in our console to check what our data looks like. We’re happy with the dataframe (its field names) and what’s its looking like as a shapefile, so we do not need to do any cleaning on this dataset. We can now turn to our London Ward Atlas dataset and load the dataset into R. Load the Ward Atlas Data csv. # Read in our ward atlas data csv from our raw data folder all_ward_data &lt;- read_csv(&quot;data/raw/ward-atlas-data.csv&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1], &#39;X2&#39; [2], &#39;X3&#39; [3], &#39;X4&#39; [4] Don’t worry if you get a few parsing errors, the dataset is not structured in a “tidy” way yet! If you go ahead and view the data, you’ll see we have a lot of information about our Wards in the dataset - we have a total of 946 variables four our 629 wards. We can’t exactly analyse all of these variables, so we’ll need to extract only the variables we need. Selecting our variables for analysis To clean our data and extract the variables for our analysis, we need to identify those most useful to our research. Of course, we need to find a variable that matches our phenomena of investigation: child obesity. We then need to identify the variables that most match the “explanatory factors” we think might contribute to childhood obesity. In conventional research, aka what you’ll do for your dissertation, the selection of these factors would always be informed by research. We would conduct an extensive Literature Review to identify the common factors associated with childhood obesity and then look to find data and the right variables to reflect these factors. In our case, I’ve conducted a brief review of the literature ( ;) ) and identifed that generally there are two sets of factors that may contribute to childhood obesity: Individual level factors: diet and exercise, parents’ weights, mode of travel to school / time taken to walk to school. Household/societal level factors: area deprivation, household income, household employment You can read a little more into these factors in this review by Mayor in 2005 of the “Health Survey for England: Obesity Among Children Under 11” report that was published by the Department of Health, based on their surveys. This report is pretty old - but this gives us opportunity to investigate into the current(-ish) situation of childhood obesity and see if these findings still are similar 10 years later. Within our Ward Atlas dataset, however, we will not be able to capture the individual level factors due to the nature of the data - we would need those individual level datasets that the Department of Health have access to! The next best thing we can do is an investigation into the more household/societal level factors that might impact childhood obesity. With the above study in mind, the variables we’ll look to include are: Obesity of children (Dependent) Deprivation (Explanatory / Independent) Household Income (Explanatory / Independent) Household Employment (Explanatory / Independent) Plus a few others that have featured in more recent literature: House Price (Explanatory / Independent) Access to Greenspace (Explanatory / Independent) These are all available within our dataset - we just need to extract them from our currently very large dataframe! If you wanted to find more recent versions of these datasets for an even more updated analysis, you can find them as follows: Deprivation data: The latest Index of Multiple Deprivation is 2019. You can find all three versions, 2011, 2015, and 2019, here. The 2019 dataset works with the 2018 Ward boundaries. Obesity data: The obesity data is extracted from the National Child Measurement Programme (NCMP). This program measures the height and weight of children in Reception class (aged 4 to 5) and year 6 (aged 10 to 11), to assess overweight and obesity levels in children within primary schools. The latest dataset is available from 2017/2018 - 2019/2020 directly from the NHS website. The newer datasets can work with 2018 boundaries but there are a few errors. Greenspace access: Access to greenspace in London is produced by GIGL. The most recent dataset (used in this analysis) is found here. This dataset works with 2011 boundaries. House prices: Yearly house price datasets (up to 2017, instead of 2014) here. These datasets work with the 2014 ward boundaries. Household income: Yearly income datasets (up to 2018, instead of 2014) here. These datasets work with MSOA boundaries. Again, you’ll run into issues of incompatability if you try to analyse across these datasets and their variables without additional processing. Let’s go ahead and get the data extracted and cleaned. Extracting and cleaning (wrangling) our dataset This week, our data wrangling is quite minimal - but it is important you follow all the steps to ensure you have the correct final dataframe for our analysis. Overall, you will: Select the required columns for our dataframe and analysis Remove the first row, which contains data for the whole of England, not a Ward Clean and rename our field columns Coerce our variables into the correct data type for our analysis Join our ‘atlas data’ dataframe to our ward spatial dataframe Now for this week, I’ve done the hardest bit for you - I’ve scrolled through ALL 946 variables to identify the fields that we need to extract for our analysis. The fields that we need to extract from our dataset include: 838: Childhood Obesity Prevalence; Year 6 (School children aged 10-11); 2011/12 to 2013/14: % obese 900: Indices of Deprivation; IDACI; 2010 248: House Prices; Median House Price; 2014 353: Household Income; Mean Modelled Household income (3); 2012/13 373: Employment; Adults not in Employment - 2011 Census; % of households with no adults in employment with dependent children 377: Qualifications; Qualifications and Students - 2011 Census; % No qualifications 859: Access to green space and nature; % homes with deficiency in access to nature; 2012 865: Public Transport Accessibility; Average PTAL score; 2014 and of course: 2: X2 - which contains our ward codes. 4: X4 - which contains our ward names. (The Xs were part of the bad formatting R interpreted as it parsed the csv). Now I’ve included the index number of each field as we’re going to use these within our selection. Currently our variable names are a) very long and b) contain spaces and special characters in them, which make them pretty useless for selection. Using the index approach will make this much easier (refer back to last week’s practical if you’ve forgotten what indexing is!). Select our 10 fields from our all_ward_data dataframe for use in analysis. # Select our 10 fields for analysis using their index number obesity_ward_data &lt;- select(all_ward_data, 2, 4, 838, 900, 248, 353, 373, 377, 859, 865) You should now have a new dataframe with our 10 variables. We’ve still got a bit of cleaning to do, before we’re happy. One issue with our original csv is that is contained two rows worth of field names - hence if you look at the first row of our dataframe, it doesn’t make sense. We therefore want to remove this row. In addition, it would be good to clean up our names for use - here we’re going to use the janitor library that I mentionned last week, which cleans our names by removing white space, special characters, capitals etc. Let’s use a pipe to do this all at once. We’ll overwrite our current variable. Remove the first line of our dataframe and clean our field names. # Remove our first row, clean the names of our fields obesity_ward_data &lt;- obesity_ward_data %&gt;% slice(-1) %&gt;% clean_names() This dataframe is already looking much better! We can now see our dataframe as the “tidy data” format. The final thing we can do with this dataset before we need to join it to our London Wards spatial dataframe is just tidy up our column names - X2 and X4 does not exactly mean much to us and it gives us a chance to shorten the names of the other variables (we could leave them as is now they’ve been cleaned, but it’ll be easier for reference later if they’re shorter!). Note, I decided to keep the information on the year of the dataset to help with our analysis later. Rename our columns to shorter field names. # Rename field names of ward data to something more useful names(obesity_ward_data) &lt;- c(&quot;ward_code&quot;, &quot;ward_name&quot;, &quot;y6_obesity_2014&quot;, &quot;IDACI_2010&quot;, &quot;med_house_price_2014&quot;, &quot;mean_hh_income_2013&quot;, &quot;per_no_adult_employ_2011&quot;, &quot;per_no_qual_2011&quot;, &quot;per_deficiency_greenspace_2012&quot;, &quot;PTAL_2014&quot;) Now we have the data we want to map, we need to do a final spot of checking - one of the main issues faced with loading data directly from a csv in R without cleaning it first in a spreadsheet program as we’ve done before, is that we can’t guarantee that the data will be loaded correctly. Unfortunately with our current dataframe we can see that this is the case - if you look at your global environment and find our obesity_ward_data variable and click on the arrow button to display its variables, you’ll see that several of our variables are of the type char. This means that these variables have been interpreted by R to be characters rather than numerics. This might be because there is some missing data or in some cases, the decimal point can interfere with the data being read as a numeric. Luckily it’s quite easy to change our data type - a bit like right-clicking on our columns in Excel and setting the format of the column to number, we’ll do this using code. If we wanted to apply this to a single column than we would use the code: as.numeric(dataframe$column) but as we want to apply this across a few columns, we’ll be using the mutate_at() function from the dplyr library. If you look at the documentation for mutate_at(), it states that it can apply the same transformation to a selection of variables, in our case, our columns. We’ll need to use a litte more complicated code that we came across last week that states that for each of our ‘variables’ (i.e. our columns), we want to run a function on it where the varialbe is defined as x. We then state our function which is to first coerce our column into the as.character() data type (to double check it is!) and then coerce is into the numeric data type through using as.numeric(). Change the data type of our variable columns to numeric. # Rename field names of ward data to something more useful obesity_ward_data &lt;- mutate_at(obesity_ward_data, vars(y6_obesity_2014, mean_hh_income_2013, per_no_adult_employ_2011, per_no_qual_2011, per_deficiency_greenspace_2012, PTAL_2014), function(x) as.numeric(as.character(x))) ## Warning: Problem with `mutate()` input `y6_obesity_2014`. ## ℹ NAs introduced by coercion ## ℹ Input `y6_obesity_2014` is `(function (x) ...`. ## Warning in (function (x) : NAs introduced by coercion ## Warning: Problem with `mutate()` input `per_deficiency_greenspace_2012`. ## ℹ NAs introduced by coercion ## ℹ Input `per_deficiency_greenspace_2012` is `(function (x) ...`. ## Warning in (function (x) : NAs introduced by coercion ## Warning: Problem with `mutate()` input `PTAL_2014`. ## ℹ NAs introduced by coercion ## ℹ Input `PTAL_2014` is `(function (x) ...`. ## Warning in (function (x) : NAs introduced by coercion You will see warnings that NAs have been introduced by this coercion in three of our variables: y6_obesity_2014, per_deficiency_greenspace_2012 and PTAL_2014 - this is something we’ll need to be aware of later in our analysis, but will not look at right now. Now our final step is to join our final obesity_ward_data dataframe to our london_wards_shp spatial dataframe so we can complete both statistical and spatial analysis. Join our obesity_ward_data dataframe to our london_wards_shp spatial dataframe - store as a new variable. # Join obesity df to ward sdf for analysis obesity_ward_sdf &lt;- left_join(london_ward_shp, obesity_ward_data, by = c(&quot;GSS_CODE&quot; = &quot;ward_code&quot;)) I told you we’d be doing a lot of these attribute joins! Have a look at your newly created spatial dataframe - for a quick look at the data, you can run the plot() command in your console. We’ll explain more about the different measures we’ve extracted during the rest of the workshop. If you’d like, you can also write out the final csv using the write.csv function to save a raw copy in your data folder. Other than that, we’re now ready to start the main content of our workshop today: analysis of spatial patterns through spatial autocorrelation and regression. Analysing Spatial Patterns If you remember from our introductory content - or have listened to my videos on your dissertations - you should know by now that when it comes to spatial analysis in research, we’re looking to achieve one (or more) of three things: Analyse the distribution of a phenomena Analyse the relationship between phenomena, to identify factors/predictors Create new spatial data that can then be used in the above In today’s practical, we’ll be looking at how we can conduct the first two, whereas in future practicals, we’ll also be looking at ways we can use spatial analysis to create new data that can then be used in applications of (1) and (2). An example of this is looking at the use of network analysis to calculate the distances for use within different types of accessibility measures. For example, in addition to the variables we are using today, we could look to create a further variable that measures the average distance each school in our wards is to their nearest fast food outlet. We’ll actually see this in action in one of our optional practicals in Week 10. A further approach is using these measures to create indices. An example of this involving network analysis, that we’ll use today, are Transport for London’s (TFL) Public Transport Accessibility Levels (PTALs). The PTALs are a detailed and accurate measure of the accessibility of a point to the public transport network. They take into account walk access time and service availability. The method is essentially a way of measuring the density of the public transport network at any location within Greater London. You can read more about the methodology behind the PTALs here and see them in action in TFL’s short video: TFL’s PTALs and WebCat As you can see from the examples, the network analysis is used to provide information on the distribution of London’s transport network to better allocate services and identify areas of improvement. Spatial analysis is our way - as geographers and spatially-fascinated thinkers - therefore to discover spatial patterns, processes and relationships (including taking into account the ‘special’ properties of spatial phenomena) within specific pheonomea or entities, using using their topological, geometric or geographic properties, and validate them with statistical and spatial quantification. This quantification is made possible by a variety of techniques that are all underpinned by key laws of geography and spatial principles and properties. For us, today, we are interested in looking at two different types of analysis techniques that address two key properties of spatial data: Spatial Autocorrelation: the effect of spatial processes on distributions. Spatial Heterogeneity: the covariances of processes over space. We’ll be using our newly created obesity_ward_sdf to investigate these two properties in action, whilst also answering questions on the overall distribution and factors of childhood obesity. Analysing Distributions Whenever dealing with data, our first question as analysts is to understand its distribution. As geographers and spatially-fascinated thinkers, analysing a data’s distribution will mean two things to us: A statistical analysis of distribution A spatial analysis of distribution We’re pretty lucky that we get to think about data in a slightly different way to those thinking about pure statistics. After all, adding a spatial component to distribution is pretty darn important when we’re looking at spatial phenomena and processes, even when looking just at statistical distribution, as we’ll see in the rest of the workshop. Statistical Analysis of Distributions Within general data analysis, when it comes to analysing the distribution of your data, you are looking to conduct what is known as Exploratory Data Analysis (EDA) which is where we look to summarise the main characteristics of our data. EDA was promoted by prominent statistician John Tukey to encourage data anlaysts to explore their data outside of traditional formal modelling - and come up with new areas of investigation and hypotheses. According to Wikipedia (that well-known academic resource!), Tukey promoted the use of five summary statistics: the max-min, the median, and the quartiles, which, in comparison to the mean and standard deviation, provide a more robust understanding of a data’s distribution, particularly if the data is skewed. We looked at how we can use R to extract some of these summary statistics briefly in Week 4, but let’s have a look at how we can add further to this EDA, including creating some statistical charts of our data’s distribution. Understanding the statistical distribution of childhood obesity In Week 4, we looked at how we can display individual descriptive statistics about our dataset. To make things even easier for us, we can instead, we can use a single function to find out about the distribution of our data: summary(). In your script, below your joining of our dataframes, summarise our y6_obesity_2014 distribution. # Summarise our obesity_ward_sdf data column. Remember we use the $ to access the specific column we want to assess summary(obesity_ward_sdf$y6_obesity_2014) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 5.929 18.569 22.670 21.752 25.325 35.000 12 You can save the output of your function to a variable if you’d like - or else, just take a look at the numbers printed in the console. This ‘summary()’ function can also be called on the dataset as a whole and will generate summary statistics for each individual numeric variable. You can execute this in your console if you’d like to get an understanding of all of variables - although we’ll focus on obesity for much of this practical. We can see that our Median and Mean are quite close to one another - and the quartiles are nearly the same amount apart from the mean, so we can start to think that our data is normally distribution. To confirm this, we can do the next best thing, which is plot our distribution using a histogram, using the base R hist() command. Plot the histogram of our y6_obesity_2014 distribution. # Plot the histogram of our y6_obesity_2014 data. Remember we use the $ to access the specific column we want to assess hist(obesity_ward_sdf$y6_obesity_2014) We can actually see our data has a slight negative skew - which would make sense given that our median is higher than our mean. Cheatsheet on data distributions: negative skew, normal, positive skew. Source: Wikipedia Just like our maps from last week, we can further customise our histograms if we want to make them more aesthetically pleasing and upate the title and axis labelling. Change the colour of our hisogram and change the labelling of our histogram. # Plot a fancy histogram of our y6_obesity_2014 data. Remember we use the $ to access the specific column we want to assess hist(obesity_ward_sdf$y6_obesity_2014, col=&quot;grey&quot;, main=&quot;Distribution of Obesity in Year 6 children within London Wards in 2014&quot;, xlab=&quot;Percentage of obese Year 6 children in the ward&quot;) We can also, as per our maps last week, export this histogram and save it as a PNG by a) storing the histogram code to a variable and b) saving this variable to a file. The code to do so is a little different - we essentially “open up” a file, called what we want to name our plot. We then run the code to plot our data, which will place the output “within” the file and then \"close’ the file down. Note, I’ve also created a new folder called graphs within my project. Export our plot to an image in our graphs folder. # Open a PNG, give it the filename we want png(&quot;graphs/Y6_obesity_distribution.png&quot;) # Run the same plot code to save our output into this opened png hist(obesity_ward_sdf$y6_obesity_2014, breaks=10, col=&quot;grey&quot;, main=&quot;Distribution of Obesity in Year 6 children within London Wards in 2014&quot;, xlab=&quot;Percentage of obese Year 6 children in the ward&quot;) # Close the PNG dev.off() You should now see the image appear in your graphs folder. Another type of chart we can create just using the base R library is a boxplot. A boxplot shows the core characteristics of the distributions within a dataset, including the interquartile range. Plot the boxplot of our y6_obesity_2014 variable. # Plot the boxplot of our y6_obesity_2014 data. Remember we use the $ to access the specific column we want to assess boxplot(obesity_ward_sdf$y6_obesity_2014) It’s not exactly the most exciting graph now is it! There is actually a lot more we can do in terms of visualising our data’s distribution - and the best way forward would be to become more familiar with the ggplot2 library, which is the main visualisation for both statistical and, increasingly, spatial graphs, charts and maps. The library is quite complex - and learning how to use it isn’t the aim of this week’s practical so we won’t cover this in any more detail now. Why do I need to care about my data’s distribution? We’ve now managed to investigate the statistical distribution of our data - but what does that mean for us as data analysts? Well put simply, we need to understand our data’s distribution if we want to test relationships between different variables, for example using linear regression. If we see that there is a gaussian (normal) distribution across two of our variables, it is likely that there will be a (linear) relationship between our dependent and independent variables. Ensuring our variables are normally distributed is a key assumption of being able to run a linear regression - as we’ll read about later below. Assignment 1: Distribution of our explanatory variables Your first assignment this week is to go ahead and test each of our variables to determine their distribution. Make a note of which ones are normally distributed and which aren’t (and their skew) - we’ll need to address this later in the workshop. From conducting your assignment above, you should see that not all of our variable have a normal distribution (in fact, there is only one that really looks somewhat “normal”!). As a result, when it comes to our relationships, we’re going to need to transform our values to see if we can achieve a linear relationship - but more on that later. There is some debate as to whether this is a wise thing to do as, amongst other things, the coefficients for transformed variables are much harder to interpret. Spatial Analysis of Distributions Whilst statistical analysis of distributions focus on tests and charts, when we want to understand the spatial distribution of our phenomena, we have a very simple solution: we make a map! In our case, we’re looking at areal unit data and therefore we can use a choropleth map to study our data across the wards: Our variables (l-r, t-d): Yr 6 Obesity, IDACI, Median House Price, Mean Household Income, Percent No Adult Employment, Percent No Qualifications, Percent Deficient in Greenspace, PTAL And specifically, we can create a single output choropleth map for each of our variables to visually understand their distribution. In this case, I’ve copied the code from Week 5’s map-making, changed the colour scheme and titles, but quite quickly managed to make a map of our obesity variable to further understand it’s distribution. Note, you do not need to copy this code over into your script - but provided as a reference to see how this map was made. # Creating a map template for us to use, legend outside tm_shape(london_ward_shp) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(obesity_ward_sdf) + tm_polygons(col = &quot;y6_obesity_2014&quot;, n = 5, style = &quot;jenks&quot;, title=&quot;Percentage of children &quot;, palette=&quot;Purples&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;Obesity of Year 6 children in London Wards in 2014&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) The thing with spatial distributions is that we can quickly pick up on spatial patterns present within our data just by looking at the data (although we will also miss some others!) - and we can even provide a description of the patterns we see. For example, in our obesity example, we can see there are clusters of high levels of obesity in East London, whilst there are clusters of low levels of obesity in the south east and south west areas of London. However, reflecting on our statistical analysis above, wouldn’t it be great if there were “metrics” out there that could actually account for and quantify these visible clusters? Well, luckily for us, several geographers and spatially-enabled thinkers thought the same way - and came up with an approach to quantify these clusters. Dealing with NAs in our dataset Every time I write a tutorial for the class, a little pesky data issue always has to come up and surpise me - and this is no difference with this workshop. If you remember earlier, when we coerced our variables into numeric values, we also introduced NAs into several variables. The issue is, the analyses we will conduct below will not work with NAs. I’ve checked the obesity_ward_sdf and it appears (and validated by our earlier facet choropleth map) that only our yr6_obesity_2014 variable contains NAs. Do double-check your own dataframe to make sure this is the case. To move forward with our analysis, we need to remove those wards that do not contain any obesity information from our spatial dataframe - this will have some issues in terms of how we think about our “neighbours”, explained further below, but will allow us to run the code required. Remove the wards in our obesity_ward_sdf that do not contain any obesity information. # Remove NAs using the filter function from dplyr and the ! (not) NA function obesity_ward_sdf &lt;- obesity_ward_sdf %&gt;% filter(!is.na(y6_obesity_2014)) Spatial Autocorrelation and Clustering Analysis “The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (1970) When you attach values to the polygons of wards in London, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like and how prominent they are to the human eye. There can appear to be clusters, or, in some cases, the distribution can be random. When values are clustered, this issue is known as spatial autocorrelation and results from what is known as spatial dependence. Understanding spatial dependence on variables. Source: Manuel Gimond. Spatial dependence is the idea, as introduced in the second week via Tobler’s Law (1970) that “everything is related to everything else, but near things are more related than distant things”. In Week 2, we used Tobler’s Law to determine how we can create sample schemes for collecting spatial data. When it comes to spatial analysis, we can use the law to help identify clusters. Within spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Here we assume that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. As a result, spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space. On the opposite end of the spectrum, when our values our not clustered, then they will be spatially dispersed. Finally, in the middle we have complete spatial randomness (CSR) - where our data’s distribution shows no identifiable pattern whatsoever - it is useful to flag that this seldomly occurs. Dispersion, Complete Spatial Randomness and Clustering. Source: Esri. We can assess the distribution of our data using what is known as spatial autocorrelation tests, which can be conducted on both a global (identify if the data is clustered) and local (identify the precise clusters) scales. Whilst these different tests quantify how clustered, how random, or how dispersed, these distributions are through various approaches, ultimately they provide us with statistical and spatial information that can be used to create quantifiable descriptions of a variable’s distribution and how it vary over space. This can be incredibly useful when looking to explain and discuss a variable, particularly as the clustering present in one variable can be compared to another, which might be useful to make connections between underlying causes or possible solutions to the issues the original variable poses. In our current focus, on childhood obesity, we can look to understand its distribution via these tests and then compare this distribution to that of the explanatory variables we’ve chosen above to see if we can identify similar patterns and determine whether we should look towards quantifying their relationship through regression techniques. Lecture: What is spatial autocorrelation and how to define neighbours? Spatial autocorrelation, its different techniques and neighbour definitions are introduced in the main lecture for this week: Slides | Video on Stream Aspects of the above short lecture are summarised below before we go ahead and conduct spatial autocorrelation tests on our y6_obesity_2014 variable. Different spatial autocorrelation techniques As shown in the lecture, we have several types of tests that look to quantify spatial autocorrelation. Of these tests, there are two categories: Global statistical tests (i.e. those that provide us with a statistic to tell us about autocorrelation in our datset) Local spatial tests (i.e. those that provide us with the cluster maps). These are outlined in the below table for easy reference: Test Scale Test Output Significance Value Global Moran’s I Global Tests how “random” the spatial distribution of values are, producing a correlation coefficient for the relationship between a variable (like income) and its surrounding values. Moran’s I statistic: Metric between -1 and 1. -1 = dispersed 0 = CSR 1 = clustered Yes - \\(p\\)-value Getis-Ord Global Tests for clusters of high and low values. Works best with use of distance/proximity weights. Getis-Ord \\(G_{i}^{*}\\) statistic: gG &gt; Expected = High values clustering gG &lt; Expected = Low values clustering Yes - \\(p\\)-value Geary’s C Global Tests whether similar or disimilar values are clustering. Geary C statistic: 1 = No spatial autocorrelation GC &lt; 1 = Similar values clustering (positive spatial autocorrelation) GC &gt; 1 = Disimilar values clustering (negative spatial autocorrelation) Yes - \\(p\\)-value Local Moran’s I Local Tests the difference between a unit of analysis and its neighbour(s). Local Moran’s I statistic: 1. Can be used alongside the mean of values to generate cluster type generations. i.e. High-High, High-Low, Low-High, Low-Low, Insignificant. 2. Can also use standard deviation of main local statistic to show just high and low clusters Yes - \\(p\\)-value Local Getis-Ord Local Identifies statistically significant hot spots and cold spots using the local Getis-Ord Gi* statistic. Can use the standard deviation of statistic to identify hot and cold spots Yes - \\(p\\)-value In each of these cases, our \\(p\\)-values are psuedo \\(p\\)-values, generated through simulations such as that outlined in the lecture. Our pseudo p-values allow us to interpret our relationships with a level of confidence (but not surety as they are only pseudo values!). If we find that our relationships do not have any significance, then we cannot be confident in presenting them as true results. We’ll look to deploy each of these techniques on our dataset today - but in the future, you do not need to use all of the techniques, but instead choose one (or two, e.g. global and local) that best help you explain the distribution in your data. Understanding the spatial lag approach for Moran’s I calculation Underlying our global Moran’s I test is the concept of a spatial lag model. A spatial lag model plots each value against the mean of its neighbours’ values, defined by our selected approach. This creates a scatter plot, from which our Moran’s I statistic can be derived. An Ordinary Least Squares (OLS) regression is used to fit the data and produce a slope, which determines the Moran’s I statistic: A spatial lag model - plotting value against the mean of its neighbours. Source: Manuel Gimond. To determine a \\(p\\)-value from our model for global Moran’s I, this spatial lag model is computed multiple times (think hundreds, thousands) but uses a random distribution of neighbouring values to determine different slopes for multiple ways our data could be distributed, if our data was distributed by random. The output of this test is a sampling distribution of Moran’s I values that would confirm a null hypothesis that our values are randomly distributed. These slopes are then compared to compare our observed slope versus our random slopes and identify whether the slope is within the main distribution of these values or an outlier: A spatial lag model - plotting value against the mean of its neighbours. Source: Manuel Gimond. If our slope is an outlier, i.e. not a value we would expect to compute if the data were randomly distributed, we are more confidently able to confirm our slope is reflective of our data’s clustering and is significant. Our pseudo-\\(p\\)-values are then computed from our simulation results: \\(\\frac{N_{extreme} + 1}{N + 1}\\) Where \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme that our observed statistic and \\({N}\\) is the total number of simulations (Gimond, 2021). In the example above, from Manuel Gimond (see acknowledgements), only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to (1+1) / (199 + 1) = 0.01. This means that “there is a 1% probability that we would be wrong in rejecting the null hypothesis Ho.” This approach is known as a Monte Carlo simulation (or permutation bootstrap test). This spatial lag model is not just used in Moran’s I, but as you’ll read about in our extension material below, is also needed for spatial regression. In this case, the model is used to introducing a “lag” or “lagged” variable. Defining neighbours For any spatial autocorrelation test that you want to conduct, you will always need one critical piece of information: how do we define ‘neighbours’ in our dataset to enable the value comparison. Every observation in a dataset will need to have a set of neighbours to which its value is compared. To enable this, we need to determine how many or what type of neighbours should be taken into account for each observation when conducting a spatial autocorrelation test. These ‘neighbouring’ observations can be defined in a multitude of ways, based either on geometry or proximity, and include: Contiguity neighbours: Queen / Rook (plus order) Fixed Distance neighbours: Euclidean Distance (e.g. within 2km) (K) Nearest Neighbours: Certain n number of closest neighbours Different approaches of conceptualising neighbours for spatial autocorrelation measurement: contiguity, fixed distance and nearest neighbours. Source: Manuel Gimond. Depending on the variable you are measuring, the appropriateness of these different types of neighbourhood calculation techniques can change. As a result, how you define neighbours within your dataset will have an impact on the validity and accuracy of spatial analysis. Whatever approach you choose therefore needs to be grounded in particular theory that aims to represent the process and variable investigated. Whilst we will not provide a range of examples here, I can highly recommend looking at Esri’s Help Documentation on Selecting a conceptualization of spatial relationships: Best practices when you come to need to define neighbours yourself for your own analysis. For our analysis into childhood obesity, we will primarly use the Queen contiguity approach outlined in the lecture. This approach is “effective when polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity (the idea that if two polygons share a boundary, spatial interaction between them increases)” (Esri, 2021). In our case, we are using Ward boundaries which are administrative boundaries and not necessarily reflective of the neighbourhoods and local processes at play in London that might contribute to childhood obesity. By selecting the contiguity approach, we can account for spatial interaction between our Ward communities and hopefully reduce issues caused by boundary effects. Analysing Spatial Autocorrelation in Childhood Obesity and Deprivation in London Wards Now we know a little more about spatial autocorrelation and how we can test for clusters in our datasets, let’s go ahead and see if we can quantify the patterns evident in our y6_obesity_2014 data’s distribution. This is where things start to get a little trickier and more technical - so if you’ve got to this point without a break, please take one now! Defining neighbours for spatial weights Before we can calculate Moran’s I and any similar statistics, we need to first define our spatial weights matrix. This is known mathematically as \\(W_{ij}\\) and this will tell our code (when we get there) which unit neighbours which, according to our neighbour definition. For each neighbour definition, there is a different approach to implementing code to calculate the \\(W_{ij}\\) spatial weights matrix. Here, we will look at three approaches: Creating a Queen \\(W_{ij}\\) spatial weights matrix Creating a Rook \\(W_{ij}\\) spatial weights matrix Creating a Fixed Distance \\(W_{ij}\\) spatial weights matrix For either approach, we use a single line of code to create the relevant \\(W_{ij}\\) spatial weights matrix: Create our three spatial weights matrices: Queen Rook Fixed Distance (at 3000m / 3km # Create a neighbours list based on the Queen neighbour definition ward_neighbours_queen &lt;- obesity_ward_sdf %&gt;% poly2nb(., queen=T) # Create a neighbours list based on the Rook neighbour definition # Note, we just need to assign the queen parameter to FALSE ward_neighbours_rook &lt;- obesity_ward_sdf %&gt;% poly2nb(., queen=F) # Creates centroid and joins neighbours within 0 and 3000 &#39;units&#39; of the CRS, i.e. metres ward_neighbours_fd &lt;- dnearneigh(st_geometry(st_centroid(obesity_ward_sdf)),0, 3000) ## Warning in st_centroid.sf(obesity_ward_sdf): st_centroid assumes attributes are ## constant over geometries of x Visualising the differences in neighbours Creating our neighbours list through a single line of code, as above, doesn’t really tell us much about the differences between these conceptualisations. It would be useful to the links between neighbours for our three definitions and visualise their distribution across space. To be able to do this, we’ll use a few lines of code to generate a visualisation based on mapping the defined connections between the centroids of our wards. What are centroids? We have not come across the concept of a centroid yet in our analysis but we can briefly explain them here today - although next week, we’ll talk about them in a little more detail. A centroid, in it’s most simplest is the central point of an areal unit. How this central point is defined can be weighted by different approaches to understanding geometries or by using an additional variable. Again, we’ll discuss this in a little more detail next week. In our case, our centroids will reflect in the “central” point of our wards, determined by its geometric boundaries. We can calculate the centroids of our wards using one of the geometric tools from the sf library (more on these tools next week): sf_centroid. Calculate the centroid of our Wards: # Calculate the centroids of all of the Wards in London, extract only the geometry (we do not need the other data/variables in our dataframe) ward_centroid &lt;- obesity_ward_sdf %&gt;% st_centroid() ## Warning in st_centroid.sf(.): st_centroid assumes attributes are constant over ## geometries of x You can plot() the ward_centroid using the console if you’d like to see what they look like as spatial data. Now we have our ward centroids, we can go ahead and plot the centroids and the defined neighbour connections between them from each of our neighbour definitions. To do so, we’ll use the plot() function, provide the relationships via our ward_neighbours_X list and then the geometry associated with these lists from our ward_centroid() We use the st_geometry command to extract the geometry column of our centroids to create a “list” of our centroid’s coordinates. Plot our different neighbour definitions and their connections between centroids for visualisation: # We can plot these neighbours alongisde our centroids and polygon plot(ward_neighbours_queen, st_geometry(ward_centroid), col=&quot;red&quot;) plot(ward_neighbours_rook, st_geometry(ward_centroid), col=&quot;blue&quot;, add=T) plot(ward_neighbours_fd, st_geometry(ward_centroid), col=&quot;green&quot;, add=T) plot (obesity_ward_sdf$geometry, add=T) Ahah! We can see that there is definitely a difference in the number of neighbours when we use our different approaches. It seems our fixed distance neighbour conceptualisation has much connections in the center of London versus areas on the outskirts. We can see that our contiguity approaches provide a more equally distributed connection map - with our Queen conceptualisation having a few more links that our Rook. To explore further, you can either place a # comment signal in front of each line of code to “turn” it off as a layer in your map or rearrange the layering to see the different distributions of neighbours. We can also type the different neighbours objects into the console to find out the total number of “non-zero links” (i.e. total number of connections) present within the conceptualisation. You should see that Queen has 3560 non-zero links, Rook has 3474 and Fixed Difference has 8238. Whilst this code simply explores these conceptualisations - and theorectically is not needed for your analysis - it helps us understand further how our different neighbourhood conceptualisations can ultimately impact our overall analysis. Creating our spatial weights matrix With our neighbours now defined, we will go ahead and create our final (two) spatial weights objects that will be needed for our spatial autocorrelation code. At the moment, we currently have our neighbours defined as a “list” - but we need to convert it to a “neighbours” object. Convert our lists of neighbours for our two definitions (Queen and Fixed Distance) into neighbour objects using the nb2listw() function: # Create a neighbours list ward_spatial_weights_queen &lt;- ward_neighbours_queen %&gt;% nb2listw(., style=&quot;C&quot;) # Creates a neighbours list based on the Fixed Distance neighbour definition ward_spatial_weights_fd &lt;- ward_neighbours_fd %&gt;% nb2listw(., style = &#39;B&#39;) We’ll be using the Queen definition for the majority of our tests, but use the Fixed Distance for Getis-Ord. You can have a look at the differences between our neighbours lists and the neighbours objects by looking at the variables in your Environment - you should see that for our ward_spatial_weights_X object, we now have a column that defines the respective weights for each of our neighbour lists. We’re now ready to run our spatial autocorrelation tests! Running our Spatial Autocorrelation Tests We’re going to run all five autocorrelation tests in relatively quick succession: Global Moran’s I General Geary’s C Global Getis-Ord Local Moran’s I Local Getis-Ord However, we will spend the most amount of time focusing on our global and local Moran’s I. These two tests, overall, are becoming increasingly the main tests you’d expect to see in spatial autocorrelation quantification - so we’ll take some time in refining our maps for these measures. Running the code for each test is relatively similar - we pass our obesity_ward_sdf dataframe into a pull() function which extracts the variable we want to run our spatial autocorrelation test on (in our case y6_obesity_2014). The output of this is then coerced into a vector through the as.vector() which is the piped into our spatial autocorrelation test of choice (this is the only piece of code that changes - expect for Getis-Ord), which is also provided with our ward_spatial_weights_X object as a parameter. Let’s get going. Note, if you did not remove the NAs in our obesity column than the following code will not run. Make sure to find the relevant note box - just after our choropleth map - and use the code to remove the NAs from your obesity column. You will need to re-run your matrix generating code again. Running the global Moran’s I With a global Moran’s I we test how “random” the spatial distribution of these values is, producing a global Moran’s statistic from the lag approach explained earlier. The global Moran’s I statis a metric between -1 and 1: * -1 is a completely even spatial distribution of values * 0 is a “random” distribution * 1 is a “non-random” distribution of clearly defined clusters Before we run our global Moran’s I test, we will first create a Spatial Lag model plot which looks at each of the values plotted against their spatially lagged values. The graph will show quickly whether we are likely to expect our test to return a positive, zero or negative statistic. Create a Moran’s I scatterplot of our y6_obesity_2014 variable and the lagged y6_obesity_2014 variable: obesity_MI_sp &lt;- moran.plot(obesity_ward_sdf$y6_obesity_2014, listw = ward_spatial_weights_queen) We can see that there is a positive relationship between our y6_obesity_2014 variable and the lagged y6_obesity_2014 variable, therefore we are expecting our global Moran’s I test to produce a statistic reflective of the slope visible in our scatter plot. Run the global Moran’s I spatial autocorrelation test and print the result: # Run the global Moran&#39;s I test on our y6_obesity_2014 data obesity_GMI &lt;- obesity_ward_sdf %&gt;% pull(y6_obesity_2014) %&gt;% as.vector() %&gt;% moran.test(., ward_spatial_weights_queen) # Print global Moran&#39;s I result obesity_GMI ## ## Moran I test under randomisation ## ## data: . ## weights: ward_spatial_weights_queen ## ## Moran I statistic standard deviate = 22.69, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.5333351039 -0.0016339869 0.0005558933 The Moran’s I statistic calculated should be 0.53. With 1 = clustered, 0 = no pattern, -1 = dispersed, this means we can confirm that childhood obesity in London is positively autocorrelated. In other words, childhood obesity does spatially cluster. We can also consider the pseudo \\(p\\)-value as a measure of the statistical significance of the model - at &lt; 2.2e-16, which is R’s way of telling us its “very small/smaller than we can print”, we can confirm our result is (psuedo) significant. Running the Geary C’s test The Gearcy C test will tell us whether similar values or dissimilar values are clustering. We can run the Geary C test using the exact same notation - we just change the test we’d like to run. Run the Geary C’s spatial autocorrelation test and print the result: # Run the Geary C&#39;s I test on our y6_obesity_2014 data obesity_GC &lt;- obesity_ward_sdf %&gt;% pull(y6_obesity_2014) %&gt;% as.vector() %&gt;% geary.test(., ward_spatial_weights_queen) # Print Geary C&#39;s statistic obesity_GC ## ## Geary C test under randomisation ## ## data: . ## weights: ward_spatial_weights_queen ## ## Geary C statistic standard deviate = 19.668, p-value &lt; 2.2e-16 ## alternative hypothesis: Expectation greater than statistic ## sample estimates: ## Geary C statistic Expectation Variance ## 0.4622507740 1.0000000000 0.0007475703 The Geary’s C statistic calculated should be 0.46. With Geary’s C falling between 0 and 2 and 1 means no spatial autocorrelation, &lt;1 = positive spatial autocorrelation or similar values clustering and &gt;1 = negative spatial autocorreation or dissimilar values clustering, we can confirm that our data shows that similar values are clustering. We can also consider the pseudo \\(p\\)-value as a measure of the statistical significance of the model - at &lt; 2.2e-16, which is R’s way of telling us its “very small/smaller than we can print”, we can confirm our result is (psuedo) significant. Running the Getis-Ord General G test The Getis-Ord General G test tells us whether high or low values are clustering. For Getis-Ord, as it is recommended to use a proximity-based neighbour matrix, we’ll use our Fixed Distance definiton. Run the Getis-Ord’s spatial autocorrelation test and print the result: # Run the Getis-Ord General G test on our y6_obesity_2014 data, change spatial weights to fd obesity_GO &lt;- obesity_ward_sdf %&gt;% pull(y6_obesity_2014) %&gt;% as.vector() %&gt;% globalG.test(., ward_spatial_weights_fd) # Print global G statistic obesity_GO ## ## Getis-Ord global G statistic ## ## data: . ## weights: ward_spatial_weights_fd ## ## standard deviate = 10.329, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Global G statistic Expectation Variance ## 2.415636e-02 2.195887e-02 4.526244e-08 The Getis-Ord General G statistic calculated should be 0.024 (2.415636e-02). The expectation is 0.021 ( 2.195887e-02). With G &gt; expected = high values clustering and G &lt; expected = low values clustering, we can confirm that high values within our variable are tending to cluster. We can also consider the pseudo \\(p\\)-value as a measure of the statistical significance of the model - at &lt; 2.2e-16, which is R’s way of telling us its “very small/smaller than we can print”, we can confirm our result is (psuedo) significant. Thinking through our findings Before we run our local spatial autocorrelation tests, let’s just take a second to think through what our results have shown. From our three statistical tests, we can confirm that: There is clustering in our dataset. Similar values are clustering. High values are clustering. Each of these tests do tell us something different about how our data is clustered and can refine our conclusions about our data without the need for our local tests - we can conclude already that areas of high levels of childhood obesity tend to cluster in the same area. We could move forward now with our investigation into why - but we wouldn’t get a sense of where these clusters are occuring. To help with this, we need to run our local models to identify where these clusters are. Understanding this where could also help us direct our investigation into understanding why. Running the local Moran’s I test We’ll first run the local Moran’s I test, which deconstructs the global Moran’s I down to its components and then constructs a localized measure of autocorrelation, which can show different cluster types - depending on our mapping approach. To run a local Moran’s I test, the code again is similar to above - however, our output is different. Run the local Moran’s I spatial autocorrelation test and print the result: # Run the local Moran&#39;s I test on our y6_obesity_2014 data obesity_LMI &lt;- obesity_ward_sdf %&gt;% pull(y6_obesity_2014) %&gt;% as.vector() %&gt;% localmoran(., ward_spatial_weights_queen) # Print local Moran&#39;s I statistic output # Here, we will only print the first 5 lines... head(obesity_LMI) ## Ii E.Ii Var.Ii Z.Ii Pr(z &gt; 0) ## 1 0.03833697 -0.0002813579 0.02955797 0.2246240 4.111359e-01 ## 2 0.47282190 -0.0008440736 0.08838455 1.5932503 5.555202e-02 ## 3 3.82565784 -0.0016881472 0.17590102 9.1256444 3.565538e-20 ## 4 0.40863261 -0.0016881472 0.17590102 0.9783389 1.639534e-01 ## 5 2.79741620 -0.0025322208 0.26254943 5.4644253 2.322044e-08 ## 6 0.16952553 -0.0025322208 0.26254943 0.3357907 3.685143e-01 As you should see, we do not get given a single statistic as per with our global tests, but rather a table of five different statistics - that are all related back to each of the wards in our dataset. If we look at the help page for the localmoran function (run ?localmoran in your console) we can find out what each of these statistics mean: Name Description Ii Local Moran’s I statistic E.Ii Expectation of local Moran’s I statistic Var.Ii Variance of local Moran’s I statistic Z.Ii Standard deviation of local Moran’s I statistic Pr() \\(p\\)-value of local Moran’s I statistic We therefore have a local Moran’s I statistic for each of our ward, as well as a significance value plus a few other pieces of information that can help us create some maps showing our clusters. To be able to do this, we need to join our local Moran’s I output back into our obesity_ward_sdf spatial dataframe, which will then allow us to map these results. To create this join, we first coerce our local Moran’s I output into a dataframe that we then join to our obesity_ward_sdf spatial dataframe using the mutate() function from the dplyr library. In our case, we do not need to provide an attribute to join these two dataframes together as we use the computer’s logic to join the data in the order in which it was created…from the already in the original order of thr obesity_ward_sdf spatial dataframe. Coerce local Moran’s output into a dataframe, then join to our obesity_ward_sdf spatial dataframe: # Coerce obesity_LMI into dataframe obesity_LMI &lt;- as.data.frame(obesity_LMI) # Update the names for bette reference names(obesity_LMI) &lt;- c(&quot;LMI_Ii&quot;, &quot;LMI_eIi&quot;, &quot;LMI_varIi&quot;, &quot;LMI_zIi&quot;, &quot;LMI_sigP&quot;) # Join to obesity_ward_sdf obesity_ward_sdf &lt;- obesity_ward_sdf %&gt;% mutate(obesity_LMI) We now have the data we need to plot our local spatial autocorrelation maps. We’ll first plot the most simplest maps to do with our local Moran’s I test: the local Moran’s I statistic and the \\(p\\)-value of local Moran’s I statistic. Map the local Moran’s I statistic: # Map our local Moran&#39;s I statistic using tmap tm_shape(obesity_ward_sdf) + tm_polygons(&quot;LMI_Ii&quot;, style=&quot;pretty&quot;, midpoint=0, title=&quot;Local Moran&#39;s I statistic&quot;) + tm_layout(main.title= &quot;Spatial Autocorrelation of Childhood Obesity in London&quot;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) From the map, it is possible to observe the variations in autocorrelation across space. We can interpret that there seems to be a geographic pattern to the autocorrelation. However, it is not possible to understand if these are clusters of high or low values. To be able to intepret this confidently, we also need to know the significance of the patterns we see in our map - we therefore need to map the \\(p\\)-value of local Moran’s I statistic. Map the \\(p\\)-value of local Moran’s I statistic: # Significance breaks breaks &lt;- c(0, 0.05, 0.1, 1) # colours for our local Moran&#39;s I statistic colours &lt;- c(&#39;white&#39;, &quot;#a6bddb&quot;, &quot;#2b8cbe&quot; ) # Map our local Moran&#39;s I statistic using tmap tm_shape(obesity_ward_sdf) + tm_polygons(&quot;LMI_sigP&quot;, style=&quot;fixed&quot;, breaks = breaks, palette= rev(colours), title=&quot;p-value of Local Moran&#39;s I stat&quot;) + tm_layout(main.title= &quot;Spatial Autocorrelation of Childhood Obesity in London&quot;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Using our significance map, we can interpret the above clusters present in our local Moran’s I statistic more confidently. As evident, we do have several clusters that are statistically significant to the \\(p\\)-value &lt; 0.05. But what if there was a better way of mapping the local Moran’s I statistic? Well, I’m here to tell you there is - we can make a cluster map! (This is actually one of the default outputs from running a local Moran’s I test in ArcMap, for example!). Mapping the local Moran’s I statistic as cluster type We’re going to create what is known as a cluster map of our local Moran’s I statistic (Ii) (LMI) which will show areas of different types of clusters, including: HIGH-HIGH: A ward of high obesity that is also surrounded by other wards of high obesity HIGH-LOW: A ward of high obesity that issurrounded by wards of low obesity LOW-HIGH: A ward of low obesity that is surrounded by wards of high obesity LOW-LOW: A ward of low obesity that is also surrounded by other wards of low obesity And those areas that have no signficant cluster or relationship to the wards around them. Our HIGH-HIGH and LOW-LOW will show our clusters, whereas the other two cluster types reveal anomalies in our variable. To create a map that shows this, we need to quantify the relationship each of our wards have with the wards around them to determine their cluster type. We do this using their observed value and their local Moran’s I statistic and their deviation around their respective means: If a ward’s observed value is higher than the observed mean and it’s local Moran’s I statistic is higher than the LMI mean, it is designated as HIGH-HIGH. If a ward’s observed value is lower than the observed mean and it’s local Moran’s I statistic is lower than the LMI mean, it is designated as LOW-LOW. If a ward’s observed value is lower than the observed mean but it’s local Moran’s I statistic is higher than the LMI mean, it is designated as LOW-HIGH. If a ward’s observed value is higher than the observed mean but it’s local Moran’s I statistic is lower than the LMI mean, it is designated as HIGH-LOW. If a ward’s LMI was found not to be significant, the ward will be mapped as not significant. It might take a few minutes to understand takes a little while to get your head around - but essentially we’re trying to observe the differences between each Ward as those around it. For example: If we know our Ward’s observed value is less than the mean value of our overall y6_obesity_2014 variable, then it means it is of course a low value for obesity in comparison to the entirety of London. If it’s local Moran’s I statistic is also lower than the mean value of the rest of the LMI_Ii statistics, it also means it is located in a neighbourhood of low values. As a result, if this relationship is significant (determined by the \\(p\\)-value from our local Moran’s I calculation), we can call this a “low-low” cluster. To achieve this cluster map, we’ll do one additional step of data wrangling and create three new columns. The first will tell us, for each Ward, whether its observed value is higher or lower than the mean observed. The second will tell us, for each Ward, whether its LMI value is higher or lower than the mean LMI. The third will use the values of these two columns - against thes expressions outlined above - to assign each Ward with a cluster type. Determine the cluster type of each ward by comparing observed and LMI values against their respective means - and the significance value: # Calculate the means of our two columns, observed and LMI_Ii, and store as new columns within our obesity_ward_sdf obesity_ward_sdf &lt;- obesity_ward_sdf %&gt;% mutate(obs_diff = (obesity_ward_sdf$y6_obesity_2014 - mean(obesity_ward_sdf$y6_obesity_2014))) %&gt;% mutate(LMI_diff = (obesity_LMI$LMI_Ii - mean(obesity_LMI$LMI_Ii))) # Set a significance threshold - note, you can alter this according to the signficance value you want to account for signif &lt;- 0.05 # Generate column with cluster type, using values above # Note our first the use of the case_when function &amp; more boolean logic statements # See the case_when documentation for furher info on it! obesity_ward_sdf &lt;- obesity_ward_sdf %&gt;% mutate(cluster_type = case_when(obs_diff &gt; 0 &amp; LMI_diff &gt; 0 &amp; LMI_sigP &lt; signif ~ &quot;High-High&quot;, obs_diff &lt; 0 &amp; LMI_diff &lt; 0 &amp; LMI_sigP &lt; signif ~ &quot;Low-Low&quot;, obs_diff &lt; 0 &amp; LMI_diff &gt; 0 &amp; LMI_sigP &lt; signif ~ &quot;Low-High&quot;, obs_diff &gt; 0 &amp; LMI_diff &lt; 0 &amp; LMI_sigP &lt; signif ~ &quot;High-Low&quot;, LMI_sigP &gt; signif ~ &quot;No Significance&quot;)) Now we have a column detailing our cluster types, we can create a cluster map that details our four cluster types as well as those that are not significant. Creating a cateogrical map in R and using tmap is a little tricky - and we’ll need to do some preparing of our colour palettes to ensure our data is mapped correctly. Essentially, we’ll need to map the number and type of colours to the variables present in our dataset manually (yes, manually, in code!) in order to create a good visualisation. Unfortunately, this is where tmap and mapping by code does let us down a little. To do this, we first need to figure out how many cluster types we have in our cluster_type field. You can check this by simply looking at the column within the obesity_ward_sdf spatial dataframe or we can use a function from the dplyr library to count the different values within our cluster_type variable. In your console, check the values in our cluster_type field: # Count the different values within our `cluster_type` variable count(obesity_ward_sdf, cluster_type) ## Simple feature collection with 3 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## CRS: 27700 ## # A tibble: 3 x 3 ## cluster_type n geometry ## * &lt;chr&gt; &lt;int&gt; &lt;MULTIPOLYGON [m]&gt; ## 1 High-High 69 (((510346.6 178212.3, 510365.3 178209.8, 510372.8 178208… ## 2 Low-High 85 (((539498.2 190420, 539501.7 190482.2, 539499.2 190536.1… ## 3 No Significan… 459 (((530488.7 160167.3, 530498.7 160159.8, 530501.1 160150… We can see we actually on have two cluster types present within our dataset - alongside the No Significance value. We’ll need to ensure our palette therefore only includes two colours for these two cluster types, plus a white colour for No Significance. Let’s go ahead and create our palette and map our cluster types. Create our cluster map using the tmap library and our pre-designated colours: # Set colours to be used: White (No Significance), Light Blue (Low-High) and Dark Red (High-High) # Set this to pal pal &lt;- c(&quot;#F5F5F5&quot;, &#39;#91bfdb&#39;, &#39;#67001f&#39;) # In the future, if you have more clusters, you would add in additional colours accordingly # Plot the cluster type of our obesity LMI Ii statistic tm_shape(obesity_ward_sdf) + tm_polygons(col = &quot;cluster_type&quot;, palette = rev(pal), title=&quot;Cluster Type&quot;) + tm_layout(main.title=&#39;Cluster Map of Childhood Obesity in London in 2014&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) # Note, we &quot;reversed&quot; our colour palette as when the map was drawn we saw that our colours were assigned to the wrong values and this was the quickest solution! And there we have it - within one map, we can visualise both the relationship of our Wards to their respective neighbourhoods and the significance of this relationship from our local Moran’s I test. This type of map is called a LISA map - and is a great way of showing how a variable is actually clustering. It took us a little bit of effort, but now we have the code written, we’ll able to add to and re-use both our clustering code and our colour palette in future tests. Running the local Getis-Ord test The final test we’ll run today is looking at the local Getis-Ord model, which will produce the Gi* (pronounched G-i-star) statistic. This statistic will identify hot and cold spots by looking at the neighbours within a defined proximity to identify where either high or low values cluster spatially and recognising statistically significant hot-spots as those areas of high values where other areas within a neighbourhood range also share high values too (and vice versa for cold spots). Don’t worry - making maps from this statistic is much simpler than the local Moran’s I. Let’s go ahead and calculate our local Gi* statistic. Run the local Gi* statistic spatial autocorrelation test and print the result: # Run the local Gi* test on our y6_obesity_2014 data, note we again use the fd weights obesity_LGO &lt;- obesity_ward_sdf %&gt;% pull(y6_obesity_2014) %&gt;% as.vector() %&gt;% localG(., ward_spatial_weights_fd) # Print the local Gi* statistic head(obesity_LGO) ## [1] 0.4009978 -3.4036923 -4.3086510 -4.5931460 -3.8301451 -4.3015237 By printing the results of our test, we can see that the local Getis-Ord test is a bit different from a local Moran’s I test as it only contains just a single value - the z-score. The z-score is a standardised value relating to whether high values or low values are clustering together, which we call the Gi* statistic. We can join this output, a list of our Gi* values, to our obesity_ward_sdf spatial dataframe and map the result. Map the the local Gi* statistic: # Join the local Gi* statistic to `obesity_ward_sdf` spatial dataframe obesity_ward_sdf &lt;- obesity_ward_sdf %&gt;% mutate(obesity_LGO_G = as.numeric(obesity_LGO)) # Map our results using tmap # Again, well use a customised colour scheme - this time from the colorbrewer pacakge GIColours&lt;- rev(brewer.pal(8, &quot;RdBu&quot;)) tm_shape(obesity_ward_sdf) + tm_polygons(&quot;obesity_LGO_G&quot;, style=&quot;pretty&quot;, palette=GIColours, midpoint=0, title=&quot;Local Gi* statistic&quot;) + tm_layout(main.title=&#39;Hot/Cold Spot Map of Childhood Obesity in London in 2014&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Our map shows quite clearly hot and cold spots of obesity across London - how does this compare with our local Moran’s I? Do we see the same clusters? Using Cluster Analysis for Distribution Analysis Through conducting our spatial autocorrelation tests, we can visually confirm the presence of clusters in our y6_obesity_2014 variable - and provide a significance value associated with these tests. This provides us with a way to explain our distributions quantitatively during the analysis of our results - it also creates some pretty cool looking maps. Now you have the code, you’ll be able to repeat this analysis on any variable in the future - just be aware of substituting your spatial dataframes and respective variables correctly. To get you into practice, I’d like you to complete the following assignment. Assignment 2: Spatial autocorrelation of the IDACI 2010 variable For this week’s assignment, I’d like you to find out whether our IDACI_2010 variable contains spatial clustering. I’d like you to produce: A choropleth map showing the distribution of the variable values All three global spatial autocorrelation statistics and explain what they show A local Moran’s I map showing our four cluster types. A local Getis-Ord hotspot map I’d like you to then compare your results to the output of our y6_obesity_2014 values. Export your maps for both variables and values into a word document. I’d recommend setting your page to Landscape, and placing your different variables and the respective maps side by side to enable comparisons. Beneath your maps, please please provide a 100 word write-up about the general distribution and clusters in both the y6_obesity_2014 and IDACI_2010 variables For example: Do both variables have clusters? Do we see similar clusters in similar locations? What might this tell us about the relationship between deprivation and obesity in children in London? Once complete, upload this into the newly created obesity_maps folder in your relevant seminar folder. We’ve just learnt all about spatial autocorrelation and how to run the five different spatial autocorrelation tests - and map the output of the local tests in R. Wow - we’ve got through a lot today and haven’t tackled the last bit of work that we’d planned to do at the outset - that is looking at analysing relationships between variables. We’re actually going to save most of this for the optional practical in Week 10, but the following provides a short introduction that you’re welcome to ignore and head straight to the Recap - Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression section. Analysing Relationships Identifying clusters and their distributions in our datasets is one way in identifying relationships between variables - but this still relies on our visual interpretation. As you’re likely to be aware, we can look at the relationship between variables quantitatively through both graphs and stastistics - the latter, through creating (with continuous data) linear bivariate and multivariate models that aim to calculate co-efficients of these relationships. For example, we could look to map the relationship between our obesity and deprivation variables. by using a simple scatter plot and through a simple linear regression model. Let’s see if we can test the relationship between our obesity and deprivation variables. Note, I will not explain the code in this section as I have done previously. I will provide an explanation of the code in the Week 10 optional practical. Create a simple scatter plot to look at the relationship between our two variables, y6_obesity_2014 and IDACI_2010: # Plot our obesity variable v. our deprivation variable plot(obesity_ward_sdf$y6_obesity_2014, obesity_ward_sdf$IDACI_2010) As you can see, it looks like there is a relationship between the two - but instead of a linear relationship, we’re almost looking at something logarithmic. This is because, if you have completed Assignment 1, you would know that the distribution of IDACI_2010 is positively skewed - as a result, we need to transform the values into something more normally distributed. For now, we will use the sqrt() (square root) transformation, which is one of the main transformations John Tukey recommended on his “Ladder of Powers” to adjust for skewed data (more on this in Week 10). Create a simple scatter plot to look at the relationship between our two variables, y6_obesity_2014 and the square root of IDACI_2010: # Plot our obesity variable v. our deprivation variable plot(obesity_ward_sdf$y6_obesity_2014, sqrt(obesity_ward_sdf$IDACI_2010)) Woohoo! That’s better - now it looks like we’ve got a linear relationship between our two variables. For this relationship, we could now model the linear relationship between the two using a linear regression model. Run a linear regression across our two variables, using the lm() function: # Plot our obesity variable v. our deprivation variable lm_model &lt;- obesity_ward_sdf %&gt;% lm(formula = y6_obesity_2014 ~ sqrt(IDACI_2010)) # Print summary of model summary(lm_model) ## ## Call: ## lm(formula = y6_obesity_2014 ~ sqrt(IDACI_2010), data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9484 -2.3052 0.0062 1.9810 17.2709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.5569 0.5342 14.15 &lt;2e-16 *** ## sqrt(IDACI_2010) 26.3623 0.9579 27.52 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.44 on 611 degrees of freedom ## Multiple R-squared: 0.5535, Adjusted R-squared: 0.5528 ## F-statistic: 757.4 on 1 and 611 DF, p-value: &lt; 2.2e-16 We can see that we have a significant positive relationship between our two variables - therefore its likely that to some extent, the level of childhood obesity of a Ward is correlated with its level of deprivation - as suggested by our literature. Now the regression model we have be experimenting with so far is a simple bivariate (two variable) model. One of the nice things about regression modelling is while we can only easily visualise linear relationships in a two (or maximum 3) dimension scatter plot, mathematically, we can have as many dimensions / variables as we like. We’re going to go ahead and run a multivariate model on ALL of our variables we’ve added to today as explantory variables of the childhood obesity of a ward. We are, for now, going to IGNORE all of the assumption tests that we should run for this, e.g. collinearity, Pearson’s test etc. - we’ll go into this in more detail in Week 10 so your time has not been wasted creating our current final dataset. We will also apply the square root transformation to all our variables except the percentage of no qualifications variable - if you’ve completed Assignment 1, you would know that all of our variables are positively skewed, bar this variable which shows a normal distribution. Run a multivariate linear regression model across ALL of our variables, using the lm() function: # Plot our obesity variable v. our explanatory variables lm_Mmodel &lt;- obesity_ward_sdf %&gt;% lm(formula = y6_obesity_2014 ~ sqrt(IDACI_2010) + sqrt(med_house_price_2014) + sqrt(mean_hh_income_2013) + sqrt(per_no_adult_employ_2011) + per_no_qual_2011 + sqrt(per_deficiency_greenspace_2012) + sqrt(PTAL_2014)) # Print summary of model summary(lm_Mmodel) ## ## Call: ## lm(formula = y6_obesity_2014 ~ sqrt(IDACI_2010) + sqrt(med_house_price_2014) + ## sqrt(mean_hh_income_2013) + sqrt(per_no_adult_employ_2011) + ## per_no_qual_2011 + sqrt(per_deficiency_greenspace_2012) + ## sqrt(PTAL_2014), data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2151 -2.0383 -0.1569 2.0714 15.8856 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4935665 2.9363681 0.509 0.61119 ## sqrt(IDACI_2010) 12.2333226 2.4799333 4.933 1.05e-06 *** ## sqrt(med_house_price_2014) 0.0003341 0.0024408 0.137 0.89117 ## sqrt(mean_hh_income_2013) -0.0053226 0.0146778 -0.363 0.71701 ## sqrt(per_no_adult_employ_2011) 1.7102246 0.5545708 3.084 0.00214 ** ## per_no_qual_2011 0.2093637 0.0395679 5.291 1.70e-07 *** ## sqrt(per_deficiency_greenspace_2012) 0.1136173 0.0420109 2.704 0.00703 ** ## sqrt(PTAL_2014) 3.4436098 0.6170026 5.581 3.61e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.243 on 605 degrees of freedom ## Multiple R-squared: 0.6071, Adjusted R-squared: 0.6026 ## F-statistic: 133.6 on 7 and 605 DF, p-value: &lt; 2.2e-16 Overall, from our quick multivariate model we can see we do have several signficant relationships between our explanatory variables and our obesity variable. Our deprivation variable is still the strongest predictor, but the qualifications and access to public transport both appear to also influence childhood obesity. This would make an extremely interesting analysis to write-up (ignoring our assumptions ;) ) and given the literature mentioned at the start of the practical. However, as we have seen from our spatial autocorrelation analysis, our obesity variable (and likely our other variables) are very spatial in their distribution - and currently, we do not incorporate this spatial distribution into our model. So if we want to develop a regression model for childhood obesity in London, we may have to recognise this spatial component. Furthermore, these types of global regression models also assumes independence of observations: what happens in one ward is not related to what happens in another. However, we know from our analysis today, that this is not always the case because of spatial autocorrelation. This brings us to the second of two key properties of spatial data: spatial heterogeneity. With the underlying process (or processes) that govern a spatial variable likely to vary across space, a single global relationship for an entire region of study may not adequately model the process that governs outcomes in any given location of the study region. As a result, multiple methods have been developed to incorporate ‘space’ into traditional regression models, including spatial lag models, spatial error models, and Geographically Weighted Regression. These models primarily look to include the spatial lag variable that we created earlier using our spatial weights matrix into their prediction modelling and calculations. Each of these three approaches, spatial lag models, spatial error models, and Geographically Weighted Regression, utilise the variable in different ways - this will be further explainedi nthe Week 10 practical. Overall, it is important to recognise that this spatial lag variable not only underpins our spatial autocorrelation analysis but also our analysis of relationships over space. We will look into how we can use these models to analyse the above relationships in our dataset as one of our optional practicals in Week 10 - but for now, I would like you to be aware that when it comes to spatial data such as what we’re dealing with here - general regression models are not nearly sufficient enough to estimate the relationships between spatial variables accurately. Extension: Geographically Weighted Regression in R If you’re keen to get ahead of the curve and are still not exhausted from all this coding, Prof James Cheshire in his previous years as convenor of Gecomputation, alongiside Guy Lansley, put together a very short practical on how to implement Geographically Weighted Regression in R. This practical is now available on the CDRC website here and is the last in the series. The code is slightly outdated - and will use sp for its loading of spatial data. You are welcome to look through this tutorial if you’d like to see what GWR looks like - for example, if you’ve deployed our multivariate model above, you might want to map the residuals as per their code to see if we need to take into account spatial dependence in our model. Note, I’ll be re-writing this tutorial for our current dataset for Week 10 - but you are of course welcome to utilise their code to go ahead and apply this to their or our dataset. ALternatively, you can wait until Week 10 for this! Recap - Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Congratulations on finishing our first workshop on analysing spatial patterns in R. You’ve learnt about our main topic of interest today - spatial autocorrelation - and how to deploy various spatial autocorrelation methods on a variable, and then map these results. You’ve also learnt about the different types of neighbour definitions we can use to ultimately create our spatial lag models - and use within our spatial autocorrelation tests. It might not sound like much when summarised into sentence or two - this learning is critical to your future analysis of both the distribution of and relationships between variables in your work. As highlighted in our small section on Analysing Relationships, understanding that variables can be spatially dependent will determine how you are able to analyse the relationships between them. And if you do need to take into account this spatial dependence, as you’ll see in action in Week 10’s practical, understanding how a spatial lag model is computed will help you substantially with moving forward with spatial regression. Learning Objectives You should now hopefully be able to: Understand how to analyse distributions of (areal) spatial data through visual and statistical analysis Explain the different approaches to defining neighbours within spatial autocorrelation Run different types of spatial autocorrelation techniques and understand their differences Understand the basics of incorporating the issue of spatial autocorrelation into regression Acknowledgements Part of this page is adapted from GEOG0114: Principles of Spatial Analysis by Dr Joanna Wilkin (this Workbook’s author), Dr Justin Van Dijk and Alfie Long at UCL, Spatial Autocorrelation by Dr Manuel Gimond at Colby College (thank you for the images!) and CASA005: GWR and Spatially Lagged Regression by Dr Andrew MacLachlan at CASA, UCL. The datasets used in this workshop (and resulting maps): Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Public Health England © Crown copyright 2021 "],["analysing-spatial-patterns-ii-geometric-operations-spatial-queries.html", "7 Analysing Spatial Patterns II: Geometric Operations &amp; Spatial Queries", " 7 Analysing Spatial Patterns II: Geometric Operations &amp; Spatial Queries Welcome to Week 7 in Geocomputation! In a change to advertised content, this week, we’ll be looking at the use of geometric operations within spatial data processing and analysis, moving Point Pattern Analysis to next week, Interpolation to Week 9 and Geodemographics to Week 10. We’ll be covering Advanced Visualisations within each of the following weeks. Geometric operations and spatial queries are not really a theorectical topic per se in spatial “pattern” analysis but rather essential building blocks to overall spatial data processing and analysis. This is because - and the clue is in the name - they conduct incredibly useful operations and queries on or using the geometry of our datasets, from calculating the area covered by an individual polygon in an areal unit dataset, or subsetting the spatial extent of a dataset based on another, to running buffer and point-in-polygon calculations. Whilst originally my intentions were to combine this week with your work on Point Pattern Analysis, after writing up this week’s practical, I’ve realised that this is more than enough content for you to look through for this week’s Geocomputation material. We’ll have a short lecture introducing these different operations and a longer practical session looking at their application in terms of spatial data cleaning, validation and analysis and we’ll also learn how to download data from OpenStreetMap. In terms of data visualisation, you’ll learn how to make interactive maps with tmap, small multiples and proportional symbol maps. Week 7 in Geocomp Video on Stream This week’s content introduces you to geometric operations and their different uses in spatial analysis. We have two areas of work to focus on: Understanding different geometric operations and spatial queries and their respective applications. Using geometric operations and spatial queries to clean, validate and analyse spatial data. This week’s content is split into 3 parts: Workshop Housekeeping (20 minutes) Geometric Operations &amp; Spatial Queries (40 minutes) Geometric Operations &amp; Spatial Queries in Action (90 minutes) This week, we have 1 lecture and 2 assignments within this week’s workshop. Learning Objectives By the end of this week, you should be able to: Understand how to use different geometric operations and spatial queries within your spatial analysis workflow for data cleaning, processing and analysis Be able to implement geometric operations, including clips and unions, and spatial queries within R and sf Know how to download data from OpenStreetMap using the osmdata package Make small multiples of maps and arrange them togoether Create proportional symbol maps This week, we’ll be investigating bike theft in London in 2019 - and look to confirm a very simple hypothesis: that bike theft primarily occurs near tube and train stations. We’ll be investigating its distribution across London using the point data provided within our crime dataset. We’ll then compare this distribution to the location of train and tube stations using specific geometric operations and spatial queries that can compare the geometry of two (or more) datasets. To complete this analysis, we’ll be using two types of spatia data: Bike theft in London in 2019: A 2019 version of our crime dataset for London. With COVID-19 changing commuting patterns etc., we will focus on 2019. Train and Tube Stations locations in London We’ll also learn how to download data from OpenStreetMap as well as use an interactive version of tmap to explore the distribution of the locations of individual bike theft against the locations of these stations. Workshop Housekeeping Let’s get ourselves ready to start our lecture and practical content by first downloading the relevant data and loading this within our script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk7-bike-theft-analysis.r. At the top of your script, add the following metdata (substitute accordingly): # Analysing bike theft and its relation to stations using geometric analysis # Script started March 2021 NAME Dependencies (aka libraries) Now we’ll install the libraries we need for this week. All of the geometric operations and spatial queries we will use are contained within the sf library. We will install the leaflet library to enable interactive mapping in tmap. We will also use osmdata to downlad data directly from OpenStreetMap. Remember to use the `install.packages(“package”) command in your console. Within your script, add the following libraries for loading: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) library(janitor) library(RColorBrewer) library(leaflet) library(osmdata) Remember to select the lines of code you want to run and press CMD (Mac)/CTRL(Windows) + Enter/Return - we won’t remind you to run each line of code in the remainder of the practical sessions. Datasets for this week This week, we’ll start off using three datasets: London Ward boundaries from 2018 (this should already be in your raw data folder) 2019 crime in London from data.police.uk Train and Tube Stations from TfL. As we’ve already been through the data processing required for the 2020 crime dataset, this week, I’ll save you on some data cleaning! 1. 2019 crime in London You can find a pre-stacked single csv of all crime from 2019 here. This csv was created using the exact same steps we completed in Practical 3 and 4. Therefore, if you would like to challenge yourself, go ahead and create this dataset yourself using the Command Line and R code we used in those practicals to create your 2019 crime csv. Whichever approach you use, store your final all_crime_2019.csv in your data/raw/crime folder. 2. London Train and Tube Stations Through an online search, I’ve managed to find a KML version of TfL’s Train and Tube Stations data feed that is accessible in their Open Data portal. You can downlaod this KML directly here as it does not seem to be available directly from the portal. Move your download to your raw data folder and create a new transport folder to contain it. Loading our data Let’s first load our London Ward shapefile from our raw -&gt; boundaries -&gt; 2018 folder. Load the 2018 London Ward boundaries: # Read in our 2018 London Ward boundaries london_ward_shp &lt;- read_sf(&quot;data/raw/boundaries/2018/London_Ward.shp&quot;) We’ve used this dataset quite a few times - and if you remember in Week 5, when we first utilised the data, we checked it’s Coordinate Reference System (CRS) to confirm that it was in British National Grid (BNG). We’ve slacked off recently with double-checking our CRSs because we’ve essentially only used one type of spatial data - these Ward shapefiles that have all come from the same source. This week, we’re going to double-check the CRS of all our datasets. If you remember the police crime data is provided with a Latitude and Longitude (i.e. coordinate references used in the Geographic Coordinate System of WGS84), so at least for this dataset, we know we’re likely going to run into differences in our CRS. We will also need to see what CRS our Tube and Train Stations data is provided in. Check the CRS of our london_ward_shp spatial dataframe: # Get the CRS 2018 London Ward boundaries st_crs(london_ward_shp) ## Coordinate Reference System: ## User input: 27700 ## wkt: ## PROJCS[&quot;OSGB 1936 / British National Grid&quot;, ## GEOGCS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB_1936&quot;, ## SPHEROID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## AUTHORITY[&quot;EPSG&quot;,&quot;7001&quot;]], ## TOWGS84[446.448,-125.157,542.06,0.15,0.247,0.842,-20.489], ## AUTHORITY[&quot;EPSG&quot;,&quot;6277&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4277&quot;]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;latitude_of_origin&quot;,49], ## PARAMETER[&quot;central_meridian&quot;,-2], ## PARAMETER[&quot;scale_factor&quot;,0.9996012717], ## PARAMETER[&quot;false_easting&quot;,400000], ## PARAMETER[&quot;false_northing&quot;,-100000], ## UNIT[&quot;metre&quot;,1, ## AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], ## AXIS[&quot;Easting&quot;,EAST], ## AXIS[&quot;Northing&quot;,NORTH], ## AUTHORITY[&quot;EPSG&quot;,&quot;27700&quot;]] Of course it should be of no surprise that our london_ward_shp spatial dataframe is in BNG / ESPG: 27700, however it’s always good to check! Let’s go ahead and read in our stations dataset. Load the London Tube and Trains dataset: # Read in our London stations dataset london_stations &lt;- read_sf(&quot;data/raw/transport/stations.kml&quot;) This dataset is provided as a kml file, which stands for Keyhole Markup Language (KML). KML was originally created as a file format used to display geographic data in Google Earth. As a result, if we think back to our lectures on Geographic vs. Projected Coordinate Systems (GCS/PCS), considering GoogleEarth is a 3D application, it is quite likely that the CRS associated with this dataset will be a GCS, not a PCS. So we definitely need to check what CRS this dataset is in and decide whether we’ll need to do some reprojecting. Check the CRS of our london_stations spatial dataframe: # Get the CRS of or london stations st_crs(london_stations) ## Coordinate Reference System: ## User input: 4326 ## wkt: ## GEOGCS[&quot;WGS 84&quot;, ## DATUM[&quot;WGS_1984&quot;, ## SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, ## AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] Ahah - our suspicions are confirmed! Our london_stations spatial dataframe is in WGS84 / EPSG: 4326. We’re going to need to reproject our data in order to use this dataframe with our london_ward_shp spatial dataframe. Luckily in R and the sf library, this reprojection is a relatively straight-forward transformation, requiring only one function: st_transform. The function is very simple to use - you only need to provide the function with the dataset and the code for the new CRS you wish to use with the data. For now, we will simply store the result of this transformation as an overwrite of our current variable - but you could, in the future, rewrite this code to use pipes to pipe this transformation when loading the dataset if you are already aware of its CRS. Transform our london_stations spatial dataframe from WGS84 / EPSG: 4326 to BNG / EPSG: 27700: # Reproject our london_stations spatial dataframe from WGS84 to BNG Overwrite our # original variable london_stations &lt;- st_transform(london_stations, 27700) We can double-check our new variable is in the correct CRS by using the st_crs command, but this time simply enter it into into the console: # Check the CRS of or london stations - TYPE INTO CONSOLE st_crs(london_stations) ## Coordinate Reference System: ## User input: EPSG:27700 ## wkt: ## PROJCS[&quot;OSGB 1936 / British National Grid&quot;, ## GEOGCS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB_1936&quot;, ## SPHEROID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## AUTHORITY[&quot;EPSG&quot;,&quot;7001&quot;]], ## TOWGS84[446.448,-125.157,542.06,0.15,0.247,0.842,-20.489], ## AUTHORITY[&quot;EPSG&quot;,&quot;6277&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4277&quot;]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;latitude_of_origin&quot;,49], ## PARAMETER[&quot;central_meridian&quot;,-2], ## PARAMETER[&quot;scale_factor&quot;,0.9996012717], ## PARAMETER[&quot;false_easting&quot;,400000], ## PARAMETER[&quot;false_northing&quot;,-100000], ## UNIT[&quot;metre&quot;,1, ## AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], ## AXIS[&quot;Easting&quot;,EAST], ## AXIS[&quot;Northing&quot;,NORTH], ## AUTHORITY[&quot;EPSG&quot;,&quot;27700&quot;]] You should see that our london_stations spatial dataframe is now in BNG / EPSG: 27700. Deciding which CRS to use This is only second time in our practicals that we’ve needed to consider reprojecting our datasets - so how did I choose to use BNG? Whilst we have two datasets in WGS84 (as we’ll confirm below) and one in BNG, unfortunately in spatial analysis, majority simply does not rule. As you should remember from our lectures in Week 3 on Projections and Coordinate Reference Systems, we should always choose a CRS that best represents the reality on the ground for our area of interest and the type of analysis we want to conduct. This week, we will be conducting a lot of analysis that is reliant on the distance between two points being captured accurately and we’ll also be only focusing on London as our area of interest. In this case, therefore, BNG is the most sensible and accurate choice moving forward (it rarely isn’t when it comes to mapping data about Great Britain!). If we didn’t transform our data, not only would our analysis be incorrect but our visualisations would also not work correctly. We’re now ready to load our final dataset - our csv that contains our crime from 2019. From this csv, we want to do three things: Extract only those crimes that are bicycle thefts, i.e. crime_type == \"bicycle theft\". Convert our csv into a spatial dataframe that shows the locations of our crimes, determined by the latitude and longitudes provided, as points. Transform our data from WGS84 / 4326 to BNG / 27700. Now, we’re getting pretty used to looking at code and cleaning data - so it should come of no suprise to you that we can complete all three steps at once, using our handy %&gt;% pipe operator. We’ll explain each line of code in the comments to help you read through the different functions, but the main function you have yet come across is: st_as_sf. What this function does is convert any vector, matrix or dataframe you provide in the argument into a point spatial dataframe, as long as you provide it with two columns to use for their coordinates (a bit like we did when using Q-GIS). You should also include the CRS for which those coordinates are based on, i.e. in our case, WGS84 / 4236. Load, clean and process our 2019 crime csv: # Read in our crime data csv from our raw data folder bike_theft_2019 &lt;- read_csv(&quot;data/raw/crime/all_crime_2019.csv&quot;) %&gt;% # clean names clean_names() %&gt;% # filter according to crime type and ensure we have no NAs in our dataset filter(crime_type == &quot;Bicycle theft&quot; &amp; !is.na(longitude) &amp; !is.na(latitude)) %&gt;% # select just the longitude and latitude columns select(longitude, latitude) %&gt;% # transform into a point spatial dataframe note providing the columns as the # coordinates to use plus the CRS, which as our columns are long/lat is # WGS84/4236 st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4236) %&gt;% # convert into BNG st_transform(27700) ## Warning: Missing column names filled in: &#39;X1&#39; [1] We now have our three datasets loaded, it’s time for a little data checking. We can see just from our Environment window that in total, we have 302 stations and 18,744 crimes to look at in our analysis. We can double-check the (Attribute) tables of our newly created spatial dataframes to see what data we have to work with. You can either do this manually by clicking on the variable, or using commands such as head(), summary() and names() to get an understanding of our dataframe structures and the field names present - you can choose your approach, but make sure to look at your data. As you should remember from the code above, for our bicycle theft data, we actually only have our geometry column because this is all that we extracted from our crime csv. For our london_stations spatial dataframe, we have a little more information, including the name of the station and its address - as well as its geometry. Let’s go ahead and do the next thing on our “how to do spatial analysis” checklist and check the distribution of our data - for the first time, we’ll actually map all of our data together on one map. We’ll only customise this a little bit for now - but you could, of course, make this look a lot fancier than the map we’ll produce now! Map all three layers of data onto a single map using tmap: # Plot our London Wards first tm_shape(london_ward_shp) + tm_fill() + # Then bike crime as blue tm_shape(bike_theft_2019) + tm_dots(col = &quot;blue&quot;) + # Then our stations as red tm_shape(london_stations) + tm_dots(col = &quot;red&quot;) + # Add a north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + # And a scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Wow, intesting to see right? Let’s think about the distribution of our data - we can already see that our bike theft is obviously highly concentrated in the centre of London although we can certainly see some clusters in the Greater London areas. This certainly is going to be an interesting dataset to analyse! Let’s go ahead and temporally remove the bike theft data from our map for now to see where our tube and train stations are located. To remove the bike data, simply put a comment sign in front of that piece of code and re-run the code: # Plot our London Wards first tm_shape(london_ward_shp) + tm_fill() + # Then bike crime as blue COMMENTED OUT tm_shape(bike_theft_2019) + # tm_dots(col=&#39;blue&#39;) + Then our stations as red tm_shape(london_stations) + tm_dots(col = &quot;red&quot;) + # Add a north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + # And a scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Hmmm - well that’s interesting. We can see our train and tube stations are only present in primarily the north of London - and not really present in the south. This isn’t quite right! Whilst this is easier to spot if you’re familiar with the London train network - but there should definitely be train stations in the south of London…?! If we look at the distribution of our stations - and use our knowledge of the London train and tube networks (I appreciate not all of us might have!) - we can quickly identify that our original dataset only contains those train stations used by TfL within the tube network rather than all the stations in London: Transport for London’s Geographic Tube and Train Network / Station Map. It looks like we’re going to need to find some more data for our analysis! But this isn’t the only problem with our dataset - we can see that both our bike_theft spatial dataframe and our london_stations spatial dataframe extend beyond our London boundary. We therefore need to do some further cleaning and processing of our datasets to get them into a more “tidy” and complete format for analysis. But so far, we’ve only completed cleaning and processing on the dataframe aspect of our spatial data, never on the geometry or spatial component of our spatial data - so, how do we do this? The answer is: geometric operations and spatial queries. Geometric Operations &amp; Spatial Queries Geometric operations use geometry functions to take spatial data as input (one or more datasets), analyze the data, then produce output data that is the derivative of the analysis performed on the input data (Esri, 2020). Spatial queries use the geometry of two datasets to compare them against specific spatial requirements - the result of these queries can then be used to select or remove features that do or do not meet these requirements. This type of spatial data manipulation and analysis is often known as geoprocessing. These operations and queries are the building blocks of GIScience and it is incredibly impressive how much you can achieve with some very simple functions. They can be used to clean, tidy, process, validate and even analyse data - all through simply managing and manipulating geometry within space. But, as these functions are spatially-dependent, it is incredibly important that we use a CRS that accurately represents our data within space, i.e. it tries to balanace the distortion between distance, area and shape (although which you prioritise out of these can be dependent on the types of analysis you are completing). We are using BNG, which, as we know from our previous lecture and above, does the best job of preserving all three for data in Great Britain. This week’s lecture outlines geometric operations and spatial queries and provides examples of what they can be used to accomplish with spatial data. But before you get started with the lecture, I’d recommend downloading the sf cheatsheet - and preferably printing it off (yes, for this once, print!) - to use as a guide as we move through the lecture and the next section. You should read each of the functions available and highlight any that you remember seeing in previous practicals. You, of course, do not need to know each of these off by heart, but reading through these examples can give you a general understanding of the operations and queries you can conduct. Lecture: What are Geometric Operations &amp; Spatial Queries? Slides | Video on Stream Geometric Operations &amp; Spatial Queries In Action As you’ll find out in this week’s practical, much of the “geoprocessing” that we need to complete with our data can be completed using different approaches of both geometric operations and spatial queries - and, as a result, can be completed in many different orders. When we use geometric operations, we are always taking a data input (or inputs), manipulating their geometry according to the function/operation you’re using and then providing a modified geometry as a new data output (a spatial dataframe in R or dataset in Q-GIS, for example). The use of these operations can be quite time-consuming and, in many scenarios, we often do not want edited geometry returned to us, but simply the information on the result of our query that we can then attribute to our original dataset. For example, does this bike theft fall within 400m of a station - yes or no? In this case, spatial queries are the way forward. As you’ll see below, a spatial query will only return a TRUE or FALSE statement to your question. In Q-GIS, this is the “selection” process seen in the above GIF; in R and sf, you’ll be returned either a list of the indexes of those features that are TRUE to the question or a matrix that states either TRUE or FALSE for every single feature. With spatial queries, you do not edit the geometry of the dataset you’re analysing, but within R, you will end up with an attribute that you can join to said dataset (as we’ll do today). With that TRUE/FALSE output joined to our spatial dataframe, you can, at a later point in time, use that attribute to select only those that are TRUE or FALSE to create a new dataset if you want. Spatial queries are often much quicker to implement, but the disadvantage is that you won’t end up with an additional dataset that you might want for visualisation purposes. We’ll see this below when using a buffer to analyse our bike theft data. Practical 6 - Part One: Geometric Operations &amp; Spatial Queries Approaches Of all spatial data processing and analysis, geometric operations and spatial queries are certainly best explained through demonstration! Therefore, to clean, process and analysis our data, we can demonstrate a range of operations and queries highlighted by our first lecture. We’re going to need to reduce both our our bike_theft spatial dataframe and our london_stations spatial dataframe to only contain data within our london_ward_shp spatial dataframe. This is known as either a subset or a clip and there are differences between them and how you implement them. Furthermore, we need additional data that adds in the remaining train stations that exist in London - but are managed by companies other than TfL. We’ll therefore look into how we can use geometric operations and spatial queries to compare two similar datasets (as well as download data from OpenStreetMap). Once we’ve created our two final datasets, we can then think of ways we can look to quantify the relationship between the locations of bike theft and train stations - using both geometric operation and spatial query approaches. Let’s get started with the simplest task at hand - “clipping” our dataset to our london_ward_shp spatial dataframe. Clipping our bike theft data to London When we want to reduce a dataset to the spatial extent of another, there are two different approaches to conducting this in spatial analysis - a subset or a clip - which each deal with the geometry of the resulting dataset in slightly different ways. A clip-type operation works a bit like a cookie-cutter - it will take the geometry of “dough” layer (i.e. the layer you want to clip), places a “cookie-cutter” layer on top (i.e. the layer you want to clip by) and then returns only the dough contained within the cookie-cutter. This will mean that the geometry of our resulting “dough” layer will be modified, if it contains observation features that extend further than the “cookie-cutter” extent - it will literally “cut” the geometry of our data. Clipping demo’ed in QGIS for easy visualisation! A subset-type operation is what is known in GIScience-speak as a select by location query - in this case, our subset will return the full geometry of each observation feature that intersects with our “clip when our”’“cookie-cutter” layer. Any geometry that does not intersect with our clip layer will be removed from the geometry of our resulting layer. In QGIS, once we complete our spatial query to generate our subset, we need to export this selection and save as a new layer. Luckily for us, as we are using point data, we can (theorectically) use either approach because it is not possible to split the geometry of a single point feature. However, if a point feature does fall on the same geometry as our “clip” layer, it will be excluded from our dataset. When it comes to polygon and line data, not understanding the differences between the two approaches can lead you into difficulties with your data processing - as outlined by our examples in the GIFs above and below - there will be differences in the feature geometry between the clipped layer and the subset layer: Zooming in to see the difference between a clip and a subset. As you should be able to see, when our polygons are clipped, those that intersect with the the boundary of the clip layer are “cut” to conform to this extent. When we use the subset / spatial query operation, those that intersect with the the boundary of the clip layer are returned in their entirety and not cut to conform to the precse extent of the clip layer. Implementing spatial subsetting and clipping in R Each approach is implemented differently in R - and can actually be used together to speed up the effiency in your code. To subset our data, we only need to use the base R library to selection we learnt about in Week 5, using [] brackets. Subset our bike_theft_2019 spatial dataframe by our london_ward_shp spatial dataframe: # Subset our bike_theft_2019 spatial dataframe by the london_ward_shp spatial # dataframe Note the comma - do not forget this! bike_theft_2019_subset &lt;- bike_theft_2019[london_ward_shp, ] You should now see our subset has 18,690 variables instead of 18,744. You can go ahead and plot() the subset layer using your console if you’d like. Conversely, if we want to clip our data, we need to use the st_intersection command from the sf library. Frustratingly, this is a slightly mis-leading name as we see the use of the word ‘intersects’ in our subset above in Q-GIS and within another function within sf, st_intersects() (which is more similar to the QGIS spatial query), however, this is what the function is called, so please just be aware of this moving forward. As you’ll see in your sf cheatsheet, the st_intersection() function “creates geometry to the shared portion of x and y” (Garnett, 2019), aka performs our clip. Clip our our bike_theft_2019 spatial dataframe by our london_ward_shp spatial dataframe using the st_intersection() query: # Subset our bike_theft_2019 spatial dataframe by the london_ward_shp spatial # dataframe Note we overwrite our original variable here, as we&#39;ll continue to # use this in our code bike_theft_2019 &lt;- bike_theft_2019 %&gt;% st_intersection(london_ward_shp) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries Again, you should now see we have 18,690 obesrvations in our bike_theft_2019 spatial dataframe. We can see, therefore, with our point data, whichever approach we would take would not affect the final outcome - this is not necessarily the same for line and polygon data as shown above. As we only need one of these outputs, you can go ahead and remove the bike_theft_2019_subset spatial dataframe from your environment (rm(bike_theft_2019_subset) in your console) to keep our Environment tidy. Which approach should I use in the future? Which approach you use with future data is always dependent on the dataset you want to use - and the output you need. For example, is keeping the geometry of your observation features in your dataset important? Out of the two, the subset approach is the fastest to use as R is simply comparing the geometries rather than also editing the geometries. As a result, when we have a significantly large dataset we want to clip, my recommendation is to actually first subset your spatial dataframe and then clip use the st_intersection() function, to reduce your processing time. Creating our London outline from our London ward data Before we go ahead and wrangle our london_stations spatial dataframe, we’re going to look at how we can dissolve our london_ward_shp spatial dataframe into a single feature. As you would have seen in the GIF of my Q-GIS processing, I used only an outline of London to clip/subset the bike theft data. Whilst both our subset and st_intersection() function has worked with our original london_ward_shp spatial dataframe, reducing this dataset to a single feature would make our processing even faster. Furthermore, reducing a spatial dataframe to a single observation is often required when using R and sf’s geometric operations to complete geometric comparisions - we’ll see what I mean about this in the next section when we look at our london_stations spatial dataframe. Sometimes, also, we simply want to map an outline of an area, such as London, rather than add in the additional spatial complexities of our wards. To achieve just a single ‘observation’ that represents the outline geometry of our dataset, we use the geometric operation, st_union. What a union does (and you can read this off your cheatsheet) is ‘creates a single geometry from multiple geometries’ (Garnett, 2019). Let’s go ahead and see if we can use this to create our London outline. Union our london_ward_shp spatial dataframe to create a single outline of London: # Use st_union to create a single outline of london from our london_ward_shp # spatial dataframe london_outline &lt;- london_ward_shp %&gt;% st_union() You should see that our london_outline spatial data frame only has one observation. You can now go ahead and plot() your london_outline spatial dataframe from your console and see what it looks like - hopefully you’ll now have an outline of London: You can also use the st_union() to union two datasets into one - this can be used to “merge” data together that are of the same spatial type. Using dplyr to complete a Dissolve geometric operation Whilst sf contains many handy geometric operations, one critical geometric operation it does not contain is something known as a dissolve. In Q-GIS (and the majority of other GIS software), a dissolve can create a union of features within a single dataset based on an attribute within that dataset. How dissolve works. Source: Esri, 2020. To achieve this type of dissolve within sf, we can actually use the dplyr library and its group_by() and summarise() functions. What we ask R to do is to group our spatial dataframe by the attribute we want to dissolve by - and then summarise these features by the sum of their area to essentially create a union of them. We can demonstrate this with our london_ward_shp spatial dataframe - for example, we can group our wards by the DISTRICT column and calculate sum the area of the individual observation features within each group using the HECTARES column, which we use to then summarise our spatial dataframe to produce the final union geometries. Dissolve our london_ward_shp spatial dataframe using the DISTRICT column using our dplyr approach: # Pipe our grouped dstrict london_wards into the dplyr summarise function and # summarise it by its area Store this output as a london_boroughs london_boroughs &lt;- london_ward_shp %&gt;% group_by(DISTRICT) %&gt;% summarise(area = sum(HECTARES)) Now, the clue is a little bit in the name - but if you now plot your dissolved dataset, you’ll see we’ve created a new spatial dataframe from our london_ward_shp spatial dataframe that resembles our Borough data all the way from Practical 1! Of course, we could have utilised the borough dataset we already have in our raw folder, but there will be times when data like this does not exist - and you need to create it from a dataset you already have! Finding an alternative train station dataset We’ve seen that our current london_stations spatial dataframe really does not provide the coverage of train stations in London that we’d expect - we’ve obviously gone and downloaded not neessarily the wrong dataset per se, but just one that does not fulfil our spatial coverage requirements. The issue we face, however, is that if you search for a London - or even England - dataset of train stations, there simply isn’t one to be found! So where can we get this data from? Introducing OpenStreetMap and its contents If you’ve never come across OpenStreetMap (OSM) before, it is a free editable map of the world. It was first created in 2004 by Steve Coast in the UK to counteract the predominance of proprietary map data in the UK and elsewhere. For example, your main interaction with maps, beyond this course, might be with a well-known application called Google Maps. All the data that underpins Google Maps is however completely properietary, i.e. owned by Google - and beyond using their services on the App and browser versions, you can’t actually “get access” to the data you see in front of you. For OpenStreetMap (OSM), this is completely the opposite - not only is the data not owned by anyone specifically (although it is licensed and copyrighted by OpenStreetMap and its contributors), you can both contribute data to the map yourself with relative ease and download the data that is within the map itself. The data on the map has been provided through a range of sources, from user submitted GIS data, contributor digitisation of satellite imagery during mapathons, as well as the incorporation of ‘authoritative’ data from Open Data programmes and initiatives, such as Ordnance Survey’s Open Data or the U.S’s TIGER dataset. Whilst February 25th 2021 saw the 100,000,000th edit to the map, the map’s spatial coverage is still unequal across the world - plus, as you’ll find if you use the data, the accuracy and quality of the data can often be quite questionable or simply missing attribute details that we’d like to have, e.g. types of roads and their speed limits, to complete specific types of spatial analysis. There are a lot of initiatives out there to change this - and something I’ve been involved in on and off for a long time and would highly advocate for you to at least read into to see where you can give back, particularly if you go on to use OSM substantially! As a result, do not expect OSM to contain every piece of spatial data that you’d want - it is doing its best, but ultimately it takes a long time and a lot of human power to map the whole world! Furthermore, it’s always a good idea to think through if and how you can validate the data to be confident in its coverage - we’re going to look at this today in terms of our station dataset. For now, we will focus on how we can download our required data - train stations in London - from OSM in R. Downloading data from OpenStreetMap Whilst there are various approaches to downloading data from OpenStreetMap, which you can read about here. We will of course use the one that is most straightforward to us using R - we’ll use the osmdata library to directly extract our required OpenStreetMap (OSM) data into a variable within our Environment. The osmdata library grants access within R to the Overpass API that allows us to run queries on OSM data and then import the data as either sf or sp objects. I’d highly recommend reading more about the Overpass API originations or experiment within its web browser form here, which even contains a “wizard” to help you get started with writing queries. These queries are at the heart of these data downloads. To use the library (and API), we need to know how to write and run a query, which requires identifying the key and value that we need within our query to select the correct data. This is the trickiest bit of using the library - and requires getting familiar with OSM’s approach to tagging datasets. Essentially every map element (whether a point, line or polygon) in OSM is “tagged” with different attribute data. The first tag any element will be provided with is its key - what type of human or physical “on-the-ground” feature does our element represent. As part of this tag, each element will also be provided with a value that is a subcategory of this key. In our case, we’re looking for train stations, which fall under the key, Public Transport, with a value of station as outlined in their wiki. These keys and values are used in our queries to extract only map elements of that feature type - to find out how a feature is “tagged” in OSM is simply a case of reading through the OSM documentation and beoming familiar with their keys and values. You can see more here for future use, for example in your coursework and/or dissertation. Extracting train stations in London from OSM To extract a train station dataset from OSM using the osmdata library, we need to follow four key steps: Obtain the bounding box of where we want our data to be extracted from, i.e. London, to prevent OSM searching the whole map of the world for our feature (although the API query does have in-built time and spatial coverage limits to stop this from happening!). Submit this bounding box to the OSM query alongside the key/value combination of the feature we want to extract. Create an sf OSM data object from this query. Retrieve the data that we want from this object - in our case, point data! Let’s see this in action. Extract elements from OSM that are tagged as public_transport = station from OSM into an osmdata_sf() object : # Define our bbox coordinates, here we extract the coordinates from our london # outline using the st_bbox function Note we also temporally reproject the # london_outline spatial dataframe before obtaining the bbox We need our bbox # coordinates in WGS84 (not BNG), hence reprojection p_bbox &lt;- st_bbox(st_transform(london_outline, 4326)) # Pass our bounding box coordinates into the OverPassQuery (opq) function london_stations_osm &lt;- opq(bbox = p_bbox) %&gt;% # Pipe this into the add_osm_feature data query function to extract our stations add_osm_feature(key = &quot;public_transport&quot;, value = &quot;station&quot;) %&gt;% # Pipe this into our osmdata_sf object osmdata_sf() Note, you can find out more about this and how to construct your queries at the following tutorial: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html and more about using OpenStreetMap and its various key/values at the OSM Wiki and specifically its [Map Features] page(https://wiki.OpenStreetMap.org/wiki/Map_features). In the code above, we have completed steps 1-3. We now need to extract only the points data from our osmdata_sf() object. Understanding the osmdata_sf() and the OSM data object When we download OSM data - and extract it as above - our query will return all elements tagged as our key/value combination into our osmdata_sf() OSM data object. This means all elements - any points, lines and polygons - associated with our tag will be returned. Obviously we would think with our public_transport = station tag, we would only return point data representing our train and tube stations in London. But if we use the summary() function on our london_stations_osm osm data object, we can see that not only is a lot of other data stored in our OSM data object (including the bounding box we used within our query, plus metadata about our query), but our query has returned both points and polygons - currently stored within this OSM data object as individual spatial data frames. Note, the length of these sf spatial dataframes within the data object is equal to the number of attributes associated with these spatial dataframes, not the number of observations within them. # Summarise our london_stations_osm - you can enter this into your console summary(london_stations_osm) ## Length Class Mode ## bbox 1 -none- character ## overpass_call 1 -none- character ## meta 3 -none- list ## osm_points 143 sf list ## osm_lines 0 -none- NULL ## osm_polygons 51 sf list ## osm_multilines 0 -none- NULL ## osm_multipolygons 5 sf list To extract only the points of our tube and train stations from our london_stations_osm osm data object, we simply need to extract this dataframe and store this under our variable as we’ll do in a second. But, whenever you’re dealing with OSM data, just remember that your query can return multiple different types of map elements (and their respective geometry), so always be clear in knowing which type of spatial data you’ll need and remember to extract this from your osm data object. As we also provided our bounding box in WGS84 coordinates, we also should know that we’ll need to reproject our extracted spatial dataframe from WGS84 to BNG - and we’ll tidy it up (with the London outline) at the same time to make sure our stations are only within London. Note, we’ll just overwrite our current variable. Extract train station points from our osm data object and process/clean ready for analysis. # Extract only the points data from our osmdata object &amp; fields we want to keep # Note, I checked the points object in the osmdata object to identify the fields # to keep london_stations_osm &lt;- london_stations_osm$osm_points[, c(&quot;osm_id&quot;, &quot;name&quot;, &quot;network&quot;, &quot;operator&quot;, &quot;public_transport&quot;, &quot;railway&quot;)] %&gt;% # Reproject our dataset to BNG for future use st_transform(27700) %&gt;% # And clip our osm data to our london outline shapefile st_intersection(london_outline) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries You should now see the our london_stations_osm variable in our Environment now stores a points spatial dataframe containing all train and tube stations within London - my query has retrieved 1573 features, with the 7 fields selected. Plotting our osm stations and its fields. Cleaning our OSM station dataset With the accuracy of OSM a little questionable, we want to complete some data validation tasks to check its quality and to confirm that it at least contains the data we see in our authoritative london_stations spatial dataframe. Check our OSM train and tube station data First, we’ll check the quality of our data extract - as we can see in our plot above, not all of our stations appear to be of the same value in our railway field. If we check the field using our count() function, you’ll see that there are some different values and NAs in our dataset. Use the count() function to understand what values we have within our railway field: count(london_stations_osm, railway) ## Simple feature collection with 7 features and 2 fields ## geometry type: MULTIPOINT ## dimension: XY ## bbox: xmin: 505082.2 ymin: 159027.2 xmax: 556214 ymax: 200138.6 ## CRS: EPSG:27700 ## railway n geometry ## 1 entrance 2 MULTIPOINT ((532814.8 16572... ## 2 station 592 MULTIPOINT ((505082.2 17672... ## 3 stop 2 MULTIPOINT ((513225.1 18452... ## 4 subway_entrance 9 MULTIPOINT ((512210.7 17613... ## 5 train_station_entrance 2 MULTIPOINT ((529171.5 18417... ## 6 tram_stop 6 MULTIPOINT ((524730.4 17025... ## 7 &lt;NA&gt; 960 MULTIPOINT ((505605.4 18418... As we can see, not everything in our london_stations_osm spatial dataframe is a ‘station’ as recorded by OSM. We actually have a lot of NAs (960!) in our spatial dataframe - and it is highly unlikely these represent stations in London. In fact, a quick search online can tell us that there are 270 tube stations in the London network as well as 330 train stations in Greater London. Never fear about searching for this information as a way to validate your data! With 592 stations detailed in our railway field, it’s likely that these are the only points in our dataset that represent actual stations in London (with a difference of 8 to our researched number likely due to not duplicating major train stations such as Waterloo or Euston that also double as tube stations). As a result, we’ll go on the best information we have from this attribute and our search and remove all other points from our OSM dataset - we’ll, of course, use the filter() function for this. Filter out those points that are not railway stations: # Extract only the points data from our osmdata object &amp; fields we want to keep london_stations_osm &lt;- london_stations_osm %&gt;% filter(railway == &quot;station&quot;) We should now have 592 stations left in our observations. That’s a much closer number to our estimated research number of 600. Check the spatial distribution of the OSM data against the authoritative TfL data through interactive mapping We’ve now cleaned our london_stations_osm spatial dataframe to remove all those points within our dataset that are not tagged as railway=station. Our london_stations spatial dataframe is of course an authoritative dataset from TfL, so we know at least that this data should be accurate - therefore, it would be great if we could compare our two datasets to one another spatially to double-check our london_stations_osm spatial dataframe contains all the data within our london_stations spatial dataframe. We can first look at this by comparing their distributions visually on a map. But first, as our london_stations spatial dataframe still extends outside of London, we’ll go ahead and clip this. Clip our london_stations spatial dataframe to the London outline: london_stations &lt;- london_stations %&gt;% st_intersection(london_ward_shp) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries We now have 286 stations left in our london_stations spatial dataframe to compare with our london_stations_osm spatial dataframe. Let’s go ahead and compare our two stations spatial data frames spatial distribution visually on a map. Map our two spatial dataframes to compare their spatial coverage: # Plot our London Wards first with a grey background tm_shape(london_outline) + tm_fill() + # Plot OSM station data in black tm_shape(london_stations_osm) + tm_dots(col = &quot;black&quot;) + # Plot TfL station data in red tm_shape(london_stations) + tm_dots(col = &quot;red&quot;) + # Add north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + # Add scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) + # Add our OSM contributors statement tm_credits(&quot;© OpenStreetMap contributors&quot;) What we can see is that it looks like our OSM data actual does a much better job at covering all train and tube stations across London (not a surpise with 300 more stations in the dataset) - but still it’s pretty hard to get a sense of comparison from a static map like this whether it contains all of the tube and train stations in our london_stations spatial dataframe. One really cool thing about the tmap library that we haven’t looked at yet, is that we can change its mode from a static map such as the one above, to an interactive map that enables us to interrogate the spatial coverage of our two station spatial dataframes further. To do so, we use the tmap_mode() function and change it from its default plot() mode to a view() model. Change tmap’s mode to view (from plot) and plot our data interactively: # Change tmap mode to view / interactive mapping tmap_mode(&quot;view&quot;) # Plot our London Wards first Plot our London Wards first with just a border tm_shape(london_outline) + tm_borders() + # Plot OSM station data in black tm_shape(london_stations_osm) + tm_dots(col = &quot;black&quot;) + # Plot TfL station data in red tm_shape(london_stations) + tm_dots(col = &quot;red&quot;) NOTE, the workbook will not display any of our interactive maps as this requires significant processing from your web browser. Here we provide a GIF of what your interactive map should look like. We’ll only do this once for this example. Well that’s super cool - using the interactive map, what we can see is that whilst we do have overlap with our datasets, and more importantly, our london_stations_osm spatial dataframe seems to contain all of the data within the london_stations spatial dataframe, although there are definitely differences in their precise location. Now depending on what level of accuracy we’re willing to accept with our assumption that our OSM data contains the same data as our TfL data, we could leave our comparison here and move forward with our analysis. There are however several more steps we could complete to validate this assumption. The easiest first step is to simply reverse the order of our datasets to check that each london_stations spatial dataframe point is covered. Reverse our map drawing order to check no red point appears through: # Plot our London Wards first with just a border tm_shape(london_outline) + tm_borders() + # Plot TfL station data in red tm_shape(london_stations) + tm_dots(col = &quot;red&quot;) + # Plot OSM station data in black tm_shape(london_stations_osm) + tm_dots(col = &quot;black&quot;) This looks pretty good - but still the question is: can we be sure? It would be really useful if we could spatially quantify whether we have any missing stations from our data. And guess what? By using geometric operations and spatial queries, we can! Comparing our stations datasets through geometric operations and spatial queries Using geometric operations and spatial queries, we can look to find if any of our stations in our london_stations spatial dataframe are not present the london_stations_osm spatial dataframe. We can use specific geometric operations and/or queries that let us check whether or not all points within our london_stations spatial dataframe spatially intersect with our london_stations_osm spatial dataframe, i.e. we can complete the opposite our clip/intersection that we conducted earlier. The issue we face however is that, as we saw above, our points are slightly offset from one another as the datasets have ultimately given the same stations slightly different locations. This offset means we need to think a little about the geometric operation or spatial query that we want to use. We will approach this question in two different ways to highlight the differences between geometric operations and spatial queries: We’ll use geometric operations to generate geometries that highlight missing stations from our london_stations spatial dataframe (i.e. ones that are not present in thelondon_stations_osm spatial dataframe.) We’ll use spatial queries to provide us with a list of features in our london_stations spatial dataframe that do not meet our spatial requirements (i.e. are not present in thelondon_stations_osm spatial dataframe.) We’ll demonstrate both below to give you an introduction to each approach - it ultimately will be up to you to decide which approach your prefer. Validating our OSM data against our authoritative TfL data using geometric operations As highlighted above, the offset between our spatial dataframes adds a little complexity to our geometric operations code. To be able to make our direct spatial comparisons across our spatial dataframes, what we first need to do is try to **snap*’** the geometry of our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a given distance threshold. This will mean that any points in the london_stations spatial dataframe that are within a specific distance of the london_stations_osm spatial dataframe will have their geometry changed to that of the london_stations_osm spatial dataframe. Snapping points to a line. In our case we snap our points to other points. By placing a threshold on this “snap”, we stop too many points moving about if they are unlikely to be representing the same station (e.g. further than 200m or so away) - but this still allows us to create more uniformity across our datasets’ geometries (and tries to reduce the uncertainty we add by completing this process). After a quick investigation of the interactive map and looking at those points that appear to be the same station but spatially apart against the scale bar, I’ve settled on a distance of 150m. Snap our our london_stations spatial dataframe to our london_stations_osm spatial dataframe for points within a 150m distance threshold. # Snap points from our london_stations sdf that are 150m within our # london_stations_osm sdf to the same geometry london_stations_snap &lt;- st_snap(london_stations, london_stations_osm, 150) Now we have out ‘snapped’ geometry, we can look to compare our two datasets to calculate whether or not our london_stations_osm spatial dataframe is missing any data from our london_stations_snap spatial dataframe. To do so, we will use the st_difference() function which will return us the geometries of those points in our london_stations_snap spatial dataframe that are missing in our our london_stations_osm spatial dataframe. However to use this function successfully (and this seems to be required for many of the comparison geometric operations in sf), we need to convert our our london_stations_osm spatial dataframe into a single geometry to enable this comparision. Note, this simplification of the geometry for geometric operations is not needed in Q-GIS. To simplify our london_stations_osm spatial dataframe into a single geometry, we simply use the st_union() code we used with our London outline above. Simplify the london_stations_osm spatial dataframe and then compare this to the london_stations_snap spatial dataframe to identify any missing stations: # Create a single geometry version of our `london_stations_osm` spatial dataframe # for comparison london_stations_osm_compare &lt;- london_stations_osm %&gt;% st_union() # Compare our two point geometries to identify missing stations missing_stations &lt;- st_difference(london_stations_snap, london_stations_osm_compare) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries You should now find that we have an answer - and apparently have 3 missing stations with our london_stations_osm spatial dataframe. Let’s go ahead and plot this interactively to see where these station are truly missing! Plot our missing stations against our london_stations_osm spatial dataframe and identify whether these stations are missing or not! Note, I recommend changing the basemap to the Esri Topo Map - do not use the OSM map as we, of course, are using data from OSM! Esri’s data is from Ordnance Survey and their own datasets so we won’t run into issues of the basemap presenting the same data. # Plot our london_stations_osm spatial dataframe in black tm_shape(london_stations_osm) + tm_dots(col = &quot;black&quot;) + # Plot our missing_stations spatial dataframe in green tm_shape(missing_stations) + tm_dots(col = &quot;green&quot;) Zoom in to our three missing stations - are these stations actually missing, or simply are the two locations (between the TfL dataset and the OSM dataset) too far apart that our snapping didn’t work? If you investigate these three missing stations, you can actually see that our london_stations_osm spatial dataframe dataset is actually more accurate than the TFL locations. All three missing stations were not in fact missing, but a) in our OSM dataset but b) at a greater offset than 150m - and in all three cases, or OSM locations were more accurate! We can safely suggest that we can move forward with only using the london_stations_osm spatial dataframe and do not need to follow through with adding any more data to this dataset. Validating our OSM data against our authoritative TfL data using spatial queries Before we go ahead and move forward with our analysis, we’ll have a look at how we can implement the above quantification using spatial queries instead of geometric operations. Usually, when we want to find out if two spatial dataframes have the same or similar geometries, we would use one of the following queries: st_equals() st_intersects() st_crosses() st_overlaps() st_touches() Specifically, we’d want to use `st_equals() to “identify if our two spatial dataframes and their geometry share the exactly the same space” (Garnett, 2019). The other four options are modifications of the exact aspect, with ‘share any’, ‘have commonalities’, ‘share space’ or ‘share a common point’ their respective qualifications: Conceptualising Spatial Relationships. Source: Esri. Ultimately which query (or spatial relationship conceptualisation) you would choose would depend on the qualifications you are trying to place on your dataset. In our case, considering our london_stations spatial dataframe and our london_stations_osm spatial dataframe, we have to consider the offset between our datasets. We could, of course, snap our spatial dataframe as above - but woudn’t it be great if we could skip this step? Luckily we can! Introducing the st_is_within_distance() spatial query To do so, instead of snapping the london_stations spatial dataframe to the london_stations_osm spatial dataframe, we can use the st_is_within_distance() spatial query to ask whether our points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe. This ultimately means we can skip the snapping and st_difference steps - and complete our processing in two simple steps. However, one thing to be aware of when running spatial queries in R and sf is that whichever spatial dataframe is the comparison geometry (i.e. spatial dataframe y in our queries), this spatial dataframe must (again) be a single geometry - as we saw above in our st_difference geometric operation. If it is not a single geometry, then the query will be run x number of observations * y spatial dataframe number of observations times, which is not the output that we want. By converting our comparison spatial dataframe to a single geometry, the query is only run for the number of observations in x. You should also be aware that any of our spatial queries will return one of two potential outputs: EITHER a list detailing the indexes of all those observation features in x that do intersect with y OR a matrix that contains a TRUE or FALSE statement about this relationship. To define whether we want a list or a matrix output, we set the sparse parameter within our query to TRUE or FALSE respectively. We will see how we can use the list in a later query, but with a matrix, we can simply join this back into our x spatial dataframe as a new column - which we’ll do now. Query whether the points in our london_stations spatial dataframe are within 150m of our london_stations_osm spatial dataframe: # Create a single geometry version of our `london_stations_osm` spatial dataframe # for comparison We did this above, so you should have this variable already We # simply place it here to remind us for future code! london_stations_osm_compare # &lt;- london_stations_osm %&gt;% st_union() # Compare our two point geometries to identify missing stations london_stations$in_osm_data &lt;- st_is_within_distance(london_stations, london_stations_osm_compare, dist = 150, sparse = FALSE) We can go ahead and count() the results of our query. Count the results of our query: # Count the number of stations within 150m of our the OSM spatial dataframe count(london_stations, in_osm_data) ## Simple feature collection with 2 features and 2 fields ## geometry type: MULTIPOINT ## dimension: XY ## bbox: xmin: 505625.9 ymin: 168579.9 xmax: 556144.8 ymax: 196387.1 ## CRS: EPSG:27700 ## # A tibble: 2 x 3 ## in_osm_data[,1] n geometry ## * &lt;lgl&gt; &lt;int&gt; &lt;MULTIPOINT [m]&gt; ## 1 FALSE 3 ((533644.3 188926.1), (537592.5 179814.9), (538301.6 17… ## 2 TRUE 283 ((505625.9 184164.2), (507565.2 185008.3), (507584.9 17… Great - we can see we have 3 stations missing (3 FALSE observations), just like we had in our geometric operations approach. We can double-check this by mapping our two layers together. Map our missing_stations spatial dataframe and our london_stations spatial dataframe (by the in_osm_data column) for comparison: # Plot our london_stations by the in_osm_data column tm_shape(london_stations) + tm_dots(col = &quot;in_osm_data&quot;) + # Add our missing_stations spatial dataframe tm_shape(missing_stations) + tm_dots(col = &quot;green&quot;) You can turn on and off the missing_stations layer to compare the locations of these points to the FALSE stations within our london_stations_osm spatial dataframe, generated by our spatial query. Quantifying distances between our bike thefts and train stations We now have our London bike theft and train stations ready for analysis - we just need to complete one last step of processing with this dataset - calculating whether or not a bike theft occurs near to a train station or not. As above, we can use both geometric operations OR spatial queries to complete this analysis, which we’ll demonstrate below. Geometric operations approach Step One: generate buffer for use within our analysis* Our first approach using geometric operations will involve the creation of a buffer around each train station to then identify which bike thefts occur within 400m of a train or tube station. Once again, the sf library has a function for generating buffers - we just need to know how to deploy it successfully on our london_stations_osm spatial dataframe. When it comes to buffers, we need to consider two main things - what distance will we use (and are we in the right CRS to use a buffer) and do we want individual buffers or a single buffer! A single versus multiple buffer. The single buffer represents a ‘unioned/dissolved’ version of the multiple buffer option. Source: Q-GIS, 2020. In terms of CRS, we want to make sure we use a CRS that defines its measurement units in metres - this is because it makes life significantly easier when calculating these buffers. If our CRS does not use metres as its measurement unit, it might be in a base unit of an Arc Degree or something else that creates difficulties when converting between a required metre distance and the measurement unit of that CRS. When using a CRS with a non-metres unit, you have two options: 1) reproject your data into a CRS that uses metres as its base unit OR 2) convert your metre distance into the respective unit. This type of manipulation applies to QGIS - but here we have a +1 for ArcGIS, where you can proactively set or change the units you’ll use for your buffer! In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres, so we do not need to worry about this. In terms of determing whether we can create a single or multiple buffers, we can investigate the documentation of the function st_buffer to find out what additional parameters it takes - and how. What we can find out is that we need to (of course!) provide a distance for our buffer - but whatever figure we supply, this will be interpreted within the units of the CRS we are using. Fortunately none of this is our concern - we know we can simply input the figure or 400 into our buffer and this will generate a buffer of 400m. The one issue we do face though is that the parameter arguments do not allow us to dissolve our buffers into a single record, as we can, for example, in QGIS. But, as we’ve seen above, there is some code we can use to dissolve our buffers into a single feature after generating them. Let’s create our single buffer: Create a single buffer representing 400m away from all train stations in London: # Generate a 400m buffer around our london_stations_osm dataset, union it to # create one buffer station_400m_buffer &lt;- london_stations_osm %&gt;% st_buffer(dist = 400) %&gt;% st_union() You can then go ahead and plot our buffer to see the results, entering plot(station_400m_buffer) within the console: We can now go ahead and utilise this buffer to find out which bike thefts have occured within 400m of a train or tube station in London. Step Two: Use buffer to identify bike theft that occurs within 400m of a station To find out which bike thefts have occured within 400m of a station, we will use the st_intersects() function. Before we move forward, one thing to clarify is that a completely *NEW function AND NOT the st_intersections() function we’ve been used so far! Unlike the st_intersections() function which creates a ‘clip’ of our dataset, i.e. produces a new spatial dataframe containing the clipped geometry, the st_intersects() function simply identifies whether “x and y geometry share any space” (Garnett, 2019). As explained above, as with all spatial queries, the st_intersects() function can produce two different outputs: either a list detailing the indexes of all those observation features in x that do intersect with y OR a matrix that contains a TRUE or FALSE statement about this relationship. As with our previous spatial query, we’ll continue to use the matrix approach - this means for every single bike theft in London, we’ll know whether or not it occured within our chosen distance of a train station and join this as a new column to our bike_theft_2019 spatial dataframe. Detect which bike thefts occur within 400m of a train or tube station: # Test whether a bike theft intersects with our station_buffer and store this as # a new column called nr_train_400m Note, we use the base R approach of creating # a new column with our code in this case We set the `sparse` parameter to # `FALSE` to have a value returned for each theft, not just the indexes of those # that do intersect bike_theft_2019$nr_train_400 &lt;- bike_theft_2019 %&gt;% st_intersects(station_400m_buffer, sparse = FALSE) We could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we’ll leave it as TRUE or FALSE. We can go ahead and now visualise our bike_theft_2019 based on this column, to see those occuring near to a train station. Map our bike_theft_2019 spatial dataframe by the nr_train_400m column: # Set tmap back to drawing mode (you are welcome to leave yours as View, if you&#39;d # like) tmap_mode(&quot;plot&quot;) # Plot our london outline border tm_shape(london_outline) + tm_borders() + # Plot our bike theft and visualise the nr_train_400m column tm_shape(bike_theft_2019) + tm_dots(col = &quot;nr_train_400&quot;, palette = &quot;BuGn&quot;) + # Plus our train stations on top tm_shape(london_stations_osm) + tm_dots(palette = &quot;gray&quot;) + # Add our OSM contributors statement tm_credits(&quot;© OpenStreetMap contributors&quot;) It should be of no surprise that visually we can of course see some defined clusters of our points around the various train stations - after all that’s what we’ve based our analysis around! We can then utilise this resulting dataset to calculate the percentage of bike thefts have occured at this distance - but we’ll do so once we’ve looked at the spatial query approach to obtaining the same data. Spatial Query Approach After our previous demonstration, it should come of no surprise that the spatial query approach is even faster than our geometric operations - and in actual fact, we’re going to go ahead and use the same spatial query we used earlier with our station comparison: st_is_within_distance(). Step One (and only one): Query Dataset We will use this query to ask: for each point within our dataset, is this within 400m of a london station? We’ll again need to use the single geometry version of our london_stations_osm spatial dataframe for this comparison - and again use the sparse=FALSE to create a matrix that we’ll simply join back to our bike_theft_2019 spatial dataframe as a new column. Calculate for each point in our bike_theft_2019 spatial dataframe whether it is within 400m of a station and store this result as a new column in our bike_theft_2019 spatial dataframe: # Compare our two point geometries to identify missing stations bike_theft_2019$nr_train_400_sq &lt;- st_is_within_distance(bike_theft_2019, london_stations_osm_compare, dist = 400, sparse = FALSE) We can again compare the outputs of our two different approaches to check that we have the same output - either through our count() or mapping approach. Map both the spatial query (nr_train_400_sq) and geoemtric operation (nr_train_400) output columns within our bike_theft_2019 spatial dataframe: # set tmap to interactive tmap_mode(&quot;view&quot;) # Plot our bike theft by the nr_train_400 column tm_shape(bike_theft_2019) + tm_dots(col = &quot;nr_train_400&quot;) + # Add our bike theft plot by the nr_train_400_sq column tm_shape(bike_theft_2019) + tm_dots(col = &quot;nr_train_400_sq&quot;) Again, you can swap between the two layers to check the different outputs. As we can see, we have achieved the exact same output - with fewer lines of code and, as a result, quicker processing. However, unlike with the geometric operations, we do not have a buffer to visualse this distance around a train station, which we might want to do for maps in a report or presentation, for example. Once again, it will be up to you to determine which approach you prefer to use - some people prefer using the more visual techniques of geometric operations, whereas others might find spatial queries to answer the same questions. Calculating bike thefts within 400m of a tube or train station Now we have, for each bike theft in our bike_theft_2019 spatial dataframe, an attribute of whether it occurs within 400m of a train or tube station. We can quite quickly use the count() function to find out just how many thefts these clusters of theft represent. ## Simple feature collection with 2 features and 2 fields ## geometry type: MULTIPOINT ## dimension: XY ## bbox: xmin: 504411 ymin: 159848 xmax: 556531.4 ymax: 199863.8 ## CRS: EPSG:27700 ## # A tibble: 2 x 3 ## nr_train_400[,1] n geometry ## * &lt;lgl&gt; &lt;int&gt; &lt;MULTIPOINT [m]&gt; ## 1 FALSE 11269 ((504411 176098.6), (504516 177404.7), (504696.1 18293… ## 2 TRUE 7421 ((505379.1 184132.3), (505432.2 184465.3), (505759.2 1… In my case, 7,405 out of 18,690 bike thefts, i.e. 40%, occur within 400m of a train station - that’s quite a staggering amount, but if we consider commuter behaviour - and certain “crime theories” such as routine activity theory - this occurence is not be unexpected. After all, if a bike is left at a train station, it is unlikely to be watched by a ‘suitable guardian’ nor is a simple bike lock likely to deter a thief, making it a ‘suitable target’. Furthermore, train stations are often located in areas of high areas of daytime populations - particularly in London. Therefore, to understand whether there are any key ‘hotspots’ of bike theft in London, it might be worth aggregating by the Ward unit and running a few spatial autocorrelation tests - as we’ll do in a minute. Overall, our main hypothesis - that bike thefts occur primarily near train and tube stations - isn’t quite proven, but so far we have managed to quantify that a substantial amount of bike thefts do occur within 400m of these areas. Assignment 1: Calculate bike theft at varying distances Before we go ahead and conduct our aggregation, I’d like you to re-run the above analysis at four more distances: 100m 200m 300m 500m and calculate the percentage of bike theft at these different distances. You can choose whether you would like to use the geometric operations or spatial queries approach. What do you think could explain the substantial differences in counts as we increase away from the train station from 100 to 200m? Note, this is not a trick question - but perhaps one you might find easier to answer if you re-read the What is Representation? section from our first week? You might want to think of an additional dataset that we could use to improve our methodology. One additional question is: what limitations do we face in studying the points of our theft dataset in terms of knowing the precise location of our theft. To answer this question, I would suggest reading back through our content in Week 5 on how the police data is processed. I will ask for answers to these questions in our seminar in Week 8. You’ve now completed the majority of data cleaning and processing we need to now move on with our analysis. Before you do, I’d recommend you take a break from the workshop. Practical 6 - Part Two: Analysing and Visualising Point Data We now have our final dataset and an answer to our question - but could we do a little more with our analysis? The answer to this is of course (and always!) yes. We’ll look at two approaches that both show how we can use the st_intersects() function to conduct what we know as point-in-polygon counts to visualise our data in two different ways. 1. Aggregating to our Ward dataset In our first approach, we’ll conduct a very familiar procedure: aggregating our data to the ward level. At the moment, we’ve now calculated for each bike theft whether or not it occurs within 400m of a tube or train station. We can use this to see if specific wards are hotspots of bike crimes near stations across London. To do this, we’ll be using the same process we used in QGIS - counting the number of points in each of our polygons, i.e. the number of bike thefts in each ward. To do so in R and with sf, it is one line of code - which at first look does not sound at all like it is completing a point-in-polygon (PIP) calculation - but it does! Running a Point-in-Polygon calculation in R To create a PIP count within sf, we use the st_intersects function again - but instead of using the matrix output of TRUE or FALSE that we’ve used before, what we actually want to extract from our function is the total number of points it identifies as intersecting with our london_ward_shp spatial dataframe. To achieve this, we use the length() function from the base R package to count the number of wards returned within the index list its sparse output creates. Remember, this sparse output creates a list of the bike thefts (by their index) that intersect with each ward - therefore, the length() function will return the length of this list, i.e. how many bike thefts each ward contains or, in other words, a Point-in-Polygon count! This time around therefore we do not set the sparse function to FALSE but leave it as TRUE (its default) by not entering the parameter. As a result, we can calculate the number of bike thefts per ward and the number of bike thefts within 400m of a station per ward and use this to generate a rate for each ward of the number of bikes thefts that occur near a train station for identification of these hotspots. Calculate the number of bike thefts per ward and the number of bike thefts within 400m of a station per ward using the PIP operation via intersects(): # Run a PIP for total number of bike thefts occuring within a ward using # st_intersects function london_ward_shp$total_bike_theft &lt;- lengths(st_intersects(london_ward_shp, bike_theft_2019)) # Run a PIP for number of bike thefts within 400m of train station within a ward # using st_intersects function Note addition of filter statement to extract only # those bike thefts near a station london_ward_shp$nr_station_bike_theft &lt;- lengths(st_intersects(london_ward_shp, filter(bike_theft_2019, nr_train_400 == TRUE))) As you can see from the code above, we’ve now calculated our total bike theft and bike theft near a train station for each ward. The final step in our processing therefore is to create our rate of potentially station-related bike theft = bike theft near train station / total bike theft. Note, we are looking specifically at the phenomena of whether bike theft occurs near to a train or tube station or not. By normalising by the total bike theft, we are creating a rate that shows specifically where there are hotspots of bike theft near train stations. This, however, will be of course influenced by the number of train stations within a ward, the size of the ward, and of course the number of bikes and potentially daytime and residential populations within an area. To do so, we’ll use the same approach of generating a new column within our london_ward_shp spatial dataframe - and then use a mathematical formula to calculate our rate, just as we did in Week 3 in QGIS. Calculate the rate of bike theft within 400m of a train or tube station out of all bike thefts for each ward # Calculate the rate of bike thefts within 400m of train or tube station Times by # 100 to get a percentage london_ward_shp$btns_rate &lt;- (london_ward_shp$nr_station_bike_theft/london_ward_shp$total_bike_theft) * 100 Visualising our data using “small multiples” To best visualise our Ward aggregated bike data, we’re going to use the approach of creating “small multiples” maps that enables, using the tm_arrange() function. Creating small multiples allows us to plot three different maps within one visualisation - enabling easy comparison and quick visualisation. Small multiples can be used in other visualisations beyond maps and is a very popular technique in statistical chart visualisation. It allows you to plot multiple data variables clearly - but also allow you to compare them to one another quickly. You should always aim to be visualising related or similar variables at the same spatial scale for them to work most effectively. In comparison to using facets mapping approach (that we’ll look at in more detail next week), using small multiples also enables us to customise/modify each map individually - such as details in their respective legends as well as individual symbolisation and styling. In addition, facet mapping really does have a specific purpose, which does not suit what we want to achieve here. To use small multiples in tmap, we simply store each map we want to create as an individual (programming) variable that we then load and visualise using the tm_arrange() function. Create a tmap of each of our three data variables, store these as programming variables and load these using tm_arrange() function: # Ensure tmap is in plot mode tmap_mode(&quot;plot&quot;) # Create our first map and store at btm_1: Plot our London wards by the # total_bike_theft column btm_1 &lt;- tm_shape(london_ward_shp) + tm_polygons(col = &quot;total_bike_theft&quot;, palette = &quot;YlGn&quot;, style = &quot;jenks&quot;, title = &quot;Total Bike Theft&quot;) + # Add a legend etc tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5, scale = 0.65, fontfamily = &quot;Helvetica&quot;) #+ # tm_compass(type=&#39;arrow&#39;, position = c(&#39;right&#39;, &#39;bottom&#39;)) + tm_scale_bar(breaks # = c(0, 5, 10, 15, 20), position = c(&#39;left&#39;, &#39;bottom&#39;)) # Create our second map and store at btm_2: Plot our London wards by the # nr_station_bike_theft column btm_2 &lt;- tm_shape(london_ward_shp) + tm_polygons(col = &quot;nr_station_bike_theft&quot;, palette = &quot;BuGn&quot;, style = &quot;jenks&quot;, title = &quot;Near Station Bike Theft&quot;) + # Add a legend etc tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5, scale = 0.65, fontfamily = &quot;Helvetica&quot;) #+ # tm_compass(type=&#39;arrow&#39;, position = c(&#39;right&#39;, &#39;bottom&#39;)) + tm_scale_bar(breaks # = c(0, 5, 10, 15, 20), position = c(&#39;left&#39;, &#39;bottom&#39;)) # Create our third map and store at btm_3: Plot our London wards by the btns_rate # column btm_3 &lt;- tm_shape(london_ward_shp) + tm_polygons(col = &quot;btns_rate&quot;, palette = &quot;YlGnBu&quot;, style = &quot;jenks&quot;, title = &quot;Near Station Bike Theft Rate&quot;) + # Add a legend etc tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5, scale = 0.65, fontfamily = &quot;Helvetica&quot;) #+ # tm_compass(type=&#39;arrow&#39;, position = c(&#39;right&#39;, &#39;bottom&#39;)) + tm_scale_bar(breaks # = c(0, 5, 10, 15, 20), position = c(&#39;left&#39;, &#39;bottom&#39;)) # You can uncomment our north arrow and scale bar if you would like # Arrange our three maps into 2 rows tmap_arrange(btm_1, btm_2, btm_3, nrow = 2) You can go ahead and customise these maps as you would like, including changing their classification styles and breaks. I have currently removed the north arrow and scale bar for clarity and the intention would be to use these maps as graphics in a report. By stacking our three variables together, we can gain a better understanding of their respective and interrelated distributions. 2. Aggregating and visualising bike theft by train station There are however many issues with aggregating point data to an areal unit - particularly when we are looking at a variable we cannot really normalise by population but ultimately know is likely to be affected by population! After all, we can see in our previously visualisations that our rates will be heavily influenced by a) the number of train stations in each ward and b) the size of the ward (particulary as we’re dealing with distances here). As a result, aggregation by an areal unit might not be the best approach to visualising our data. When we are use counts of points from specific locations, as in our scenario, we can look to use proportional symbols to visualise our data. Proportional symbols are a type of thematic map that use map symbols that vary in size to represent a quantitative variable. Proportional symbols maps can “take totals as input data and it uses the data values to calculate the area of a symbol, that are then proportional and comparable to one another” (Field, 2020). Mapping coronavirus responsibly One of the best explanations on how to map event data - in this case, COVID-19 - well, is a blog post released by one of Esri’s resident cartographers, Kenneth Field. In February 2020, as COVID-19 was beginning to spread across the world, Kenneth took aim at the irresponsible maps coming out that were trying to map COVID-19. (He later went on to advise the BBC in the UK to help them out after they had a dodgy few weeks in the cartography business.) You can read his blog post here - it’s only approximately 15 minutes, but gives you a good insight into how to make a good map when it comes to mapping event data. To use proportional symbols with our london_stations_osm spatial dataframe, we need to calculate, for each train station, the number of bike thefts that have occured within 400m. To calculate this, we’ll use a solely geometric operations approach as, at the point of writing this practical, this seems to be the best way forward. We will: Create individual 400m buffers for each of the stations in our london_stations_osm spatial dataframe. Use the st_intersects function for a PIP calculation of number of bike thefts within each buffer. Join this number to our individual train stations using a spatial join. Visualise this count at each station using proportional symbols. We’ve completed steps 1 and 2 before, but we’ve not yet come across our last but super important geometric operation: a spatial join. Spatial joins can be used to match attributes between two features that do not have a similar attribute to match them through an attribute join. Whilst we are certainly now familiar with joining two datasets by using shared ‘key’ variable, a spatial data join applies the same concept, but instead “relies on shared areas of geographic space (it is also known as spatial overlay). As with attribute data, joining adds a new column to the target object (the argument x in joining functions), from a source object (y)” (Lovelace et al, 2021). To conduct a spatial join in R, we use the st_join() function from the sf library. In our st_join() code below, we also add two parameters: left=TRUE (in the similar idea of a left_join) that all observations in our london_stations_osm spatial dataframe is kept during our join. largest=TRUE to ensure that if we find that point overlaps with any other buffers, the buffer it generated is the buffer from which our count statistic is taken. Let’s move ahead with our final steps of data processing and analysis. Generate the individual buffers for our stations and count the number of bike thefts within each. # Generate a 400m buffer around each of our london_stations_osm dataset indi_station_400m_buffer &lt;- london_stations_osm %&gt;% st_buffer(dist = 400) # Count the number of bike thefts within each of our buffers, store as a new # column indi_station_400m_buffer$total_bike_theft &lt;- lengths(st_intersects(indi_station_400m_buffer, bike_theft_2019)) Join the total_bike_theft column to our london_stations_osm spatial dataframe using the st_join() function to enable a spatial join with the indi_station_400m_buffer: # Join the `total_bike_theft` column to our `london_stations_osm` spatial # dataframe london_stations_osm &lt;- london_stations_osm %&gt;% st_join(indi_station_400m_buffer[&quot;total_bike_theft&quot;], left = TRUE, largest = TRUE) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries We can now go ahead and plot our data as proportional symbols by using the tm_bubbles() function - here we simply provide our function’s size parameters with the column we want to visualise our data by. To avoid overcomplicating our visualisation, we’ll keep our bubbles to one colour. Instead of using proportional symbols, you could also colour our london_stations_osm spatial dataframe by the total_bike_theft column, with the parameters as col=\"total_bike_theft\", palette=\"Blues\". But for now we’ll keep it focused on our proportional symbols as this is the point of our mapping here! Plot the resulting count column as proportional symbols using the tm_bubbles() function: # Ensure tmap is in the plot mode tmap_mode(&quot;plot&quot;) # Plot our London outline in grey tm_shape(london_outline) + tm_fill(palette = &quot;grey&quot;) + # Plot our lond_stations_osm as bubbles, using the total_bike_theft to determine # size tm_shape(london_stations_osm) + tm_bubbles(size = &quot;total_bike_theft&quot;, col = &quot;skyblue4&quot;, style = &quot;pretty&quot;, scale = 1, border.col = &quot;white&quot;, title.size = &quot;Bike theft within 400m of station&quot;) + tm_layout(legend.position = c(&quot;left&quot;, &quot;top&quot;)) + # Add a north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;top&quot;)) + # Add a scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) + # Add our OSM contributors statement tm_credits(&quot;© OpenStreetMap contributors&quot;) And that’s it - here’s potentially our final map, and perhaps the best representation of our data. We’ve just only tackled a small amount of the different geometric operations and spatial queries you can do with spatial data within R. There’s many more available for us to use - but this would make this practical so much longer. Again, make sure to read through your sf cheatsheet to find out about the different types of operations and queries we can run on spatial data - and feel free to try them out on our data above. Assignment 2: Small multiples of bike theft near train stations in London in 2019 For your second assignment this week, I’d like you to add your Proportional Symbols map to your small multiples arrangement and export your four maps. Try to style these in similar aesthetics to ensure they work with one another. Upload your final map visualisation to the respective folder in your seminar group. I’d like you to also think about what each map tells us about bike theft in London and specifically bike theft near train stations in London. Prepare some notes, as I will be asking about this in our seminar in Week 8. Limitations of visualising and analysing point data through aggregation Our final approaches here to analysing our point data looks to aggregate our bike theft crime by either aggregating it at the ward scale or to each station. But, often, aggregation of individual points does not communicate the clustering present within our data very clearly. Instead, it might have been best to look to analyse the precise clustering of our data and compare the distributions of these clusters against our train station locations. To achieve this type of analysis, we can look to the topic of Point Pattern Analysis, which we’ll be covering in detail next week, to see whether our clusters of bike theft can help answer our original hypothesis: that bike theft primarily occurs near tube and train stations. Extension: Using spatial queries to calculate the number of bike thefts within 400m of each station As we’ve demonstrated, there are often several approaches to using geometric operations and spatial queries to process the same data and achieve the same output. To create the data for our proportional symbols map, we focused on a purely geometric operations approach. But could we have completed this processing using spatial queries - and would it have bee faster? The task for this week’s extension is to simply answer this question - and provide the code that completes this analysis. This truly is a challenge I’d be very keen to hear the answer to - as it’s not a question I’ve yet asked or solved myself in R (just simply not having time to consider it!). Therefore, if you can solve this, please upload your final script to our seminar folders as usual. My recommendations are to study the outputs of your queries, for example, if you remove the sparse parameter or try to you run the query on the entire dataset, not the single unioned feature, and see what is produced. Remember, each time, we’ve had to union the comparison dataset in order to run our query successfully - but not unioning our data could be the answer to this coding challenge! I look forward to seeing if any of you can solve this by our seminar in Week 8! Recap - Analysing Spatial Patterns II: Geometric Operations and Spatial Operations This week, we’ve looked at Geometric Operations and Spatial Operations and how we can put them into action to process, clean, validate and analyse our spatial data. They really are the building blocks to spatial analysis! What you should also have seen is that there are of course differences in how they work, with geometric operations editing the geometries of our datasest, whilst spatial operations providing simply an answer to our question. We’ve seen how we can use both of these approaches to conduct proximity-based calculations, as well as using geometric operations for Point-in-Polygon counts. We’ve also expanded our visualisation knowledge, creating both an examplee of small multiple maps and a proportional symbol map. Finally, you’ve had a basic introduction into how to download OpenStreetMap data for use within R. Overall, a very busy week indeed. Next week, we’ll move onto the next section of content on [Analysing Spatial Patterns: Point Pattern Analysis]. Learning Objectives You should now hopefully be able to: Understand how to use different geometric operations and spatial queries within your spatial analysis workflow for data cleaning, processing and analysis Be able to implement geometric operations, including clips and unions, and spatial queries within R and sf Know how to download data from OpenStreetMap using the osmdata package Run Point-in-Polygon operations Make small multiples of maps and arrange them togoether Create proportional symbol maps Acknowledgements This page is original to GEOG0030. The datasets used in this workshop (and resulting maps): © OpenStreetMap contributors Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). "],["analysing-spatial-patterns-iii-point-pattern-analysis.html", "8 Analysing Spatial Patterns III: Point Pattern Analysis", " 8 Analysing Spatial Patterns III: Point Pattern Analysis Welcome to Week 8 in Geocomputation! This week, we’ll be looking at how we can use Point Pattern Analysis (PPA) to detect and delineate clusters within point data. Within point pattern analysis, we look to detect clusters or patterns across a set of points, including measuring density, dispersion and homogeneity in our point structures. There are several approaches to calculating and detecting these clusters, which are explained in our main lecture. We then deploy several PPA techniques, including Kernel Density Estimation, on our bike theft data to continue our investigation from last week. Within the Extension, we then look at the DB-Scan algorithm as an alternative approach to detecting and confirming the location of clusters in relation to our train and tube stations. In terms of data visualisation, you’ll learn how to add a basemap to a tmap static map and how to use tmap to map a raster dataset. Week 8 in Geocomp Video on Stream This week’s content introduces you to Point Pattern Analysis and its use in spatial analysis. We have three areas of work to focus on: Understanding Point Pattern Analysis and its different techniques Applying different PPA techniques in R using the spatstat library Extension: Application of dbscan for Point Pattern Analysis This week’s content is split into 4 parts: Workshop Housekeeping (20 minutes) Point Pattern Analysis (30 minutes) Point Pattern Analysis in R with spatstat (90 minutes) Extension: Point Pattern Analysis in R with dbscan (45 minutes) This week, we have 1 lecture and 1 assignment within this week’s main workshop content. In addition, we have a second short video and pratical in this week’s Extension. Learning Objectives By the end of this week, you should be able to: Explain the different approaches to detecting clusters in Point-Pattern Analysis Run a Kernel Density Estimation and explain the outputs of a KDE confidently Run a Ripley’s K function and compare to a Poisson distribution Add a basemap within the tmap environment Map a raster dataset within the tmap environment Extension: run a DBSCAN analysis and interact with its outputs Extension: Identify a for loop within a chunk of code This week, we continue to investigate bike theft in London in 2019 - as we look to confirm our very simple hypothesis: that bike theft primarily occurs near tube and train stations. This week, instead of looking at the distance of individual bike thefts from train stations, we’ll look to analyse the distribution of clusters in relation to the stations. We’ll first look at this visually in our main workshop content and then, in our Extension task, look to compare these clusters to the location of train and tube stations quantitatively using geometric operations, covered last week. To complete this analysis, we’ll continue to use: Bike theft in London in 2019: A 2019 version of our crime dataset for London. Train and Tube Stations locations in London from last week. As a result, our workshop housekeeping this week will be relatively short! Workshop Housekeeping Let’s get ourselves ready to start our lecture and practical content by first downloading the relevant data and loading this within our script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk8-bike-theft-PPA.r. At the top of your script, add the following metdata (substitute accordingly): # Analysing bike theft and its relation to stations using point pattern analysis # Script started March 2021 NAME Dependencies (aka libraries) Now we’ll install the libraries we need for this week. All of the geometric operations and spatial queries we will use are contained within the sf library. For our Point Pattern Analysis, we will be using the spatstat library (“spatial statistics”). The spatstat library contains the different Point Pattern Analysis techniques we’ll want to use in this practical. We’ll also need the raster library, which provides classes and functions to manipulate geographic (spatial) data in ‘raster’ format. We’ll use this package briefly today, but look into it in more detail next week. We’ll also using the rosm library (“R OSM”), which provides access to and plots OpenStreetMap and Bing Maps tiles to create high-resolution basemaps. If you complete the Extension, you’ll also need to install dbscan. Remember to use the `install.packages(“package”) command in your console. Within your script, add the following libraries for loading: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) library(janitor) library(RColorBrewer) library(spatstat) library(raster) library(rosm) library(dbscan) library(leaflet) Remember to select the lines of code you want to run and press CMD (Mac)/CTRL(Windows) + Enter/Return - we won’t remind you to run each line of code in the remainder of the practical sessions. Datasets for this week This week, we’ll continue to use our data from last week. This includes: London Ward boundaries from 2018 (this should already be in your raw data folder) 2019 crime in London from data.police.uk Train and Tube Stations from OpenStreetMap. You should load these datasets as new variables in this week’s script. You should have the original data files for both the London Wards and 2019 crime already in your raw data folder. If you did not export your OpenStreetMap train and tube stations from our practical last week and do not have your working environment saved, you will need to re-run parts of your code from last week to download and then export the OpenStreetMap data. If this is the case, before you get started with this week’s script, open last week’s script: wk7-bike-theft-analysis.r. Then, if your london_stations_osm is not already in your Environment window, to generate and export the OpenStreetMap (OSM) train and tube stations data, you will need to run the code that: Loads your libraries Generates your OSM download Filters your OSM download to stations only Once you’ve run this code, you can write out your OSM station data to your raw -&gt; transport folder using the following code in your console: # Write out london_stations_osm to a shapefile st_write(london_stations_osm, &quot;data/raw/transport/osm_stations.shp&quot;) For those of you with your variable saved from last week and already in your Environment, I would still recommend writing out our london_stations_osm variable to a shapefile anyway and then follow the next instructions to reload it back in. This ensures that in the future, this script will work as a standalone script and is not reliant on Week 7’s output. We can now load all three datasets into R. Loading our data Let’s go ahead and load all of our data at once - we did our due diligence last week and know what our data looks like and what CRS they are in, so we can go ahead and use pipes to make loading our data more efficient. Load all three datasets and conduct necessary pre-processing: # Load all three datasets # Read in our 2018 London Ward boundaries Already in BNG - we checked last week london_ward_shp &lt;- read_sf(&quot;data/raw/boundaries/2018/London_Ward.shp&quot;) # Read in our OSM tube and train stations data Already converted into BNG last # week london_stations_osm &lt;- read_sf(&quot;data/raw/transport/osm_stations.shp&quot;) # Read in our crime data csv from our raw data folder bike_theft_2019 &lt;- read_csv(&quot;data/raw/crime/all_crime_2019.csv&quot;) %&gt;% # clean names clean_names() %&gt;% # filter according to crime type and ensure we have no NAs in our dataset filter(crime_type == &quot;Bicycle theft&quot; &amp; !is.na(longitude) &amp; !is.na(latitude)) %&gt;% # select just the longitude and latitude columns dplyr::select(longitude, latitude) %&gt;% # transform into a point spatial dataframe note providing the columns as the # coordinates to use plus the CRS, which as our columns are long/lat is # WGS84/4236 st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4236) %&gt;% # convert into BNG st_transform(27700) %&gt;% # clip to London st_intersection(london_ward_shp) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries Let’s create a quick map of our data to check it loaded correctly: # Plot our London Wards first tm_shape(london_ward_shp) + tm_fill() + # Then bike crime as blue tm_shape(bike_theft_2019) + tm_dots(col = &quot;blue&quot;) + # Then our stations as red tm_shape(london_stations_osm) + tm_dots(col = &quot;red&quot;) + # Add a north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + # And a scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Great - that looks familiar! This means we can move forward with our data analysis and theoretical content for this week. Point Pattern Analysis In our previous practicals, we have aggregated our point data into areal units, primarily using administrative geographies, that facilitates its easy comparison with other datasets provided at the same spatial scale, such as the census data, as well as enables us to conduct spatial autocorrelation tests and choropleth mapping. However, when locations are precisely known, spatial point data can be used with a variety of spatial analytic techniques that go beyond the methods typically applied to areal data. As we saw last week, we were able to use specific geometric operations, such as distance queries, buffers and point-in-polygon counts, on the precise locations of our points for analysis, as well as proportional symbols mapping for visualisation. Depending on your research problem and aim, points do not necessarily have to be aggregated and there are many applications in which you want to work with the point locations directly. In particular, there are two core areas of spatial analysis techniques that have developed that are unique to point data: point pattern analysis and geostatistics. Whilst we will look at geostatistics next week, this week, we focus on Point Pattern Analysis. What is Point Pattern Analysis? Point pattern analysis (PPA) studies the spatial distribution of points (Boots &amp; Getis, 1988). As outlined above, PPA uses the density, dispersion and homogeneity in our point datasets to assess, quantify and characterise its distribution. Over the last fifty years, various methods and measurements have been developed to analyze, model, visualize, and interpret these properties of point patterns (Qiang et al, 2020). There are three main categories of PPA techniques: Descriptive statistics Density-based methods Distanced-based merthods The use of descriptive statistics will provide a summary of the basic characteristics of a point pattern, such as its central tendency and dispersion. Descriptive statistics provide a simple way of visualising a dataset as a whole, from plotting the median or mean centre, or, often preferably, a standard deviational eclispse for those datasets that display a directional pattern. Descriptive statistics are however somewhat limited in what they can communicate about a dataset’s pattern. More powerful techniques have been developed to explore point patterns, which will either be density-based or distanced-based, depending on the spatial properties the technique is considering (Gimond, 2020). Density-based methods focus on the first-order properties of a dataset, i.e. the variation in the individual locations of the points in the dataset across the area of interest, and will characterise our dataset’s distribution accordingly in terms of density. Distanced-based methods focus on the second-order properties of a dataset, i.e. the interactions between points within our data and whether they appear to have influence on one another and form clusters, and will characterise our dataset’s distribution accordingly in terms of dispersion. In our lecture, we will look at all three categories and their specific techniques in preparation for applying several of them to our bike theft dataset afterwards in R ,using the spatstat library. Lecture: Point Pattern Analysis Slides | Video on Stream Point Pattern Analysis in R with spatstat We’ve now heard about the different types of PPA techniques available to us as geographers and spatially-enabled thinkers to assess our dataset and its distribution - so it’s about time we apply this within R to our bike theft dataset. To do so, we’ll be using the spatstat library, that has been developed by Baddeley, Rubak and Turner since 2005. As their documentation states, spatstat “is a package for the statistical analysis of spatial data. Its main focus is the analysis of spatial patterns of points in two-dimensional space” (Baddeley et al, 2021). According to it’s “Get Started with spatstat” documentation, \"spatstat supports a very wide range of popular techniques for statistical analysis for spatial point patterns, including: Kernel estimation of density/intensity Quadrat counting and clustering indices Detection of clustering using Ripley’s K-function Model-fitting Monte Carlo tests as well as some advanced statistical techniques\" (Baddeley et al, 2020). We will only cover a brief amount of the functionality the package offers - it has almost 1,800 pages of documentation and over 1000 commands, so it would be near impossible to cover everything even if we had a full module dedicated just to PPA. Instead, this week, we’ll look to see how we can use spatstat to conduct the key PPA techniques outlined earlier in our lecture, including: Quadrat Analysis Kernel Density Estimation Nearest Neighbour Ripley’s K function But before we get started with our analysis, you need to know one critical piece of information in order to use spatstat: we need our data to be in the format of the ppp object. Using spatstat in R: the ppp object If you remember me explaining in Week 5, there are some spatial packages in R that require us to convert our data from an sf simple features object (e.g. for point data, a SpatialPoints object) into a different spatial object class - and spatstat is one of them! The spatstat package expects point data to be in the ppp format. The ppp format is specific to spatstat, but you may find it used in other spatial libraries. An object of the class ppp represents a two-dimensional point dataset within a pre-defined area, known as the window of observation, a class in its own right, known as owin in spatstat. We can either directly create a ppp object from a list of coordinates (as long as they are supplied with a window) or convert from another data type (using the as.ppp() function). However, as spatstat predates sf, this conversion function does not yet work with sf data objects. Instead, therefore, we have to create a workaround workflow that enables us to extract the coordinates from our bike_theft_2019 spatial dataframe for use within the ppp function. We could of course simply reload the csv from our raw data files and supply the coordinates from the dataframe we would generate - but where’s the fun in that?! Instead, we will: Extract the geometry of our bike theft points from our bike_theft_2019 spatial dataframe using the st_coordinates() function from the sf library Store this geometry as two separate columns within a matrix Provide these columns, alongside an observation window equal to our london_ward_shp spatial dataframe, to create a PPP object Create our spatstat ppp object for use in our PPA: # First, let&#39;s set our window of observation as the entirety of London window &lt;- as.owin(london_ward_shp) # Next, extract the coordinates of our bike_theft_2019 sdf This stores our # coordinates as a matrix bike_theft_xy &lt;- bike_theft_2019 %&gt;% st_coordinates() # Create a ppp object, setting x and y equal to the respective columns in our # matrix Set the window equal to our window variable bike_theft_ppp &lt;- ppp(x = bike_theft_xy[, 1], y = bike_theft_xy[, 2], window = window) ## Warning: data contain duplicated points # Plot our ppp object plot(bike_theft_ppp) Our plot shows us our bike_theft_ppp ppp object, which includes both the coordinate points of our bike theft data and our London window. You should also see your bike_theft_ppp ppp object variable appear in your Environment window - as well as see a message stating that our data contain duplicated points. Let’s see if this is true - we can first use a logical statement from the R base library to check if our bike_theft_ppp object contains duplicated points and then count the total number of duplicates exist using the multiplicity function (which “counts the number of duplicates for each point in a spatial point pattern”) from the spatstat library. Check and count how many duplicated points our dataset contains: # Check for duplicates using the anyDuplicated function anyDuplicated(bike_theft_ppp) ## [1] TRUE # Count the number of duplicated points and sum this sum(multiplicity(bike_theft_ppp) &gt; 1) ## [1] 12323 This means we have 12,323 (out of 18,690!) duplicated points. What’s the issue with duplicated ponts? One of the key assumptions underlying many analytical methods is that all events are unique. In fact, some statistical procedures actually may return very wrong results if duplicate points are found within the data. In terms of our bike theft data, it is unsurprising that it contains duplicates. If you remember from earlier on in our module, we explained how the Police service record the locations of the crimes within the dataset and how they use snapping points, to which crimes are snapped to in order to preserve the anonynmity and privacy of those involved. This is an issue in spatial point pattern analysis as we need our “events”, i.e. each record of a crime and its respective location, to be unique in order for our analysis to be accurate. To account for these issues within our dataset (and other datasets that contain duplicates), we have three options: We can remove the duplicates and pretend they simply are not there. However, this is feasible only when your research problem allows for this, i.e. the number of points at each location is not as important as the locations themselves, and therefore you are happy to ‘ignore’ some of the data. Create and assign a weighting schema to our points, where each point will have an attribute that details the number of events that occur in that location - and utlise this weight within our PPA techniques. Weights however can only be used with certain PPA techniques (e.g. Kernel Density Estimation). Force all points to be unique by utilising a function that offsets our points randomly from their current location. If the precise location is not important for your analysis - or, for example, you are dealing with data that in our case is already slightly offset, we can introduce a “jitter” to our dataset that slightly adjusts all coordinates so that the event locations do not exactly coincide anymore. This way, our duplicates will no longer have the same precise location. This approach however introduces a certain level of uncertainty into the precise location of any analysis derived from our datasets, e.g. cluster extents, as we’ll see later. Each approach will have a specific compromise, which you will have to decide upon depending on the type of analysis you are completing. In our case, we will choose the jitter approach to keep all of our bike theft events. We know that already the location of our bike thefts are not precise locations of the original theft, therefore adding additional offset will not detract from our analysis. Furthermore, the number of thefts is incredibly important to our analysis, thereore option 1 is not feasible with 12,323 duplicated points. We also want to demonstrate a range of techniques in our practical today, so option 2 is also not viable. Let’s shift all our coordinates slighlty to ‘remove’ our duplicates and essentially ‘move’ all points into unique locations. We’ll use the rjitter function from the spatstat library, which applies an independent random displacement to each point in a point pattern. If you look at the rjitter documentation, you’ll also find that we can set many parameters to ensure that the radius of pertubation is kept small etc and you can read about the parameters we’ve used in our current code. Add a “jitter” (i.e. offset) to our bike_theft_ppp object - and then check for duplicates: # Add an offset to our points using the rjitter function bike_theft_ppp_jitter &lt;- rjitter(bike_theft_ppp, retry = TRUE, nsim = 1, drop = TRUE) # Count the number of duplicated points of the new jitter dataset anyDuplicated(bike_theft_ppp_jitter) ## [1] FALSE # Plot the resulting ppp object plot(bike_theft_ppp_jitter) Great, we now have our bike theft data in a format ready to be analysed with our different PPA techniques using the spatstat library! Events, marks and ppp objects One additional thing to note about the ppp data object is that a ppp object does not necessarily have to have any attributes (our fields) associated with the events each point our point data represents. If your data does have attributes (such as calculating a weight as outlined above for dealing with duplications), these attributes are referred to as marks within the spatstat environment and thus documentation. Be aware that some functions do require these marks to be present - and you’ll find this out only from the spatstat documentation. We will not use any functions/techniques today that require marks. We’ll first look at how we can deploy density-based methods, including Quadrat Analysis and Kernel Density Estimation, and then distance-based methods, including Nearest Neighbour and Ripley’s K function, on our bike theft data using spatstat. Despite its extensive coverage of many complex PPA techniques, the spatstat library does not contain many functions to analyse our data using basic descriptive statistics, therefore we will not look at descriptive statistics at this time. However, if you would like to create a Standard Deviational Ellipse (SDE) of your data at any point, you should look at the aspace library which contains a function to create a SDE. This library is also available as a plug-in in QGIS. Density-Based Methods Density-based techniques are used to characterise the pattern of a point dataset utilising its general distribution. A bit like our spatial autocorrelation techniques, we can calculate densities at both the global and local scale. However, as you’ll see, for PPA, global density really does not tell us much more about the distribution of our data - in terms of areas of high and low densities, for example. This is where local density techniques such as Quadrat Analysis and Kernel Density Estimation can help us visualise these differences in density in our data’s distribution. Global Density We can create a simple understanding of our data’s distribution by first understanding its global density - this is simply the ratio of the observed number of points, \\(n\\) , to the study region’s surface area, \\(a\\): \\(\\widehat{\\lambda} =\\frac{n}{a}\\) Calculate the global density of our bike theft point data relative to London: # Calculate the global density of our bike points Try to understand this code # yourself - I&#39;ll ask in the seminar how this equation works! global_density &lt;- length(bike_theft_2019$NAME)/sum(st_area(london_ward_shp)) # Return global_density to our console global_density ## 1.172009e-05 [1/m^2] We can see that we have a global density of 0.0000172 bike thefts per m^2 in London. This simple density analysis could be supported with further descriptive statistics, however we still would know little about the local density of our points. Local Density: Quadrat Analysis The most basic approach to understanding a point pattern’s local density is to simply measure the density at different locations within the study area. This approach helps us assess if the density is constant across the study area. The most simplest approach to this measurement is through Quadrat Analysis, where the study area is divided into sub-regions, aka quadrats. The point density is then computed for each quadrat, by dividing the number of points in each quadrat by the quadrat’s area. As you will have seen in the lecture, quadrats can take on many different shapes (and utlise different approaches to creating these shapes). The most basic approach is using squares (or rather, a grid). Furthermore, the choice of quadrat numbers and quadrat shape can influence the measure of local density and therefore must be chosen with care. We will start with a simple quadrat count by dividing the observation window into 15 x 15 sections and then counting the number of bicycle thefts within each quadrant using the quadratcount() function within R. Calculate the number of bike thefts in our quadrats in London: # quadratcount in a 15 x 15 grid across the observational window biketheft_quadrat &lt;- quadratcount(bike_theft_ppp_jitter, nx = 10, ny = 10) # inspect plot(biketheft_quadrat) Our resulting quadrat count shows total counts of bike theft - we can see quite quickly that the quadrats in central London are likely to have a higher local density as their count is much higher than those on the outskirts of London. If we divided our count by the area covered by each quadrat, we’d also be able to calculate a precise local density. We won’t do this for now, as realistically, it is not often that you’d want to use quadrat analysis for actual PPA. But the reason why we look at this technique is that it provides us with an easy way to think about how to compare our data distribution and how this relates to the Poisson distribution of Complete Spatial Randomness (CSR). Quadrat Analysis &amp; Complete Spatial Randomness Testing When looking at the distribution of our points and the respective patterns they show, the key question we often want to answer as geographers and spatially-enabled thinkers is: are our points clustered, randomly distributed (i.e. display complete spatial randomness), uniform or dispersed? Whilst we can visually assess this distribution, to be able to statistically quantify our data’s distribution, we can compare its distribution to that of the Poisson distribution. The Poisson distribution describes the probability or rate of an event happening over a fixed interval of time or space. The Poisson Distribution applies when: The events are discrete and can be counted in integers Events are independent of each other The average number of events over space or time is known Point data that contains a random distribution of points is said to have a Poisson distribution. The Poisson distribution is very useful in Point Pattern Analysis as it allows us to compare a random expected model to our observations. Essentially, if our data does not fit the Poisson model, then we can infer that something interesting might be going on and our events might not actually be independent of each other. Instead, they might be clustered or dispersed and there is likely to be underlying processes influencing these patterns. The most basic test of CSR with the Poisson distribution in PPA can be completed with our Quadrat Analysis results. We compare our quadrat results with a Poisson distribution for the same quadrats and determine whether the pattern is generated in a random manner; i.e. whether the distribution of points in our study area differs from complete spatial randomness (CSR) or whether there are some clusters present. To enable this, we can run a Chi-Squared Test of our data against a theoretical randomly generated point pattern dataset with the same number of points and window, with the null hypotheses that our point data have been generated under complete spatial randomness. Our chi-squared test will tell us whether our data is distributed under the null hypothesis - and determine whether there is a statistically significant difference between the expected distribution (i.e. CSR) and the observed distribution (our bike theft point data). We use the quadrat.test() function from spatstat that “performs a test of Complete Spatial Randomness for a given point pattern, based on quadrat counts” (spatstat documentation, 2020). Run a Chi-Squared Test of our data to confirm whether or not the data is randomly distributed: # Chi-square between observed point pattern of our data and Poisson sampled # points quadrat.test(bike_theft_ppp_jitter, nx = 10, ny = 10) ## Warning: Some expected counts are small; chi^2 approximation may be inaccurate ## ## Chi-squared test of CSR using quadrat counts ## ## data: bike_theft_ppp_jitter ## X2 = 56524, df = 80, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## ## Quadrats: 81 tiles (irregular windows) Our \\(p\\) value is well below 0.05 (or 0.01 for that matter), which means there is a statistically signficant difference between the expected distribution (i.e. CSR) and the observed distribution (our bike theft point data). We can therefore reject the null hypothesis that our point data have been generated under complete spatial randomness and confirm that our point pattern was not generated in a random matter. This is hardly very suprising! However our completing both a quadrat analysis and the resulting Chi-Squared test is not exactly the most efficient way of looking to understand the relative local densities of our dataset - nor can we compare these results to the location of our train and tube stations to look into our original hypothesis: that bike theft primarily occurs near tube and train stations. Local Density: Kernel Density Estimation We now have an understanding of whether our data is randomly distributed or not - and our quadrats give us a very coarse understanding of where there may be clusters within our data. But instead of looking at the distribution of our bike theft with the boundaries of our quadrats (or any other tessellation we could pick), we can also analyse our points using a Kernel Density Estimation (KDE). As explained in our lecture, KDE is a statistical technique to generate a smooth continuous distribution between data points that represent the density of the underlying pattern. Within spatial analysis, a KDE will produce a surface (raster) that details the estimated distribution of our event point data over space. Each cell within our raster contains a value that is this estimated density at that locatiion; when visualised in its entirety as the whole raster, we can quickly identify areas of high and low density, i.e. where are clusters are located in our dataset. To create this surface, a KDE computes a localised density for small subsets of our study area - but unlike quadrat analysis, these subsets overlap one another to create a moving sub-region window, defined by a kernel. A kernel defines the shape and size of the window and can also weight the points, using a defined kernel function (Gimond, 2020). The simplest kernel function is a basic kernel where each point in the kernel window is assigned equal weight. The kernel density approach generates a grid of density values whose cell size is smaller than that of the kernel window. Each cell is assigned the density value computed for the kernel window centered on that cell (Gimond, 2020). The resulting surface is created from these individually, locally calculated density values. Producing a KDE in R is very straight-forward in spatstat, using your ppp object and the density.ppp() function. However, you will need to consider both the bandwidth or diameter of your Kernel (sigma) and whether you want to apply a weighting to your points using a function, as we’ll see below. First, let’s go ahead and create a simple KDE of bike theft with our bandwidth set to 100m. Generate a KDE of our bike theft data, with a kernel of 100m: # Kernel density estimation of our bike theft ppp object Note sigma is defined in # the same units as your CRS, in this case metres for BNG plot(density.ppp(bike_theft_ppp_jitter, sigma = 100)) We can see from just our KDE that there are visible clusters present within our bike theft data, particularly in and around central London. We can even see our south-west cluster that we saw in our proportional symbols map last week. We can go ahead and vary our bandwidth to to see how that affects the density estimate. Change the sigma to a bandwith of 500m: # Kernel density estimation of our bike theft ppp object with 500m bandwith plot(density.ppp(bike_theft_ppp_jitter, sigma = 500)) Our clusters now appear brighter and larger than our KDE with a 100m bandwidth - this is because changing the bandwidth enables your KDE to take into account more points within its calculation, resulting in a smoother surface. However, there are issues with oversmoothing your data - as you can see above, our clusters are not as well defined and therefore we may attribute high levels of bike theft to areas where there actually isn’t that much! Smaller bandwidths will lead to a more irregular shaped surface, where we have more precision in our defined clusters - but, once again, there are issues of undersmoothing. In our case, as we know bike theft is not exactly a phenomena that obeys strict square boundaries, we may run into similar issues of boundary effects that we see in areal unit aggregation, and end up not extending our clusters far enough to cover our “hotspot” areas. Whilst there are automated functions (e.g. based on maximum-likelihood estimations) that can help you with selecting an appropriate bandwidth, in the end you will have to make a decision on what is most appropriate for your dataset. Thinking through the phenomenom that you are analysing will help - a bit like our decisions we made last week in terms of thinking through our buffer sizes! Although bandwidth typically has a more pronounced effect upon the density estimation than the type of kernel used, kernel types can affect the result too. When we use a different kernel type, we are looking to weight the points within our kernel differently: Kernel Types and Their Distributions. Source: Wikipedia. Each function will result in a slightly different estimation. As Levine explains: “The normal distribution weighs all points in the study area, though near points are weighted more highly than distant points. The other four techniques use a circumscribed circle around the grid cell. The uniform distribution weighs all points within the circle equally. The quartic function weighs near points more than far points, but the fall off is gradual. The triangular function weighs near points more than far points within the circle, but the fall off is more rapid. Finally, the negative exponential weighs near points much more highly than far points within the circle and the decay is very rapid.” (Levine, 2013: 10.10). So which one should you use? Levine (2013, ibid) produces the following guidance: “The use of any of one of these depends on how much the user wants to weigh near points relative to far points. Using a kernel function which has a big difference in the weights of near versus far points (e.g., the negative exponential or the triangular) tends to produce finer variations within the surface than functions which weight more evenly (e.g., the normal distribution, the quartic, or the uniform); these latter ones tend to smooth the distribution more.” Deciding which function is most suitable for your analysis will all depend on what you are trying to capture. We can compare and see the impact of different functions on our current dataset looking at the default kernel in density.ppp(), which gaussian, alongisde the epanechnikov, quartic or disc kernels. Note, the sigma in these KDEs is set to 400m: To change the kernel within your KDE, you simply need to add the kernel= parameter and set it to one of the kernels available, denoted as a string, e.g. “epanechnikov”, “quartic”, “disc”. Ultimately, bandwidth will have a more marked effect upon the density estimation than kernel type. For now, however, no matter which kernel or which bandwidth (within reason, of course) we use, we can be quite confident in stating that bike theft in London in 2019 is not a spatially random process and we can clearly see the areas where bicycle theft is most concentrated. KDE and Raster Mapping We’ve now seen how we can create a KDE to show the local density of our dataset - but how can we use this new data in our original analysis that looks to find out whether bike theft primarily occurs near tube and train stations? The main use of a KDE is primarily for visual analysis of our point data distribution - we could easily write at least a hundred words on what our KDE above shows. However, our current plotting approach is quite limited - if you hadn’t noticed, we’ve primarily been using the R base plotting techniques to display the results of our density.ppp() function. It would therefore be difficult to create any maps that allow visual comparison to our train stations - nor could we really complete any further analysis on our KDE dataset. This is because, at the moment, our KDE raster is stored as a spatstat object - and is not, currently, a standalone raster dataset. As a result, we cannot use our KDE with other visualisation libraries such as tmap - or in the future ggplot2. To enable this use, we need to first export our KDE spatstat object into a standalone raster that can be used with these libraries. We therefore need to look to the raster library that is capable of doing just that! Until now, with our spatial data, we’ve primarily used vector data that the sf library can read, load and manage - however the sf library does not contain the right functions to enable the same reading, loading and management of raster data. As a result, it is not a suitable spatial library for dealing with raster data. Instead, we need to use the raster library, which is the default spatial library for dealing with raster data (i.e. just as we use sf for vector, we use raster for raster!). We’ll look into this library and its many functions in a little more detail next week as we look at geostatistics and interpolation. For this week, we need the library for only one very specific task: export our spatstat KDE object into a raster dataset that we can then map in the tmap library - alongside, as you’ll see, a basemap for visualisation. Converting our spatstat KDE object into a raster To convert spatstat KDE object into a raster, we only need one very simple function from the raster library: raster(). This function “creates a RasterLayer object. RasterLayer objects can be created from scratch, a file, an Extent object, a matrix, an ‘image’ object, or from a Raster, Spatial, im (spatstat), asc, kasc (adehabitat), grf (geoR) or kde object” (raster documentation. We can check to see if this function will work with our current spatstat KDE object by double-checking the documentation for density.ppp() function and looking at what Value the function returns (i.e. what object type is the KDE image we see above). The documentation tells us that the result, by default, from the density.ppp() function is “a pixel image (object of class \"im\")” (spatstat documentation) - this matches one of the accepted inputs of the raster() function, as shown above, so we know our function will work with our resulting kde object. To return a raster from our density.ppp() function, we simply need to pipe its output into the raster function, as we’ll do below. Pipe the output of our density.ppp() function into the raster() function: # Create a raster directly from the output of our KDE 400g stands for a 400m # bandwidth, to match our distance from last week, with a gaussian kernel kde_400g_raster &lt;- density.ppp(bike_theft_ppp_jitter, sigma = 400, edge = T) %&gt;% raster() # Plot the resulting raster plot(kde_400g_raster) We now have a standalone raster we can use with a) any (analysis-oriented) functions in the raster() library (more on this next week) and b) our visualisation libraries, including tmap. Before we go ahead, one issue we will face - although it is not clear in the plot above - is that our resulting raster does not have a Coordinate Reference System. Without a CRS, as we should know by now, that we’ll have issues with both any analysis or visualisation that we’d like to do with our raster, particularly if we use other datasets with our raster. A bit like using the st_crs in sf, we can use the crs() function within the raster library to check our kde_400g_raster CRS. Check the CRS of our kde_400g_raster: # Check the CRS of the `kde_400g_raster` crs(kde_400g_raster) ## CRS arguments: NA You should see an NA appear within our CRS arguments - the kde_400g_raster does not have a CRS, so we’ll need to set the CRS. We can use the same crs() function to set a raster’s CRS - unlike sf’s st_crs() function though, we need to provide our CRS as “a character string describing a projection and datum in the PROJ.4 format” (raster documentation), rather than only the EPSG code. Finding your PROJ4 string You’re likely to have missed this link in our previous practical/lecture on CRSs, but if you need to find the Proj4 string for a CRS, as in our case here, https://spatialreference.org is your one-stop shop for this information. It not only provides the Proj4 string, but also lots of other ways of defining a CRS that you might use in your code. Here is the direct link to its webpage for British National Grid (EPSG:27700): https://spatialreference.org/ref/epsg/osgb-1936-british-national-grid/ Set the CRS of our kde_400g_raster using the Proj4 string from https://spatialreference.org and check the resulting raster CRS: # Set the CRS of the `kde_400g_raster` to BNG crs(kde_400g_raster) &lt;- &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs &quot; # Check the CRS of the `kde_400g_raster` crs(kde_400g_raster) ## CRS arguments: ## +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 ## +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs ## +towgs84=446.448,-125.157,542.060,0.1502,0.2470,0.8421,-20.4894 Great, you should now see your raster has the correct CRS arguments! It’s certainly not as easy to read as “British National Grid” or “27700”, but if you deal within BNG and R for long enough, you’ll probably be able to write out the Proj4 string from memory ;) . Mapping our KDE raster using tmap Now we have our raster ready to map - let’s map it using tmap. We now can introduce a new function from the tmap library: tm_raster() that allows us to map rasters using tmap. We use the same “Grammar of Graphics” approach that we’ve used with our vector data and will first provide tmap with our kde_400g_raster as a tm_shape(). We then add the tm_raster() function, with the required and optional parameters we want to include. In this case, we’ll start off with providing the tm_raster() function with the data variable we’d like to map, plus a palette to use to map our data. Map our kde_400g_raster using tmap: # Map our kde_400g_raster using tmap tm_shape(kde_400g_raster) + tm_raster(&quot;layer&quot;, palette = &quot;viridis&quot;) With our kde_400g_raster using tmap, we can go ahead and customise our map as we would do with our vector mapping - for example, changing the legend tile, adding a title, north arrow etc. Again, I highly recommend exploring the tm_raster() Help documentation to see the different parameters you can provide within the function to change its visualisation with tmap. This would be a completely acceptable way to map our kde_400g_raster - but we’ve been there, done that before, so why not try something different?! Mapping our KDE raster with a basemap and change of opacity So far in our mapping and visualisation efforts, we’ve created static plots with several layers or used tmaps interactive mode to interrogate our data with a basemap. In our current case using a KDE, wouldn’t it be great if we could mix these two approaches and layer our kde_400g_raster over a basemap to identify statically (i.e. for use in a report or paper), where are clusters are? Well, of course, once again thanks to R and its awesome community base, there’s a library and function for that! We can use the rosm library to generate a static raster “tile” from OpenStreetMap that we can use as our basemap for our KDE. Within the rosm library, we use the osm.raster() function that generates a raster image of OpenStreetMap for the Area Of Interest (provided either as a Bounding Box or a Spatial Object, i.e. our raster) provided. Generate a raster of OpenStreetMap that covers the bounding box of our kde_400g_raster: # Generate basemap for our kde raster, crop to its bounding box osm_basemap &lt;- osm.raster(kde_400g_raster, crop = TRUE) # Plot the resulting basemap, using the tm_rgb to plot the three Red, Green, Blue # layers together We&#39;ll explain a little more about RGB next week! tm_shape(osm_basemap) + tm_rgb() ## Warning: Raster values found that are outside the range [0, 255] We can now see we have a base map for the whole of London that we can layer our kde_400g_raster upon - but as you might see, it needs a little bit of tidying up before we go ahead and use it as a basemap. We can see that we have some whitespace either side of our basemap, which we’d like to remove. To remove this whitespace, we can use the crop() function from the raster() package and we can crop our basemap to the spatial extent of our london_ward_shp spatial dataframe. Crop our OpenStreetMap basemap to our london_ward_shp spatial dataframe: # Crop our osm_basemap to the extent of our london_ward_shp london_osm_basemap &lt;- crop(osm_basemap, london_ward_shp) To crop our basemap to the precise shape of our london_ward_shp spatial dataframe, we’d need to look into generating something called a mask - we’ll not look at this week, but hopefully have time to quickly look at the process in Week 9! We should now have our OpenStreetMap basemap cropped to the right extent. But if we look back at our kde_400g_raster map, we can see that in its current format, we won’t exactly see much of London beneath our KDE: ## Warning: Raster values found that are outside the range [0, 255] We can do two things to improve our visualisation here. We can change the opacity (alpha) of our kde_400g_raster to make the OSM basemap more visible through our map. We can remove the lowest values (&lt;0.0001) from our kde_400g_raster to show ony the areas of higher density (i.e. the clusters) within our kde_400g_raster. We’ll do both in one chunk of code and then go ahead and create our final map. We use the base R approach here of selecting all rows with values less than 0.0001 and reclassing them as NAs. We obviously cannot use dplyr to do this because our data is a raster and not a dataframe! There is a reclassify() function within the raster library, but this is a much more complicated function than we need right now - we might have chance to look at it next week instead! Reclass our low values to NA within our kde_400g_raster and set its opacity to for mapping on our OSM basemap: # Reclass our low raster values to NA using the base R approach kde_400g_raster[kde_400g_raster &lt; 1e-04] &lt;- NA # Layer two rasters together for final visualisation tm_shape(london_osm_basemap) + tm_rgb() + tm_shape(kde_400g_raster) + tm_raster(&quot;layer&quot;, palette = &quot;viridis&quot;, alpha = 0.5) ## Warning: Raster values found that are outside the range [0, 255] It looks like we have a map showing the kernel density estimation of bike theft in London! Great work! We’re now one step closer to visually investigating our original hypothesis - that bike theft primarily occurs near tube and train stations. To help with our analysis, we could go ahead and change the tmap_mode to the view mode and use the basemap to visually investigate the distribution of bike theft and its relation to train stations. Alternatively, we could make a static map that overlays our london_stations_osm spatial dataframe as part of our map and compare their location against our visible clusters, that we could use for visual analysis and confirmation of our hypothesis. Fortunately enough, this is exactly the task set for you in this week’s assignment! Mapping and comparing our bike theft KDE and train and tube station locations in London For this week’s assignment, I’d like you to produce a finished KDE map for bike theft in London that also shows the location of train stations relative to our KDE clusters. You can adjust the bandwidth and kernel settings if you’d like - but I’d like you to create a final map that contains this additional data layer as well our expected mapping conventions, plus a title and a renamed legend title. Also think about adding your tm_credits() box as well - we’ve got a whole basemap that uses OpenStreetMap aswell as our stations so it’s a priority that we attribute this dataset accordingly! To export your map as a PNG, simply use the same code from previous weeks. I’d like you to then upload your PNG within the bike-thefts folder for your relevant seminar - you’ll see I’ve created a specific KDE folder for this PNG. You don’t need to write anything alongside your PNG - but just have a look at our datasets and decide whether or not our hypothesis is confirmed, and how confident you are in this assessment. Kernel Density Estimation is one of the most well-used PPA techniques, and as you’ll see from your final map, a good way of visualising the general distribution of points against another spatial feature or phenomena. Understanding how to use KDE and make great KDE maps is therefore the biggest priority coming out of this week’s workshop. Alternative KDE mapping using the adehabitatHR library One alternative method to creating a KDE within R - outside of the spatstat library - that Q-Steppers on this course will have come across is the use of the adehabitatHR library. This library is actually a package that is aimed at estimating the “Home Range Habitat” of animals, primarly for ecologists and those that study wildlife. It utilises theory from PPA to create Kernel Density Estimations of animal habitats, primarily using points generated from GPS or VHF collars. As a result, for us Geographers and spatially-enabled thinkers, it has relatively limited utility for more general PPA, i.e. it does not contain all the other PPA techniques we’ve come across and will come across today. But, what it does have is a really useful function that enables us to create “thresholds” of our KDE in terms of “50% of bike theft occurs within this area”. For example, using the adehabitatHR library, we can create a map that looks like this: ## Warning in kernelUD(bike_theft_2019_sp): xy should contain only one column (the id of the animals) ## id ignored That details where 25%, 50% and 75% of all bike theft within London in 2019 occurred (according to the KDE). For those of you not on the Q-Step course (or those of you who are but have forgotten where to access a tutorial on this), Prof James Cheshire has a very simple and straightforward practical here that takes you through the steps to creating the above map. To get the code to work, you’ll need to: Install and load the sp and adehabitatHR libraries for your code to work (you do not need the other libraries used within the pracitcal). Coerce your bike_theft_2019 sf spatial dataframe to an sp object type (the adehabitatHR library does not accept sf objects at the moment), using the following code: bike_theft_2019_sp &lt;- as_Spatial(bike_theft_2019) before using the kernelUD() function shown in the practical. And now you have two ways of visualising point densities using Kernel Density Estimation. You could, if you wanted (this really is optional!), create a second KDE map using this approach but mapping the location of train and tube stations with our thresholds instead of the bike theft data as above. If you do create this map, feel free to upload it to the folder with your assignment map. If you haven’t already, go take and break before continuing on with this workshop! Distance-Based Methods We’ve spent a good amount of time looking at using density-based methods to a) quantify whether our point data is randomly distributed or not and b) visualise and identify high and low areas of density, showing if our data is clustered and where. This is because, as you’ll see, the KDE is the most effective way of visualising clusters within our point data. Overall, within PPA, (kernel) density estimation is certainly a prominent technique, but as we outlined in our lecture, we also have distance-based methods that allow us to quantify the 2nd order properties of our data, i.e. the influence the location of these points have on one another. Distance-based measures analyze the spatial distribution of points using distances between point pairs, with the majority using Euclidean distance, to determine quantitatively whether our data is, again, randomly distributed or shows sign of clustering or dispersion. These methods are a more rigorous alternative to using the Quadrat Analysis approach, and enables us to assess clustering within our point data at both a global and local scale. Global Clustering: Average Nearest Neighbour Average Nearest Neighbour (ANN) is the average distance between all points within a dataset and their invidividual nearest point (known as Nearest-Neighbour Distance (NND)). ANN is used as a global indicator to measure the overall pattern of a point set (Clark &amp; Evans, 1954). The ANN of a given point collection can be compared with the expected ANN from points following complete spatial randomness (CSR) to test whether our point data is clustered or dispersed (Yuan et al, 2020). Relations between different point patterns and mean nearest neighbor distance (NND).. Source: Yuan et al, 2020. The approach is similar to that of the Quadrat Analysis simulation we saw above, but by using distance rather than density grouped to arbitrary quadrats, ANN is likely to be a more robust quantification of our point distribution. We can calculate the ANN for our dataset by using the nndist() function from the spatstat library. Calculate the average nearest neighbour for our bike theft data: # Calculate the average distance to nearest neighbour We calculate the nndist # (NND) for each point, then take the mean for ANN mean(nndist(bike_theft_ppp_jitter, k = 1)) ## [1] 61.39365 We can see that the average nearest neighbour for all points is 61.3 metres (1dp). To understand whether our dataset is clustered or dispersed, we now need to run a Monte Carlo simulation of running the ANN test for multiple (think hundreds) of Poisson distributions of our 18,690 bike thefts within our London ward in order to generate a graph as above. Then, similar to our Global Moran’s I calculation in Week 6, we would compare our mean to that of all of the means generated to determine whether our dataset is clustered or dispersed. The code and computing requirements to complete this analysis is quite complex, therefore, we look instead to different approach, which is to plot the ANN values for different order neighbours (i.e. first closest point, second closest point etc), to get an insight into the spatial ordering of all our points relative to one another. For point patterns that are highly clustered, we would expect that the average distances between points to be very short. However, this is based on the important assumption that the point pattern is stationary throughout the study area. Further to this, the size and shape of the study area also have a very strong effect on this metric. Calculate the average nearest neighbour to the \\(k\\) nearest neighbours for our bike theft data: # We can calculate the ANN for up to 100 neighbours using the apply function and # providing k with a range as follows: bike_theft_ann &lt;- apply(nndist(bike_theft_ppp_jitter, k = 1:100), 2, FUN = mean) # plot the results of k ANN plot(bike_theft_ann ~ seq(1:100)) In our case, the plot does not reveal anything interesting in particular except that higher order points seem to be slightly closer than lower order points. Overall, the ANN is a good approach to conduct statistical tests on large datasets (if we were to run the Monte Carlo simulation explore above), but visually it does not tell us a huge amount about our dataset! Local Clustering: Ripley’s K function One way of getting around the limitations of both the ANN and Quadrat Analysis is to use Ripley’s K function. Ripley’s K function looks at the distance between a point and ‘all distances’ to other points and automatically compare this to a Poisson-distribution point pattern (without the need to add in Monte Carlo simulation code as above). Ripley’s K function essentially summarises the distance between points for all distances using radial distance bands. The calculation is relatively straightforward: For point event A, count the number of points inside a buffer (radius) of a certain size. Then count the number of points inside a slightly larger buffer (radius). Repeat this for every point event in the dataset. Compute the average number of points in each buffer (radius) and divide this to the overall point density. Repeat this using points drawn from a Poisson random model for the same set of buffers. Compare the observed distribution with the distribution with the Poisson distribution. We can conduct a Ripley’s K test on our data very simply with the spatstat package using the Kest() function. Overdoing it with Ripley’s K function Be careful with running Ripley’s K on large datasets as the function is essentially a series of nested loops, meaning that calculation time will increase exponentially with an increasing number of points. Run a Ripley’s K function on our bike theft data using the Kest() function: # calculate Ripley&#39;s K for our bicycle theft locations, maximum radius of 1 # kilometre plot(Kest(bike_theft_ppp_jitter, correction = &quot;border&quot;, rmax = 5000)) The Kpois(r) line shows the theoretical value of K for each distance radius (r) under a Poisson assumption of Complete Spatial Randomness. When our observed/calculated K values are greater than the expected K, this indicates clustering of points at a given distance band. In our dataset, we can see our observed distribution exceeds the Poisson distribution across our distance bands - therefore our data is more clustered than what would be expected in a random distribution across our various local scales. There are several limitations to Ripley’s K - in the same fashion as the Average Nearest Neighbour Analysis- it assumes a stationary underlying point process. From our previous analysis and visualisation of our bike theft data, we know that this is unlikely to be the case, with the prevalence of bike theft influenced by a multitude of factors that they themselves vary over space. Using Point Pattern Analysis for Research Point Pattern Analysis comes in many shapes and forms. You now have been introduced to some of the most well-known techniques such as Kernel Density Estimation and Ripley’s K to analyse point patterns and understand our key question: is our data clustered, randomly distributed or dispersed? We can use a combination of the techniques deployed above to provide a statistical answer to this question, whilst also visualsing the location of these clusters using Kernel Density Estimation. Keep in mind that there is not really ‘one method’ to use within PPA, which methods are suitable for your research problem depend on the questions you want answered as much as they depend on the underlying point distribution. In our case, the use of KDE - and the resulting map you’ll have produced for your assignment - is the closest we can get in terms of using PPA to answer our main research hypothesis: that bike theft primarily occurs near tube and train stations. If we reflect on our KDE map versus our geometric operations / spatial query approach of last week, which do you think has been most successful in helping us answer our research question? Do you think it would be worthwhile using a mixture of approaches to investigate our research question? As you’ll find, if you move forward with the analysis of point dattern in your coursework or dissertation research, it can be quite difficult to obtain an “absolute” answer to our question - and instead we can only do what we can in terms of quantifying and visualising our data and their relationships to try to provide a “best answer” to our problem. There are also plenty of other PPA techniques that we have not covered in our workshop this week - but I highly recommend you look at “An Introduction to R for Spatial Analysis &amp; Mapping” by Brundson and Comber (2015) if you’d like to find alternatives. Extension: Point Pattern Analysis in R with dbscan The use of our techniques above are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us precisely where in our area of interest the clusters are occurring - even with KDE, there are obvious limitations to the spatial resolution of our clusters. One popular technique for discovering precise clusters in space (be this physical space or variable space) is an algorithm known as DB-SCAN, a density based algorithm. For the complete overview of the DBSCAN algorithm, you can read the original paper by Ester et al. (1996) or consult the wikipedia page. Whilst DBSCAN is a relatively old algorithm, in recent years, there has been a substantial resurgence in its use within data science and specifically, within spatial analysis. You can read an excellent “Call to Arms” paper that backs the DBSCAN algorithm here. Within spatial data science, DB-Scan is being used within a lot of urban analysis, including delineating urban areas through the use of point data such as this paper by Arribas-Bel et al in 2019 at the University of Liverpool. For our extension this week, we’ll look at how we can use DB-Scan to detect clusters within our bike theft dataset - and then how we can use the clusters to further answer our original research question/hypothesis: . What is DBSCAN? DBSCAN is an algorithm that searches our dataset to create clusters of points, using two inputs: The minimum number of points, MinPts, that should be considered as a cluster The distance, or epsilon, within with the algorithm should search. Clusters are identified from a set of core points, where there is both a minimum number of points within the distance search, alongside border points, where a point is directly reachable from a core point, but does not contain the set minimum number of points within its radius. Any points that are not reachable from any other point are outliers or noise. Both parameters need to be finely tuned, typically requiring manual experimentation in both cases before an appropriate value can be selected. The following short video on YouTube explains the algorithm really effectively: Original Video In summary, across a set of points, DBSCAN will group together points that are close to each other based on a distance measurement and a minimum number of points. It also marks as outliers the points that are in low-density regions. The algorithm can be used to find associations and structures in data that are hard to find through visual observation alone, but that can be relevant and useful to find patterns and predict trends. However, DBSCAN will only work well if you are able to successfully define the distance and minimum points parameters and your clusters do not vary considerably in density. We have two libraries we can use to complete a DBSCAN analysis, the dbscan library and fpc. In our case today, we’ll use the newer dbscan package and its dbscan function. According to the creators of this package, “the implementation is significantly faster and can work with larger data sets then dbscan in fpc.”. It is however worth knowing that fpc was one of the first libraries to include DBSCAN analysis within R, so you may see others using it. Make sure you have installed and loaded the dbscan library before completing this analysis. Conducting a DBSCAN analysis To conduct a DBSCAN analysis using the dbscan library, we use the dbscan() function. It’s an incredibly simple function as it only takes 3 parameters: our point dataset, the epsilon and the minimum points. For our analysis, we’ll set our epsilon to the 200m and then set our minimum cluster to 20. The function however will only take a data matrix or dataframe of points - not a spatial dataframe, as we have with our original bike_theft_2019 spatial dataframe variable. Luckily, previously in our earlier analysis, we already created a matrix - bike_theft_xy- when we needed to supply coordinates to the ppp() function in spatstat. We can therefore easliy proceed with our DBSCAN analysis. Run a DBSCAN analysis on our bike theft data: # Run dbscan&#39;s DB-Scan function on our bike theft matrix, with eps and minPts set # as follows We add dbscan:: prior to our dbscan function to ensure R uses the # dbscan function from the dbscan library bktheft_dbscan &lt;- dbscan::dbscan(bike_theft_xy, eps = 200, minPts = 20) If you investigate the result from the dbscan function, you’ll see that it is a “list” of three objects: a vector detailing the cluster for each of our bike theft observations / events, and two double objects that simply contain the eps and minPts parameter values used. We can go ahead and quickly plot our DBSCAN result from this list using the R base plotting as follows. Plot our DBSCAN analysis: # Plot our bike theft points, styling them by the associated cluster in the # bktheft_dbscan outpout plot(bike_theft_xy, col = bktheft_dbscan$cluster) # Add our london_ward_shp file plot(london_ward_shp$geometry, add = T) These initial results are super ineresting. We can see some very defined clusters around the outskirts of central London - and of course, a few significantly large clusters within central London. Here we run into our main issue with DBSCAN, as outlined above, that it will not work too well if our clusters vary considerably in density. It’s evident just from this initial run, that the clustering we see in outer vs. central London has different density properties: to account for the higher occurence of bike theft and thus higher density, we theoretically need to have a more sensitive epsilon measure to enable more refined mapping of the clusters in this area. However, this will then create more separation in our otherwise well-defined outer London clusters. For example, below is a plot of a DBSCAN analysis run at a 100m: Therefore, to obtain a more precise analysis for central London, it would be worth seperating this bike theft data out and re-running DBSCAN at a more local scale, and with finer resolution eps and minPts parameter values. For now, we’ll continue with our 200m DBSCAN output. Working with the DBSCAN output As stated above, the DBSCAN output contains three objects, including a vector detailing the cluster for each of our bike theft observations / events. At the moment, we’re visualising our clusters by mapping our points from the bike_theft_2019 spatial dataframe, but colouring them by their related cluster number. To be able to work with our DBSCAN output effectively - and for example, plot the clusters as individual polygons - we first need to add our cluster groups back to into our original point dataset (the bike_theft_2019 spatial dataframe). To add the cluster groups to our point data, we can simply use the mutate function from the dplyr library - as you should know by now, computers are very organised and not random, therefore we can trust that the computer will use the same order to read in the points and therefore join the correct cluster to the correct point. Add the associated DBSCAN cluster number to each of the bike_theft_2019 observations: # Add the cluster number column to our monduli points data frame, store as new # variable bike_theft_2019 &lt;- bike_theft_2019 %&gt;% mutate(dbcluster = bktheft_dbscan$cluster) Now we have each of our bike theft points in London associated with a specific cluster, we can generate a polygon that represents these clusters in space, as we can sort of see in our above plot. To do so, we will utilise a geometric operation from last week we did not use: the st_convex_hull() function. If you look at your sf cheatsheet, you’ll see that the st_convex_hull() function “creates geometry that represents the minimum convex geometry of x” (sf cheatsheet). I.e. it can be used to create a polygon that represents the minimum coverage of our individual clusters - as we’ll see below. As a result, we can use this function to create a polygon that represents the geometry of all points within each cluster - if we provide the function with each cluster and its associated points - which can then ultimately create a final spatial dataframe that contains polygons for all clusters. To enable this, we’ll use something called a for loop that will make our processing way more efficient - although it might seem quite complex at the start! A for loop simply repeats the code contained within it - for a specific value, usually contained within a index. Don’t worry if you do not understand this or our explanations below - we have an optional tutorial for you to look at over Easter which goes into much finer detail in explanation of how a for loop works if this becomes of interest to you. For loops enable more efficient programming in R (and other programming languages) - but luckily in R, we have functions such as lapply() that allow us to do some of the basic tasks we use for loops in other programming languages, easily and without having to learn how to use a for loop. Below, we use a for loop to: Filter our bike_theft_2019 spatial dataframe by each cluster into a subset For each subset, we union these points into a single set of geometry observations We then calculate the convex hull of that single set of geometry observations This creates a polygon which we extract and store its geometry into a list We then have final list containing all the geometries of our cluster polygons We convert this list of geometries into a spatial dataframe, referenced to British National Grid To make our for loop work, we utilise both indexing and selection techniques that we came across partly in Week 5 to make sure our for loop iterates over each cluster in our dataset and then stores this within our geometry list. Let’s take a look: Run our for loop to generate a polygon dataset that represents our bike theft clusters: # First we create an empty list to store the resulting convex hull geometries Set # the length of this list to the total number of clusters found geometry_list &lt;- vector(mode = &quot;list&quot;, length = max(bike_theft_2019$dbcluster)) # Create a counter, starting it at 0, our first cluster index counter &lt;- 0 # Begin for loop, iterating across each clusters, cluster_index starts at 0, goes # to 101 (our total number of clusters) for (cluster_index in seq(0, max(bike_theft_2019$dbcluster))) { # filter our entire bike_theft_2019 sdf by the cluster index returns only points # for *that* cluster biketheft_cluster_subset &lt;- filter(bike_theft_2019, dbcluster == cluster_index) # for these points, first union them, then calculate the convex hull cluster_polygon &lt;- biketheft_cluster_subset %&gt;% st_union %&gt;% st_convex_hull() # at this point, you could export this single cluster polygon into a single # dataset if you wanted - but we&#39;d prefer not to have 101 shapefiles to then # union! instead, we&#39;ll add the geometry of the polygon into our list. store # these geometry of this polygon into its position within the list geometry_list[counter] &lt;- (cluster_polygon) # add one to the counter, to move to the next cluster and the next position # within the list counter &lt;- counter + 1 } # Set our geometry list to a multi-polygon bike_theft_clusters &lt;- st_sfc(geometry_list, crs = 27700) And that is our for loop complete - if you didn’t quite understand it, do not worry as this is much more advanced programming than you’re expected to know in Geocomputation! There will be an optional tutorial released at the end of term that will go into more detail about the for loop construction - and if you take Mining Social and Geographic Datasets next year, you’re very likely to come across building for loops there! Mapping DBSCAN clusters We now have a polygon spatial dataframe, bike_theft_clusters, that show the general location and distribution of bike theft clusters in London. Let’s go ahead and investigate them - we will use tmap to interactively look at their coverage. I’d highly recoomend swapping your basemap from the Esri canvas to OpenStreetMap - and zoom in!: Map our bike theft clusters: # Set tmap to view mode tmap_mode(&quot;view&quot;) # Map our bike theft clusters and their borders tm_shape(bike_theft_clusters) + tm_borders() # Note, our interactive map will not load here to save on webpage processing # requirements! What do you think of the clusters? What do they tell you about the distribution of bike theft in London? Let’s go ahead and add our london_stations_osm dataset to our map - this will help highlight the location of the stations relative to our bike thefts. Map our bike theft clusters and stations: # Set tmap to view mode tmap_mode(&quot;view&quot;) # Map our bike theft clusters and their borders tm_shape(bike_theft_clusters) + tm_borders() + tm_shape(london_stations_osm) + tm_dots() # Note, our interactive map will not load here to save on webpage processing # requirements! So what does our data show? What can we conclude from our current analysis? What steps could we next complete with our bike theft analysis? These are all questions I’d like you to think about in preparation for our seminar in Week 10 - but essentially, as you can see, investigation a pheonomena like theft, in relation to another geographical feature, i.e. stations, can be quite open-ended and we can utilise a range of techniques to try to investigate and “proove” a relationship between our variables of interest. There are many more calculations we could do with our clusters and station data - I’ll be interested to hear if you think of any in our seminar in Week 10. Overall, I think the results of our analysis using DBSCAN present some really interesting insights into bike theft relative to train stations - particularly if you know anything about cycling in London, and typical areas of cycling (or rather road cycling) that certainly are present in the south-west area (e.g. Richmond Park etc.). Ultimately, there will be a lot of factors that will influence bike theft within the city and its wider metropolitan area - but through both our geometric and point pattern analysis, there are certainly some stations I would not be keen to leave my bike (looking at you Hampton Wick - and of course, the central city area!). These insights can be used to help local planning organisations and communities think about improving bike storage at these various locations - as not having safe and secure areas to lock a bike will certainly put people off from cycling to a train station and other areas where clusters of bike theft is present. Here, we’ve shown how we can use DBSCAN to create clusters around point data - and how to then extract these clusters as polygons for use within mapping (and further analysis if you would like). There are many applications of this workflow beyond mapping crime clusters - you’re welcome to look through a tutorial I wrote on using DBSCAN for mapping settlement outlines for my MSc course in Autumn 2020 that provides a similar application of DBSCAN as the Arribas-Bel et al (2020) paper highlighted above - altough you won’t be able to access the videos. Recap - Analysing Spatial Patterns III: Point Pattern Analysis This week, we’ve looked at Point Pattern Analysis and how it can be used to analyse spatial patterns within point data. Primarily, as geographers and spatially-enabled thinkers, we are keen to understand our point data’s distribution and understand whether are our points clustered, randomly distributed (i.e. display complete spatial randomness), uniform or dispersed. We’ve looked at various techniques that enable us to statistically and visually assess our data’s distribution and understand whether our data is randomly distributed or not. Primarily, our main focus is using Kernel Density Estimation as a technique for the visual display and analysis of our data and its respective areas of high and low density. However, as shown in our Extension, we can also use DBSCAN to create precise cluster outlines that can be also used for visual analysis - but also can be further used for quantitative analysis, via geometric operations. We’ve also taken our bike theft analysis as far as we want to at the moment - you’ll be happy to know we’re moving onto new datasets next week - but ultimately, there is no “one-way” to analyse spatial data. Whether using PPA or geometric analysis, such as a Point-in-Polygon count, we’ll face limitations in our methods, introducing a certain level of uncertainty in our findings at each stage. However, as you become more familiar with spatial analysis (and read more papers on how others investigate specific pheonomena!), you’ll be able to decide which approach suits the questions you ultimately want to answer. Learning Objectives You should now hopefully be able to: Explain the different approaches to detecting clusters in Point-Pattern Analysis Run a Kernel Density Estimation and explain the outputs of a KDE confidently Run a Ripley’s K function and compare to a Poisson distribution Add a basemap within the tmap environment Map a raster dataset within the tmap environment Extension: run a DBSCAN analysis and interact with its outputs Extension: Identify a for loop within a chunk of code Acknowledgements Part of this page is adapted from GEOG0114: Principles of Spatial Analysis: Point Pattern Analysis by Dr Joanna Wilkin (this Workbook’s author) and Dr Justin Van Dijk at UCL, Point Pattern Analysis by Dr Manuel Gimond at Colby College, CASA005: Point Pattern Analysis by Dr Andrew MacLachlan at CASA, UCL and Crime Mapping in R: Studying Point Patterns by Juanjo Medina and Reka Solymosi. Significant updates to code and explanations have been made in this practical. The datasets used in this workshop (and resulting maps): © OpenStreetMap contributors Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). "],["rasters-zonal-statistics-and-interpolation.html", "9 Rasters, Zonal Statistics and Interpolation", " 9 Rasters, Zonal Statistics and Interpolation Welcome to Week 9 in Geocomputation! This week, we’ll be covering two topics: 1) Raster data and 2) Interpolation. So far, the majority of our module has focused on the use of vector data and table data (that we’ve then joined to vector data). This week, we switch it up by focusing primarily on raster data and its analysis. As you saw last week, our analysis of point data using the Kernel Density Estimation created a raster dataset that we needed to process for further analysis and visualisation. This week’s focus on Interpolation will also yield a raster dataset from our analysis of point data - therefore we’ll start this week with a focus on raster data and its applications, including the analysis of several raster datasets together using map algebra. We’ll then see how we can use different interpolation methods to generate raster data from point data. These techniques are split into two categories, deterministic and geostatistical, and we will look to make sure we understand the difference between the two. Within the Extension, we’ll take a quick look at satellite imagery and how and why we use the tm_rgb() function to map raster datasets that have multiple bands (rather than the tm_raster() function), including the OpenStreetMap basemap from last week. Week 9 in Geocomp Video on Stream This week’s content introduces you to raster data, map algebra and interpolation. I told you much of spatial analysis seems like spatial maths! We have three areas of work to focus on: Understanding raster data and map algebra Applying different interpolation techniques in R using the raster library Extension: Using raster and vector data together through zonal statistics This week’s content is split into 4 parts: Workshop Housekeeping (10 minutes) Raster Data (60 minutes) Interpolation: Theory and Techniques (45 minutes) Interpolation: Application in R (60 minutes) Extension: Single-Value Rasters v. Multi-Band Imagery (5 minutes) This week, we have 2 short lectures and 2 assignments within this week’s main workshop content. Learning Objectives By the end of this week, you should be able to: Use, analyse and visualise raster data in R confidently. Utilise map algebra to analyse two or more raster datasets together. Utilise vector and raster data together using zonal statistics. Explain what interpolation is and the different techniques we can use. Implement different geostatistical techniques in R. Utilise vector and raster data together using zonal statistics. After first looking at population change in London using raster data, we will then look at generating pollution maps in London from individual point readings taken from air quality monitoring sites across London. To complete this analysis, we’ll be using several new datasets: Population rasters for England: Raster datasets containing estimated population counts for England in 2001 and 2011 at a spatial resolution of 1km. NO2 Readings across London: A (csv) dataset contain readings of NO2 for individual air quality monitoring sites in London. We’ll also use our London Wards (2018) adminstrative boundaries dataset at various points within both practicals. Workshop Housekeeping Let’s get ourselves ready to start our lecture and practical content by first downloading the relevant data and loading this within our script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk9-pollution-raster-analysis.r. At the top of your script, add the following metdata (substitute accordingly): # Analysing population change and pollution in London Script started March 2021 # NAME Dependencies (aka libraries) Now we’ll install the libraries we need for this week. In addition to those libraries you should now be familiar with, we will need to install and use: rgdal, preferably version 1.4-8: for under-the-hood spatial data management rgeos, preferably version 0.5-2: for more under-the-hood spatial data management gstat: to complete our various interpolation techniques opendata: to download our pollution data directly from London Air We’ll also be using spatstat as an alternative for gstat for those of you who have issues running this code. To download the specific versions of rgdal and rgeos, first install the package devtools (install.packages(\"devtools\")), then use the following code: install_version(\"rgdal\", version=\"1.4-8\") install_version(\"rgeos\", version=\"0.5-2\") install_version(\"gstat\", version=\"2.0-6\") This should install the correct version of the packages you’ll need for today. You can double-check this by looking inthe Packages window after installation. If you already have these packages installed (which it is quite likely you will) and they are a newer version than the version listed above, you may need to uninstall these packages using the Package window if you want to run the gstat code. However, do wait until you’re at relevant section in the practical to see if this is necessary. Remember to use the install.packages(\"package\") command in your console. Within your script, add the following libraries for loading: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) library(RColorBrewer) library(raster) library(sp) library(rgdal) library(rgeos) library(gstat) library(spatstat) library(openair) Remember to select the lines of code you want to run and press CMD (Mac)/CTRL(Windows) + Enter/Return - we won’t remind you to run each line of code in the remainder of the practical sessions. Datasets for this week 1) Population Data For the first part of this week’s practical material we will be using raster datasets from the Population Change and Geographic Inequalities in the UK, 1971-2011 (PopChange) project. In this ESRC-funded project, researchers from the University of Liverpool created raster population surfaces from publicly available Census data (1971, 1981, 1991, 2001, 2011). These population surfaces are estimates of counts of people, displayed within a regular grid raster of a spatial resolution of 1km. These surfaces can be used “to explore, for example, changes in the demographic profiles of small areas, area deprivation, or country of birth” (PopChange, 2021). To enable this, the researchers have created several categories of rasters, including: Total Population, Population by Age, Population by Country of Birth, Population by Ethnicty etc. This week we will use the Total Population datasets. Downloading Total Population Datasets from PopChange To access data directly from the PopChange website requires a simple registration for log-in, you can then navigate through the datasets and choose those you would like to download. For this week, I’ve gone ahead and downloaded the data for you, which you can access directly from the links below: PopChange Raster File Type Link Population surface GB 2001 - Total Population asc Download Population surface GB 2011 - Total Population asc Download Once downloaded, copy over these files into your data –&gt; raw –&gt; population folder. Note, I went ahead and found the metadata file for these datasets which confirm that “each ASCII GRID is in BNG coordinate system”, therefore we will not need to worry about checking our data’s CRS this week. 2) Pollution Data For the second part of this week’s practical material, we will explore several methods of interpolation by looking at air pollution in London by getting data from the Londonair website. Londonair is the website of the London Air Quality Network (LAQN), and shows air pollution in London and south east England that is provided by the Environmental Research Group of Imperial College London. The data are captured by hundreds of sensors at various continuous monitoring sites in London and the south east of England. The best of it all? The data are publicly available for download - and we can use an R package to directly interact with the data without needing to download it! The openair R package enables us to import our data directly using the importMeta() and importKCL() functions. To understand these functions, I’d recommend looking at the documentation of the openair package so that you get an idea why we use them in our code below! However, there is one issue with this package. Because openair contains C++ code, a compiler is needed (C++ is a compiled language, which was briefly covered in Week 4 - you don’t really need to know all the details about this at this stage in your programming career/for Geocomputation). For Windows, for example, Rtools is needed. Depending on your system and configuration this can sometimes be a hassle and simply not worth the various instructions I’d probably need to write out to help each of you. Fortunately, even though packed with functionality, we will only use openair to download all the air pollution data we are interested in: in case you cannot get openair to work on your computer, I’ve provided a direct download of the dataset instead. However, I’ve gone ahead and provided the code in the relevant section if you’d like to try ti use the library to interact with the data - a small warning is that it took over 10 minutest to download the data required for our practical. Pollution Data Type Link Air pollution in London for 2019 (NO2) csv Download Once downloaded, copy over these files into your data –&gt; raw –&gt; pollution folder (i.e. create a new pollution folder for your dataset!). 3) London Ward Data We’ll also be using our London Ward data (2018), so make sure you haven’t deleted this from your raw folder ;) . Raster Data In the previous weeks, we have predominantly worked with vector data and/or table data that we then join to vector data for analysis. However, depending on the nature of your research problem, you may also encounter raster data. Each of these GIS models has its own advantages and disadvantages, that were briefly explored in Week 2 of our module. A hypothetical raster and a vector model of landuse. Source: Esri, 2019. If you remember, the main difference between vector and raster models is how they are structured. Our vectors are represented by three different types of geometries: points, lines and polygons. We’ve used point data in the form of our stations and bike theft, and polygons in the form of our Ward and Borough boundaries. In comparison, our raster datasets are composed of pixels (or grid cells) - a bit like a photograph. This means that a raster dataset represents a geographic phenomemon by dividing the world into a set of rectangular cells that are laid out in a grid. Each cell holds one value that represents the value of that phenomena at the location, e.g. a population density at that grid cell location. In comparison to vector data, we do not have an attribute table containing fields to analyse. All analysis conducted on a raster dataset therefore is primarily conducted on the cell values of a raster, rather than on the attribute values of the observations contained within our dataset or the precise geometries of our dataset, as we’ve seen in the last two weeks, with our vector data. Probably one of the most common or well-known types of raster data are those that we can derive from remote sensing, including satellite and RADAR/LIDAR imagery that we see used in many environmental modelling applications, such as land use and pollution monitoring. However, over the last few years, raster data has increasingly being used within spatial data science applications. For example, Worldpop and Facebook have created raster-based estimates of population density (and other variables), that you can access openly via their respective links. Beyond their benefits in computational requirements and even, for some geographical phenomena, visualisation capacity and capabilities, a key advantage of raster data is that is relatively straight-forward to standardise data across space (i.e. different countries) and across variables (i.e. different datasets) to enable greater compatibility and easier comparison of datasets than its vector counterparts. We have, for example, seen that we can run into issues quickly even with data on London, as our ward boundaries have changed so frequently even over just the last ten years. This standardisation can occur as raster data has: An origin point from which the grid extends and then a precise number of columns and rows within said dataset; A specifc spatial resolution which refers to the cell size of the raster dataset, e.g. are the grid square 100m x 100m, 1000m x 1000m etc? From these two values, it is possible to calculate the size of our raster (number of columns X spatial resoution by the number of rows X spatial resolution) as well as * snap future rasters (or resample current rasters) to both the spatial extent and the spatial delineation of one raster dataset (i.e. ensure the cells between the rasters will align with one another). This enables us to create rasters that essentially “line up with one another” - and by doing so, we areable to complete specific calculations between our raster datasets known as Map Algebra. What is Map Algebra? Map algebra is exactly what it sounds like - it basically involves doing maths with maps! The key difference is that, within spatial analysis, it only applies to raster data, hence it’s name as either map algebra or raster math. Map algebra is a set-based algebra for manipulating geographic data, coined by Dana Tomlin in the early 1980s. Map algebra uses maths-like operations, including addition, subtraction and multiplication to update raster cell values - depending on the output you’re looking to achieve. Conceptual idea of what is map algebra. Source: GISGeography, 2021. The most common type of map algebra is to apply these operations using a cell-by-cell function. Conceptually, this approach will directly stack rasters on top of one another and complete the mathematical operations that you’ve supplied to the cells that are aligned with each other. These operations might include: Arithmetic operations that use basic mathematical functions like addition, subtraction, multiplication and division. Statistical operations that use statistical operations such as minimum, maximum, average and median. Relational operations, which compare cells using functions such as greater than, smaller than or equal to. Trigonometric operations, which use sine, cosine, tangent, arcsine between two or more raster layers. Exponential and logarithmic operations that use exponent and logarithm functions. But it is also possible to run (some of) these operations at a different scale. Map algebra functions can be applied using for four different approaches: Local: The simplest approach - completing functions on a cell-by-cell basis. Global: Used to apply a bulk change to all cells in a raster using a function, e.g. add 1 to all values in a raster, or calculate the euclidean distance each cell is away from a specifc cell. Focal: Apply a function to a set of neighborhood values to calculate the output for a single cell, e.g. using a moving window, such as kernel. Zonal: Apply a function to a group of cells within a specified zone (zone can be provided as a raster or vector format). The utilisation of these functions can enable many different types of specialised raster analysis, such as recoding or reclassifying indivdual rasters to reduce complexity in their data values, generating the Normalised Difference Vegetation Index for a satellite imagery dataset, or calculating Least Cost Estimate Surfaces to find the most “efficient” path from one cell in a raster to another. Furthermore, using multiple raster datasets, it is possible to combine these data through our “mathemetical overlays”, from the basic mathematical operations mentioned above to more complex modelling, such as prediction using Bayes theorem. The results of these overlays have many applications, including identifying suitable locations for placing a new school or modelling risk to viruses, such as the Zika virus (e.g. Cunze et al, 2019 and Santos &amp; Meneses, 2017 for those of you interested in this application), and, of course, as highlighted above, population density. Our first lecture for this week provides raster data and map algebra. Lecture: Raster Data and Map Algebra Lecture slides | Watch on MS stream We do not have a huge amount of time this week to look into map algebra fully - including the many applications mentioned above. But for those of you interested in environmental modelling or more complex data science specialising in Bayesian modelling, there are many tutorials and videos out there that can explore these in more detail. GIS Geography’s explanation on Map Algebra The majority of the images on our lecture slides today on Map Algebra are taking from GIS Geography’s excellent article on “What is Map Algebra?”. This website is an excellent resource for simple explanations of basic GIS concepts such as Map Algebra and, as we’ll see later on in the workshop, our different interpolation techniques. Alternatively, Manuel Gimond’s Lecture Notes on Map Algebra are also an excellent resource, as usual. To get to grips with the concept of Map Algebra, we will finally conduct the population change analysis I mentioned at the start of the module, using the first set of raster data we downloaded first. Analysing Population Change in London using Map Algebra The first part of our practical this week will look at Map Algebra in action - and some simple raster data process - by looking to analyse population change in London between 2001 and 2011 (i.e. the formative years of your very own childhood!). To do so, we’re going to complete a very simple bit of map algebra - we will substract the values of the 2011 raster dataset from the 2011 raster dataset and then map the resulting values, i.e. population change. One question to think about - and reflect on as we move forward with this practical - is that we already know that small-area counts of people in a variety of population subgroups are publicly released for each Census and via the Mid-Yeat estimates, so why was it necessary to create these raster population surfaces? Before we open up the data in R, try to have a ‘non-spatial sneak peak’ at the .asc file by opening it in a normal text editor, for instance, TextEdit on Mac OS or NotePad on Windows. What you will notice is that the asc file, which is an exchange format, is in very fact a flat plain text file! Reflecting on what we’ve just read about rasters and their format, what do you think the first few lines of the asc file, when opened with a text editor, mean? Loading and Processing Raster Data Let’s get started and take a look at our data - first we need to load it into R (using the raster library) and then we can quickly plot it using the base R plot function. Load our two population rasters and plot using R’s base function: # Load our two raster datasets pop_2001 &lt;- raster(&quot;data/raw/population/5a_ascii_grid2001_Total_Population_UsRsPopA.asc&quot;) pop_2011 &lt;- raster(&quot;data/raw/population/5a_ascii_grid2011_Total_Population_URPopAll.asc&quot;) # Inspect 2001 - this can be a little slow, especially for large raster plot(pop_2001) # Plot 2011 - this can be a little slow, especially for large rasters plot(pop_2011) Note, if your maps have struggled to plot, do not worry - we’re going to go ahead and reduce the size of our rasters ASAP! You may however need to terminate and/or restart R if it has got stuck trying to load your raster maps. You should see that whilst your maps look very similar, the legend certainly shows that the values associated with each cell has grown over the 10 years between 2001 and 2011 - we see our maximum increase from 15,000 people per cell to 20,000 people per cell. Now we could complete some more simple analysis on our raster dataset (e.g. extracting basic descriptive statistics for each), but we’ll move forward with our London-focused analysis instead. Now we have our raster data loaded, we want to reduce it to show only London using our London Ward shapefile. To do so, we will use a combination of two techniques - the crop() function we came across last week - and then using a mask to refine our raster further. If you remember using the crop() function last week on our OSM basemap, it will crop any raster by the overall spatial extent or rather bounding box of the y dataset. As a result, the raster returned will be rectangular (or square) in shape - and not cropped to the precise geometry of the y dataset that we see in the use of the st_intersections() function that we use with vector data. To reduce a raster to the (almost) precise geometry of the y dataset, we need to instead use a mask approach. The reason why I say “almost” is because a mask will only work when using two raster datasets. As a result, we need to turn our y dataset (in our case, the London Ward shapefile) into a raster - a process simply known as “rasterize” or “rasterizing”. This process of rasterizing will turn our polygon dataset into a raster and thus simplify/alter the geometry of our dataset to coerce it into a grid-based dataset: Rasterising a line vector - forcing geometries into a grid. Source: Lovelace et al, 2020. As a result, it will be an “almost” precise geometry. To ensure our resulting raster of our London Ward shapefile matches the spatial delineation (aligns our cells) and resolution (make cells the same size) of our population rasters, instead of separately rasterising (using the rasterise() function) our London Ward shapefile and then masking (using the mask() function) our rasters by the resutling raster, we can combine this into one, still using the rasterise() function but adding the london population rasters into the function and the mask parameter set to True. Let’s go ahead and generate our output and see this code in action. Load our London Ward shapefile and use this to first crop, then mask our population rasters (through rasterising): # Load london ward data as per usual london_ward &lt;- read_sf(&quot;data/raw/boundaries/2018/London_Ward.shp&quot;) # Crop raster to extent greater london lonpop_2001 &lt;- crop(pop_2001, london_ward) lonpop_2011 &lt;- crop(pop_2011, london_ward) # Rasterise London Ward, and mask each pop raster by this new raster using # mask=True parameter lonpop_2001 &lt;- rasterize(london_ward, lonpop_2001, mask = TRUE) lonpop_2011 &lt;- rasterize(london_ward, lonpop_2011, mask = TRUE) # Plot the 2001 London population raster plot(lonpop_2001) # Plot the 2011 London population raster plot(lonpop_2011) You should now have generated two plots for each year - you can quickly flick between the two and see there is evidence of population change between our two datasets. We could go ahead and make a few nicer tmaps of our current datasets, but we’ll save this until we’ve managed to process our population change variable as well. Calculating Population Change Now we have our two London population rasters, we’re now ready to go ahead and calculate population change between our two datasets - and the code in R to do so is incredibly simple: it’s simple subtraction! Subtract our 2001 population raster from our 2011 population raster: # Subtract 2001 population from 2011 population to get population change lonpop_change &lt;- lonpop_2011 - lonpop_2001 # Plot the results plot(lonpop_change) We now have a raster that shows us population change in London - and to our surprise, there are areas in which population has actually declined. We, again, could go ahead and make a few nicer tmaps of our current datasets now, but I’m still not happy with our final dataset. We can utilise some of the focal and zonal functions from our map algebra catalogue to further “enhance” our understanding of population change in London. Analysing Population Change To further analyse our population change raster, we can create a ‘pseduo’ hotspot map of our lonpop_change raster by calculating a smoothed version of our raster using the focal() function. This will enable us to see more clearly where there are areas of high counts (surrounded by areas of high counts) and vice versa - just like our KDE analysis of bike theft last week. Using the focal() function, we generate a raster that summarises the average (mean) value, using the fun= parameter set to mean, of the 9 nearest neighbours for each cell, using a weight matrix defined in our w parameter and set to a matrix (consisting of our cell with 3 rows and 3 columns as neighbours) as you’ll see in the code below. Calculate the smoothed estimation of our lonpop_change raster: # Using a focal statistics (of 9 neighbours) to calculate smoothed raster lonpop_smooth &lt;- focal(lonpop_change, w = matrix(1, 3, 3), fun = mean) # Plot results plot(lonpop_smooth) Our areas of high population growth are now more visible in our dataset. Our areas of population decline are potentially not as stark, but are certainly still visible within our raster. What do you think? Does this communicate population change better than our raw values? We can also look to use zonal functions to better represent our population change by aggregating our data to coarser resolutions. For example, we can reisze our raster’s spatial resolution to contain larger grid cells which will, of course, simplify our data, making larger trends more visible in our data - but of course, may end up obfuscating smaller trends. We can resize our lonpop_change raster by using the aggregate() function and setting the fact= (factor) parameter to the “order” of rescaling we’d like (in our case, 2 times larger both width and height). We then provide the fun= (function) by which to aggregate our data, in this case, we’ll continue to use the mean but we could in fact provide min or max depending on our future applications/analysis of our dataset. Aggregate our lonpop_change raster to a coarse spatial resolution, at an order of 2: # Rescale raster and aggregate based on mean lonpop_change_agg &lt;- aggregate(lonpop_change, fact = 2, fun = mean) # Plot resulting raster plot(lonpop_change_agg) Another very common technique used in raster analysis via map algebra is the use of zonal statistics. As outlined earlier, a zonal statistics operation is one that calculates statistics on cell values of a raster (a value raster) within specific zones that are defined by another dataset. The zones can be provided by both raster and vector data - as a result, zonal statistics are a really useful tool if we need to aggregate data from a raster dataset for use within further analysis that primarily uses vector data, such as when we’re analysing data within administrative boundaries. For example, in our case, we can aggregate the lonpop_change raster to our actual London Ward boundaries, i.e. calculate for each ward in our dataset, the average (or other function) population change, as determined by our raster. We can, of course, use other functions other than the mean - what function you use will simply depend on your application. Esri has a great resource on how Zonal statistics works with other functions and raster data, found here. Aggregate our lonpop_change raster to our London Ward boundaries: # Aggregate to administrative geography Note: the output is a vector that is # forced to a SpatialPolygons object (not sf) london_ward_pop &lt;- raster::extract(lonpop_change, london_ward, fun = mean, sp = TRUE) # Plot via tmap tm_shape(london_ward_pop) + tm_polygons(col = &quot;layer&quot;) We now have a vector dataset that we could go ahead and run many of the analyses that we’ve completed in Week 7, such as a spatial autocorrelation tests, to prove any of the visual analysis claims we might have made in our analysis of our raster above. Furthermore, we can use this data within other analyses we might want to complete - for example, if we are using population change as a specific variable to analyse another dataset that is only available as a vector dataset / at the ward level. Trying to calculate populaton change, particularly across decades as we have done here, is quite difficult with our Census and Mid-Year Estimates given the differences in our Ward boundaries and the impact this has when we try to join datasets from different years that then have different codes that we need to join by attribute. Using raster data, such as these datasets, are a good workaround to these issues, but, of course, with any data processing, will add some level of uncertainty into our datasets. Using Functions from different libraries within the same name Whilst I’ve mentioned this before, I wanted to flag this again, particularly with the introduction of the raster package. As you’ve seen in the previous code, and in other practicals, many libraries (loaded packages) share the same function names. This can be a problem when these packages are loaded in a same R session. For instance extract is not only the name of a function in the raster package, but also the name of functions in the magrittr and tidyr packages. To ensure you are using the function that you think you are using, you can specify the package using the :: approach, as follows: library::function, e.g. tidyr::extract or raster::extract. We have had a quick exploration of raster data and seen specific ways we can process the data (e.g. crop and mask to specific extents) as well as shown how we can use map algebra to process two datasets together. There is of course so much more that you can do with these datasets and mathemetical functions that we just do not have time for in our workshop - but you’ll be able to find specific tutorials on how to utilise map algebra for specific applications online, if it ends being a suitable technique for your own future analysis. Online Tutorials on Map Algebra and Raster Analysis To further your understanding on Map Algebra and general spatial data processing (for both raster and vector data), one resource I can highly recommend you continue to work through is the Geocomputation with R online book by Lovelace et al (2020). Chapters 4 and 5 give you a cohesive introduction to the many functions the sf and raster libraries contain and may proove to be of significant use for both your coursework or your Dissertation analysis. We, of course, cannot cover everything in this course - or else our Workbook would be even longer than it is! Instead, we provide you with examples, as above, in the spirit that you’ll be able to utilise resources such as this online book to add to your analysis code as and when you need! For now, I have a short theory-based assignment I’d like you to complete in time for next Week’s seminar. Calculating the number of people in London underserved by public transport The first assignment this week is a purely theoretical question: How can we use a combination of the techniques we’ve used over the last few weeks to calculate the number of people in London underserved by public transport? To answer the question, I would like you to think of a method using what you’ve learnt above in regards to map algebra and your use of point data in the previous weeks, to think about how we can calculate the number of people who are not within 400m euclidean distance walk of a bus, tube or train station in London. I’ll ask for suggestions in our Seminar in Week 10. This is a prime example of how we can use mathematical raster overlays to complete spatial analysis that by using vector data alone is likely to be incredibly difficult. Interpolation: Theory and Techniques The second half of our workshop this week focuses on interpolation. Spatial interpolation is the prediction of a given phenomenon in unmeasured locations. There are many reasons why we may wish to interpolate point data across a map. It could be because we are trying to predict a variable across space, including in areas where there are little to no data. We might also want to smooth the data across space so that we cannot interpret the results of individuals, but still identify the general trends from the data. This is particularly useful when the data corresponds to individual persons and disclosing their locations is unethical. To predict the values of the cells of our resulting raster, we need to determine how to interpolate between our points, i.e. develop a set of procedures that enable us to calculate predicted values of the variable of interest with confidence - and, of course, repetitively. Within spatial interpolation, two approaches have developed: deterministic and geostatistical. This week’s lecture outlines the difference between the two and explains how the two main techniques associated with each, Inverse Distance Weighted and Kriging, work to create our resulting surfaces. Lecture slides | Watch on MS stream We’ll now put these techniques into action by interpolating our air quality point data into a raster surface to understand further how air pollution varies across London. Interpolation: Application in R Before we get going within interpolating our pollution dataset, let’s first take a look at the distribution of the London Air monitoring sites in London: Locations of the London Air monitoring sites in London (Londonair 2020). What are your thoughts about the distribution of the sites? Do you think they’ll provide enough data for an accurate enough interpolation? Ultimately, monitoring sites and the sensor stations present at them can be expensive to install and run - therefore, identifying the most important places for data collection will somewhat determine their location, alongside trying to create a somewhat even distribution over London. As we can see in the locations of the stations above, there are certainly some areas in London that do not have a station nearby, whilst others (such as central London) where there are many stations available. When using interpolation, the distribution and density of our data points will impact the accuracy of our final raster - and we may end up with a level of uncertainty in the areas where data is more sparse, such as the north-west and the south-east of London. Despite this, we can still create an interpolated surface for our pollutant of interest - we just need to interpret our final raster with acknowledgement of these limitations. For this week’s practical, we’ll go ahead and use the Londonair’s data to study the levels of Nitrogen Dioxide (NO2) in London for 2019. Why study Nitrodgen Dioxide levels? For those of you unfamiliar with atmospheric gases and their relation to pollution, Nitrogen Dioxide is one of the main pollutants we monitor due to its adverse impacts on health. Londonair provide an excellent guide on the different pollutants they monitor, with the following is directly extracted from the guide: Nitrogen dioxide (NO2) is one of a group of gases called nitrogen oxides. Road transport is estimated to be responsible for about 50% of total emissions of nitrogen oxides, which means that nitrogen dioxide levels are highest close to busy roads and in large urban areas. Gas boilers in buildings are also a source of nitrogen oxides. There is good evidence that nitrogen dioxide is harmful to health. The most common outcomes are respiratory symptoms such as shortness of breath and cough. Nitrogen dioxide inflames the lining of the lung and reduces immunity to lung infections such as bronchitis. Studies also suggest that the health effects are more pronounced in people with asthma compared to healthly individuals. In recent years the average level of nitrogen dioxide within London has not fallen as quickly as predicted. This largely appears to be the result of diesel cars creating more nitrogen dioxide than was anticipated. Nitrogen dioxide also reacts with hydrocarbons in the presence of sunlight to create ozone, and contributes to the formation of particles. In addition to NO2, we could also extend our study to other pollutants to get a more cohesive picture of how pollution varies over London. Londonair collects data on the main pollutants monitored in London: carbon monoxide (CO); nitrogen dioxide (NO2); ground level ozone (03); particles (PM10); sulphur dioxide (SO2). We will access data directly from Londonair that contains readings for the various Monitoring Locations shown above. The data will be provided to us either as a “served” database (via the openair library) or via the csv provided for download. Either approach will provide us with a dataframe that contains the coordinates of these Monitoring Locations (i.e. a potential point data) alongside the readings taken for each of these Locations the entirety of 2019. Once we have our data loaded and processed in the right format, we will start interpolating our data using at first two deterministic models: 1) Thiessen Polygons and 2) Inverse Distance Weighting (IDW). Next, we will then look at how we can conduct a Geostatistical interpolation through Ordinary Kriging. Loading and Processing Pollution Data in R Option 1: Import data using openair Let’s get started by importing our data using the functionality provided by the openair package or, alternatively, reading in the data you downloaded above. Remember, you should have installed and loaded the openair package during our housekeeping part to make this work! Import our data from Londonair using the openair: I will leave this code here as our example - but even for my connection, this took over 10 minutes to run. As a result, I’d advise using Option 2 and read in the zipfile.csv. # Option 1: get data using openair package # Import all monitoring sites using the importMeta function sites &lt;- importMeta(source=&#39;kcl&#39;,all=FALSE) # Import pollution data for 2019 for all monitoring sites // this will take 5-10 minutes # If you have a poor internet connection or slow computer, I&#39;d recommend not using this approach - but at least you can see its possible in R! pollution &lt;- importKCL(site=c(sites$code), year=2019, pollutant=&#39;no2&#39;, meta=TRUE) # Filter out NA values for nitrogen dioxide pollution &lt;- filter(pollution,!is.na(no2)) If you do run the above code, you should end up with 995 sites (i.e. we are taking data from 995 individual monitoring stations) - setting our all parameter to FALSE means that only the site code, site name, latitude and longitude and site type are imported. Setting all = TRUE will import all available meta data and provide details (when available) or the individual pollutants measured at each site. For pollution, you’ll see messages that warn us the certain sites do not exist, e.g. ‘XXX_2019 does not exist - ignoring that one’ message. We are essenitally just forcing a download for all possible sites, irrespective of whether the monitoring site was active in 2019. As you’ll see, quite a few of the sites appear to not exist in 2019. If your code is still running after 10-15 minutes, I’d highly recommend terminating the process and simply reading in the data as below. Option 2: Import data from csv As the openair download does take a signinifcant amount of time to download, you are welcome to use the csv I have processed for you - you can download the csv from the 2) Pollution Data section. Import our data directly from the csv within the zip folder you have downloaded: # Option 2: using downloaded csv file # Read in downloaded data As the file is quite large, we will read it directly # from zip pollution &lt;- read_csv(&quot;data/raw/pollution/no2_london_2019.zip&quot;) # pollution dataframe dimensions dim(pollution) ## [1] 1596509 7 Reading in the csv might take a little time - we have 1,596,509 observations with 7 variables - that’s quite a substantial dataset! Let’s take a look at why it’s so large. Look at the first five rows of our dataframe: # Return first five rows of our pollution dataframe head(pollution) ## # A tibble: 6 x 7 ## date no2 site code latitude longitude site_type ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-07-01 12:00:00 14.5 Southwark - Towe… SK8 51.5 -0.0782 Roadside ## 2 2019-07-01 13:00:00 16.1 Southwark - Towe… SK8 51.5 -0.0782 Roadside ## 3 2019-07-01 14:00:00 16.2 Southwark - Towe… SK8 51.5 -0.0782 Roadside ## 4 2019-07-01 15:00:00 21.8 Southwark - Towe… SK8 51.5 -0.0782 Roadside ## 5 2019-07-01 16:00:00 19.7 Southwark - Towe… SK8 51.5 -0.0782 Roadside ## 6 2019-07-01 17:00:00 17.5 Southwark - Towe… SK8 51.5 -0.0782 Roadside Interesting - we can see that in our first five rows we have data for the same site - and if we look at the date field, we can see we have a reading observation for every hour! With 24 hours in the day, 365 days in a year and potentially hundreds of sites, it should therefore be of no surprise that we have such a big csv! In the end, for this practical, we only want to create one raster - so to make our data more useable, we will go ahead and aggregate the data and get the average NO2 value for each monitoring site over 2019. We could, of course, look to return the max or min, or, for example, create monthly averages instead (and create 12 rasters!) - there’s a lot we could do with just this single dataset beyond what we’ll look at today! Use the dplyr library functions to return the mean NO2 value for each monitoring site over 2019. Let’s also make sure that we retain the latitude and longitude of our monitoring sites: # Aggregate data to unique latitude and longitude combinations, remove monitoring # sites without coordinates Summarise the no2 by the mean avg_pollution &lt;- pollution %&gt;% group_by(latitude, longitude) %&gt;% summarise(no2 = mean(no2)) %&gt;% filter(!is.na(latitude | longitude)) # Return the first five rows of our new avg_pollution dataframe head(avg_pollution) ## # A tibble: 6 x 3 ## # Groups: latitude [6] ## latitude longitude no2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 49.8 -7.56 35.6 ## 2 50.4 -4.14 17.5 ## 3 50.7 -1.83 11.5 ## 4 50.8 0.284 15.7 ## 5 50.8 0.181 7.23 ## 6 50.8 0.272 11.4 # Return the histogram of our no2 values hist(avg_pollution$no2) We should now see that we only have our (hopefully unique!) latitude and longitude coordinates and the average NO2 value associated with each. Our histogram also shows us the general distribution of our values - we can see that we have a slight positive skew to our dataset. To use this data within our different interpolation methods, we’ll need to transform our data into a point spatial dataframe using the st_as_sf() function that we’ve come across before. One thing you should notice is that the latitude and longitude are, of course, in WGS84 - therefore, we’ll need to reproject our resulting spatial dataframe into British National Grid. We’ll also make sure that all of our points are within our London Ward extent, using the st_intersection() function from the previous week. Also, as we’re yet to make any pretty maps this week, we’ll go ahead and deploy our proportional symbols map code on our resulting spatial dataframe to see the distribution of our variables spatially. All this code must be getting pretty familiar to you by now! Create a spatial dataframe containing our London monitoring sites and their average NO2 reading - then map the data: # load London Wards for reference map as we&#39;ve already done this, I&#39;ve commented # it out - but if you are running this practical separately to above you might # need to reload it in! london_ward &lt;- &lt;- # read_sf(&#39;data/raw/boundaries/2018/London_Ward.shp&#39;) # Create a point spatial dataframe and project into british national grid (epsg # 27700) pollution_points &lt;- st_as_sf(avg_pollution, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326)[, 1] %&gt;% st_transform(27700) # Ensure all points are within the boundaries of Greater London We could have # directly pipe our output above into this, but now we have all points in one # spatial dataframe as well lonpollution_points &lt;- pollution_points %&gt;% st_intersection(london_ward) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries # Create a proportional symbols map # Ensure tmap is in the plot mode tmap_mode(&quot;plot&quot;) # Plot our London wards in grey tm_shape(london_ward) + tm_polygons(palette = &quot;grey&quot;, border.col = &quot;white&quot;) + # Plot our pollution_points as bubbles, using the NO2 field to determine size tm_shape(lonpollution_points) + tm_bubbles(size = &quot;no2&quot;, col = &quot;mediumorchid&quot;, style = &quot;pretty&quot;, scale = 1, border.col = &quot;white&quot;, title.size = &quot;Average NO2 ug/m3 reading in 2019&quot;) + tm_layout(legend.position = c(&quot;left&quot;, &quot;top&quot;)) + # Add a north arrow tm_compass(type = &quot;arrow&quot;, position = c(&quot;right&quot;, &quot;top&quot;)) + # Add a scale bar tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) + # Add our data statement tm_credits(&quot;Air quality data from London Air&quot;) Our proportional symbols map already tells us a little about our dataset - we can see that NO2 levels are much higher towards the centre of London, although we can see some anomalies in the south-west, for example. But we can also see how and why a smoothed surface of our data could be really useful for further interpretion - and this is where interpolating our data comes in! Thiessen Polygons: Basic Deterministic Approach The first step we can take to interpolate the data across space is to create Thiessen polygons. Thiessen polygons are formed to assign boundaries of the areas closest to each unique point. Creating a set of thiessen polygons or voronois. Source: Esri, 2020. Therefore, for every point in a dataset, it has a corresponding Thiessen polygon. Thiessen versus Voronoi - are they the same thing? The quick answer to this is YES! You’ll see the words thiessen and voronoi being used interchangeably to describe this type of geometry created from point data. In the field of GIS we tend to refer to them as Thiessen polygons, after the American meteorologist who frequented their use. In other fields, particularly mathematics and computer science, they are generally referred to as Voronoi diagrams, in honour of the mathematician Georgy Voronyi. Let’s go ahead and create our thiessen-voronoi polygons. Whilst the spatstat library from last week offers a simple function to create our thiessen-voronois (dirichlet() tesselation function), to use the spatstat library, we’ll first need to convert our lonpollution_points into a ppp spatial object. Whilst this is a completely feasible and valid approach to generating these polygons (and one you’ll see in some of the tutorials linked below), we can actually go ahead and create thiessen polygons within the sf library. You might remember from the sf cheatsheet that there is in fact a function called st_voronoi()- however to use it with our dataset takes a little bit of “fudging” with our code as it does not directly work with a point geometry as we would expect. Luckily, after a quick browse of various help forums and trial and error, and I was able to find a very simple bit of code that enables us to generate our voronois all in sf. The post can be found here. The code creates a simple function called st_voronoi_point() that we can use to generate voronois directly from a point dataset. You do not need to understand the code behind the function (all the code contained in the {} brackets), but simply understand what input (a point spatial dataframe) and output (a voronoi polygon spatial dataframe) it will provide. You need to copy over both the function and the code underneath. Copying the function stores this function in your computer’s memory for this R session and means the function itself can be used time and time again within the same session or script. The first of the two lines of code below the function then “call” this function on our lonpollutions_points spatial dataframe. The second essentially joins the attribute fields of our lonpollutions_points spatial dataframe to our new voronoi spatial dataframe and stores this as a new variable. (The code actually “sets” the geometry of our lonpollutions_points spatial dataframe to that of the lon_points_voronoi spatial dataframe and stores this as a new variable, but the sentence above is a little easier to understand!) We then map our resulting thiessen-voronoi polygons by the NO2 value associated with them. Create the st_voronoi_point() function and then generate and map thiessen-voronoi polygons: # First set of code creates our function # function to get polygon from boundary box st_voronoi_point &lt;- function(points) { ## points must be POINT geometry if (!all(st_geometry_type(points) == &quot;POINT&quot;)) { stop(&quot;Input not POINT geometries&quot;) } g = st_combine(st_geometry(points)) # make multipoint v = st_voronoi(g) v = st_collection_extract(v) return(v[unlist(st_intersects(points, v))]) } # Second set of code calls our function and creates our voronois with attributes # Generate voronois from our point data using our new function lon_points_voronoi = st_voronoi_point(lonpollution_points) # Add attribute data to our voronoins lonpollution_tv = st_set_geometry(lonpollution_points, lon_points_voronoi) # Visualise our voronois by the NO2 value tm_shape(lonpollution_tv) + tm_fill(col = &quot;no2&quot;, palette = &quot;Purples&quot;) We can go ahead tidy this up further by clipping our thiessen polygons to the extent of London. Clip thiessen polygons to London extent: # Generate London outline through st_union london_outline &lt;- london_ward %&gt;% st_union() # Clip our thiessen polygons to our london outline lonpollution_tv &lt;- st_intersection(lonpollution_tv, london_outline) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries # Map the results tm_shape(lonpollution_tv) + tm_fill(col = &quot;no2&quot;, palette = &quot;Purples&quot;) And that’s it! We now have our values “interpolated” using our coarse thiessen polygon approach! We could go ahead and make our map look a lot prettier, adding our usual map conventions and a title - and even changing the colour palette. However, as you can see, our approach is quite coarse. Whilst we, of course, can see areas of high and low pollution, it really does not offer us as much spatial detail as we’d like, particularly when we know there are better methods out there to use. Therefore our time is probably best spent looking at these other methods, than trying to improve the aesthetics of this map! Inverse Distance Weighting: A Deterministic Approach A second deterministic method to interpolate point data is Inverse Distance Weighting (IDW). An IDW is a means of converting point data of numerical values into a continuous surface to visualise how the data may be distributed across space. The technique interpolates point data by using a weighted average of a variable from nearby points to predict the value of that variable for each location. The weighting of the points is determined by their inverse distances drawing on Tobler’s first law of geography that “everything is related to everything else, but near things are more related than distant thing”. The distance weighting is done by a power function: the larger the power coefficient, the stronger the weight of nearby point. The output is most commonly represented as a raster surface. Generating an IDW raster We’ll use the idw() function within the gstat library to conduct an IDW on our lonpollution_points spatial dataframe. Before we can run IDW, we must first generate an empty grid within which to store our data. To do so, we can use either the spsample() function from the sp library or the st_make_grid() function from the sf library. For this once, we will use the sp library instead as the sf version - whilst trialling these different approaches, I haven’t quite found a way to ensure the sf method works. As a result, it is better to provide you with working code at this point (and an updated sf version at a later stage!). We’ll go ahead and create a grid that covers the entirety of our london_outline, which we’ll transform into the sp format using the as() function. We then run the gstat idw() function on an sp version of our lonpollution_points dataset, specificying the cell size. Issues with different versions of R Some of the following tutorial will unfortunately not work with the most recent versions of the rgdal package. This is because the gstat package that both our IDW and Kriging functions are from has not managed to yet accomodate the changes rgdal has made to using gdal 3 and proj 6. For those of you with more recent version (type: rgdal_extSoftVersion() into your console - and check if your GDAL version is 3 or higher / your PROJ.4 is 6 or higher), there is a second section of code using the spatstat library that should hopefully work. We then specify that our IDW result is a gridded format that we then coerce into a raster! Once we have our raster, we can reset its CRS and of course utilise other functions from the raster library to process (e.g. the mask function) and then visualise our dataset within tmap. Let’s get going! 1a. Generate an IDW raster using the gstat library: # Convert our lonpollution_points into the sp format lonpollution_pointsSP &lt;- lonpollution_points %&gt;% as(., &quot;Spatial&quot;) # Convert our london_outline into the sp format london_outlineSP &lt;- london_outline %&gt;% as(., &quot;Spatial&quot;) # Create an empty raster grid the size fof our london_outline over which to # interpolate the pollution values We set a cell size of 450 to try to match the # output from spatstat But normally this should be decided through a literature # search grid &lt;- spsample(lonpollution_pointsSP, type = &quot;regular&quot;, cellsize = 450, bb = bbox(london_outlineSP)) # OPTION 1: GSTAT Run an IDW for the NO2 value with a power value of 2 idw &lt;- gstat::idw(lonpollution_pointsSP$no2 ~ 1, lonpollution_pointsSP, newdata = grid, idp = 2) ## [inverse distance weighted interpolation] # Specify idw spatial data as being gridded gridded(idw) &lt;- TRUE # Coerce to our gridded idw to the raster format lon_poll_raster_idw &lt;- raster(idw) # Set our raster CRS to BNG crs(lon_poll_raster_idw) &lt;- &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs &quot; # Mask our raster to only the london outline It&#39;s currently a rectangle! lon_idw &lt;- rasterize(london_ward, lon_poll_raster_idw, mask = TRUE) # Plot the resulting raster plot(lon_idw) Great - if this code has worked for you and you have generated an IDW raster, you can move onto the next task which is to create a proper map of our resulting IDW. You do not need to complete the code below. Spatstat Alternative for IDW generation For those of you that cannot run the code above, we can look to spatstat as an alternative option - however, it just brings with it its few complications in terms of converting our datasets into our ppp object (and hence our focus on using gstat instead, with only a simple conversion to sp). As a result, in this chunk of code, we will first convert our data to the ppp object type and then use this within the idw() function spatstat offers. 1b. Generate an IDW raster using the spatstat library: # First, let&#39;s set our window of observation as the entirety of London window &lt;- as.owin(london_outline) # Next, extract the coordinates of our pollution points sdf This stores our # coordinates as a matrix points_xy &lt;- lonpollution_points %&gt;% st_coordinates() # Create a ppp object, setting x and y equal to the respective columns in our # matrix Set the window equal to our window variable Set our &#39;marks&#39; equal to the # NO2 column in our points pollution_ppp &lt;- ppp(x = points_xy[, 1], y = points_xy[, 2], marks = lonpollution_points$no2, window = window) # Run the IDW Note, we do not have as much flexibility to control our output # raste settings easily here There is a &#39;as.mask&#39; argument that might allow you # to specify cell size, but I could not get it to work ss_idw &lt;- spatstat::idw(pollution_ppp, power = 2, at = &quot;pixels&quot;) # Coerce our im output (the outputs from the IDW) directly to raster lon_idw &lt;- raster(ss_idw) # Set the CRS of our raster crs(lon_idw) &lt;- &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs &quot; # Plot our results plot(lon_idw) # Note, as we set London as our window, we do not need to mask our raster by our # london outline! You should see we actually get a very similar result to the IDW of the gstat library - that is because our cell sizes resolutions are very similar to one another. We set our cell resolution as 500m x 500m above - and we can check the cell size of our spatstat idw raster using a very simple command: res(lon_idw). You’ll see that the IDW spatstat auto-generated has a 456m cell size, so not too far off our provided cell size in gstat. Mapping our final IDW raster We now have our final predicted raster surface - let’s go ahead To do so, we’ll again use the tm_raster() function within our tmap “grammar of graphics” system. For our raster, the name of the layer we need to provide is var1.pred for those using the gstat result and simply layer for those using the spatstat result. Map our IDW result using tmap: # Use the tmap library to map our IDW properly! # Plot our gstat IDW raster tm_shape(lon_idw) + tm_raster(&quot;var1.pred&quot;, style = &quot;quantile&quot;, n = 100, palette = &quot;Reds&quot;, legend.show = FALSE) + tm_shape(london_ward) + tm_borders(col = &quot;white&quot;, lwd = 0.1) # Plot our spatstat IDW raster tm_shape(london_ward) + tm_borders() + # tm_shape(lon_idw) + tm_raster(&#39;var1.pred&#39;, style=&#39;quantile&#39;, n=100, # palette=&#39;Reds&#39;, legend.show=FALSE) + tm_shape(london_ward) + tm_borders() And that’s it - for those of you able to use the gstat code, it’s highly worth playing around with the cell size to look at how it changes the smoothness of our resulting IDW. A smaller cell size will create a smoother IDW output, but it does add uncertainty to these estimates as we do not exactly have a substantial amount of data points to interpolate from! To help with minimising this uncertainty, there are two additional “data checking” steps you can take with your IDW output: Testing and fine-tuning the power function you’ve used to ensure it is a valid parameter by using something known as the Leave One Out Cross Validation. Generating a 95% confidence interval map of our interpolation mode using cross-validation methods. We do not have time to cover these in our workshop today, but Manuel Gimond provides a short tutorial explaning both of these procedures, available here for your future reference. Note, I would not expect you to produce these for your coursework unless you would like to put in that level of effort! I.e. I will accept the IDW on its own, without the CI map - however, for your power function, it would be great to see if you can find a reference behind why you chose a specific value! Kriging: A Geostatistical Approach An IDW is not the only means of interpolating point data across space to produce a raster sruface. A range of geostatistical techniques have also been devised. One of the most commonly used is kriging. Whilst an IDW is created by looking at just known values and their linear distances, kriging also considers spatial autocorrelation in its interpolation calculations. The approach is therefore more appropriate is there is a known spatial or directional bias in the data. Kriging is a geostatistical approach to interpolation, using variograms to calculate the autocorrelation between points and distance. Like an IDW the the values across space are estimated from weighted averages formed by the nearest points and considers the influence of distance. In our case with pollution data, our data is likely to have spatial bias due to the location and density of roads in London and their influence on the levels of NO2. As a result, kriging is likely to be a better approach for interpolating our data. Several forms of kriging interpolators exist, including ordinary, universal and simple. In our practical today, we will focus on ordinary kriging (OK) interpolation. This form of kriging usually involves four steps: Removing any spatial trend in the data (if present). Computing the sample experimental variogram and fitting the variogram model. Interpolate the surface using the fitted variogram model. Add the kriged interpolated surface to the trend interpolated surface to produce the final surface output. In addition, a confidence interval map can also be made. To conduct these four steps, we’ll be using gstat to conduct our kriging analysis. Issues with different versions of R As mentioned above, the gstat library will not run with newer versions of gdal or proj. We currently do not have an alternative tutorial for those of you who this does not work for - but please make sure you read through the code and look at the outputs, in preparation for the library becoming available. Alternatively, you may look into the kriging package and see if you can follow their documentation to recreate the below tutorial. Step 1) De-trend the data To be able to conduct ordinary kriging, a key assumption must be met: the mean and the variation in the entity being studied is constant across the study area. In other words, there should be no global trend in the data, i.e. we should not see spatial autocorrelation in our data. We know with our data that we have a “trend” to our dataset, with the roads in and around London, of course, influencing pollution levels. We therefore need to remove the trend before proceeding with kriging. To do so, we will create a trend model, representing a first, second or third order polynomial, which we use to de-detrend our point values. Removing the trend will leave us with residuals that we will then use in kriging interpolation, with the modeled trend then added to the kriged surface at the end of the kriging process. Whilst normally we should look into which polynomial is best to use for our, as explained further here, we will compute the trend for the first polynomial within our data. Define the 1st order polynomial in order to use de-trended data in our variogram computation: # To run the polynomial, we first need to extract our coordinates as individual # columns And add these to our lonpollution_points dataframe By adding X and Y to # londpollution_points lonpollution_points$X &lt;- st_coordinates(lonpollution_points)[, 1] lonpollution_points$Y &lt;- st_coordinates(lonpollution_points)[, 2] # Define 1st order polynomial equation, for use in our variogram computation poll_detrend &lt;- as.formula(no2 ~ X + Y) Step 2) Compute the sample experimental variogram and fit the variogram model Once we have detrended our data, we next need to fit a variogram model to the results of this dataset, i.e. the residuals of our linear model. We do this by first creating our variogram plot (using the variogram() function) and checking the results to identify the likely variogram model, nugget and sill that we can fit to our data. Here, instead of plotting all calculations of semi-variance in a cloud plot, we “bin” our cloud points into our lag intervals and summarise the points within each interval. The result is known as the sample experimental variogram plot and is what we then use to fit our model. To “bin” our data, we provide a width parameter to determine the subsequent intervals into which data points are grouped for the semi-variance estimates. There are many other parameters we can also pass with our variogram() function, but we’ll stick to a simple variogram for our practical. Create a semi-variogram of our de-trended pollution points data: # Create a variogram for our lonpollution_points point data var_poll &lt;- variogram(poll_detrend, lonpollution_points, cloud = FALSE, width = 1000) # Inspect results to determine model, sill and nugget plot(var_poll) The next step is to fit a mathematical model to our sample experimental variogram. Different mathematical models can be used, although their availability is often software dependent. (You can see which models - and their parameter names - are available in R by typing vgm() into your console). We can see that our model is a little tricky to identify - looking back at the graphed examples provided in our lecture, there are many models that might suit our data: A subset of variogram models available in R’s gstat package. Source: Gimond, 2020. The goal is to apply the model that best fits our sample experimental variogram. This requires picking the proper model, then tweaking the sill, range, and nugget parameters (where appropriate). In this case, we will try to use the linear model, fitting it to a nugget of 165 and a sill of 175 - in the end, we can see that, although we have a dip in our semi-variance at around 12,500, a linear model may be the best estimator for our detrended data. We use the fit.variogram() function to create this fitted variogram model. Fit a variogram model to our detrended NO2 data points: # Compute the variogram model var_mod &lt;- fit.variogram(var_poll, fit.sills = FALSE, vgm(psill = 175, model = &quot;Lin&quot;, nugget = 165)) # Inspect our sample vs model variograms by plotting plot(var_poll, var_mod) Hmm, what do you think? Like I said, we do have a substantial decrease in our semi-variance that we do not account for using the linear model - but I’m unsure if any other model, such as the wave model, will help us with this. A linear model is in fact one of the three more popular models using in SV fitting, with the other two being spherical and gaussian models. You can go ahead and experiment with different sill, nugget and model parameters to see if you can find a better statistical representation - but for brevity in our practical, we’ll keep going with our results above. Independent decision-making in spatial analysis As I’ve mentioned many times, there is a lot of “estimation” or “guesswork” that comes with spatial analysis - and you often have to many independent decisions on your analysis methodology and approach! My main recommendation is to read through previous literature using the same techniques and/or data as yourself to see what decisions they have made and why - and utilise these as justifications behind your own decisions! Step 3) Interpolate the surface using the variogram model Now we have our de-trended data (poll_detrend) and our fitted variogram model (var_mod), we can go ahead and create our kriged surface. To do so, we use the krige() function, which actually allows us to include the trend model as a parameter in addition to our original lonpollution_points points dataset - this saves us from manually having to combine our kriged trend model (which is a raster in itself) with what would be the main kriged interpolated surface. Before we do so, we’ll also need to create a new grid for us to use and store our kriged result. Create our kriged surface and store it within our newly created grid: # Create empty raster grid over which to interpolate the pollution values Note, # we use the SP version of our lonpollution_points again We use the same cellsize # as our IDW for comparison grid &lt;- as.data.frame(spsample(lonpollution_pointsSP, type = &quot;regular&quot;, cellsize = 450, bb = bbox(london_outlineSP))) # We then need to process our grid for use with our raster Set the names of the # grid&#39;s columns to X and Y names(grid) &lt;- c(&quot;X&quot;, &quot;Y&quot;) # Set the coordinates of the grid to our new X and Y columns coordinates(grid) &lt;- c(&quot;X&quot;, &quot;Y&quot;) # Specify our data as being gridded gridded(grid) &lt;- TRUE # Make sure our grid is &#39;full&#39;, i.e. complete fullgrid(grid) &lt;- TRUE # Set the CRS of our grid to that of our spatial points proj4string(grid) &lt;- proj4string(lonpollution_pointsSP) # As we&#39;re using a grid in SP, and will therefore parse our points in SP, we need # to go ahead and add the X and Y columns to our SP points dataset as we did with # our sf version # To run the polynomial, we first need to extract our coordinates as individual # columns And add these to our lonpollution_pointsSP dataframe Add X and Y to # londpollution_pointsSP lonpollution_pointsSP$X &lt;- coordinates(lonpollution_pointsSP)[, 1] lonpollution_pointsSP$Y &lt;- coordinates(lonpollution_pointsSP)[, 2] # We&#39;re now ready to krige our data! # Perform the kriged interpolation on our lonpollution_pointsSP, including our # trend model Using the detrend data (poll_trend) earlier on and the variogram # (var_mod) created just now Store in our new grid poll_krige &lt;- krige(poll_detrend, lonpollution_pointsSP, grid, var_mod) ## [using universal kriging] The next step within our kriging is to turn our kriging result into a raster. Once we’ve done this, we can go ahead and map our result. Convert kriged result to raster and map using tmap: # Convert our kriged result to a raster lonpoll_krige &lt;- raster(poll_krige) # Mask our kriged raster to London and map tm_shape(mask(lonpoll_krige, london_outlineSP)) + tm_raster(&quot;var1.pred&quot;, style = &quot;quantile&quot;, n = 50, palette = &quot;Reds&quot;, legend.show = FALSE) + tm_shape(lonpollution_pointsSP) + tm_dots(col = &quot;yellow&quot;) + tm_shape(london_ward) + tm_borders(col = &quot;white&quot;) And that’s it - we have our final kridged raster. Extra Step - Variance and Confidence Interval Maps A valuable by-product of the kriging operation is the variance map which can give us a measure of uncertainty in our resulting interpolated values. To create a variance map, we can simply use an output from our krige() function as our poll_krige kriged object actually stores not just the interpolated values, but the variance values as well. We simply generate a similar raster using these values and look to understand the variance in our interpolated values. Essentially, the smaller the variance, the better - note, that our variance values will be squared units - so the square of ug/m^3. Create a variance map of our kriged result: # Convert our kriged variances to a raster lonpoll_krige_var &lt;- raster(poll_krige, layer = &quot;var1.var&quot;) # Mask our variance raster to London and map tm_shape(mask(lonpoll_krige_var, london_outlineSP)) + tm_raster(n = 7, palette = &quot;Blues&quot;, title = &quot;Variance Map (in ug/m^3 squared&quot;) + tm_shape(lonpollution_pointsSP) + tm_dots(col = &quot;yellow&quot;) + tm_shape(london_ward) + tm_borders(col = &quot;white&quot;) + tm_legend(legend.outside = TRUE) Unsuprisingly, we can see our variance increases the further. As we know we have a denser set of points in the centre of London compared to the outskirts, which is likely to explain smaller variance in our central areas versus the outskirts. A more easily interpretable map is the 95% confidence interval map which can be generated from the variance object as follows (the map values should be interpreted as the number of ug/m^3 above and below the estimated pollution amount). Create a confidence interval map of our kriged result: # Convert our kriged variances to a raster lonpoll_krige_ci &lt;- sqrt(raster(poll_krige, layer = &quot;var1.var&quot;)) * 1.96 # Mask our variance raster to London and map tm_shape(mask(lonpoll_krige_ci, london_outlineSP)) + tm_raster(n = 7, palette = &quot;Oranges&quot;, title = &quot;Confidence Interval Map (in ug/m^3)&quot;) + tm_shape(lonpollution_pointsSP) + tm_dots(col = &quot;yellow&quot;) + tm_shape(london_ward) + tm_borders(col = &quot;white&quot;) + tm_legend(legend.outside = TRUE) We now have a confidence map to help us interpret our kriged result. As we can see, we can be more confident in our values located in the centre of London, whereas, for example, we should be more uncertain when looking at the values (and overall patterns) in the north-east. Now you have both an IDW and an ordinary kriging raster that predict the average value of Nitrogen Dioxide (in \\(\\mu\\)g/m3) in 2019 in London - which method do you think gives a better performance and why? You’ll need to make a decision in order to complete this week’s second assignment. Calculating NO2 change in London between 2019 and 2020: Impact of COVID Your second (and very optional) assignment for this week is to calculate either an IDW or kriging raster for the same NO2 dataset in 2020. You can find the same csv zipfile - but for 2020 - for download here. You can then use one of several of the techniques we learnt earlier above in our map algebra section to understand how did NO2 change in London between 2019 and 2020? I’ll collect any final maps in our seminar at the start of Week 10, so no need to submit anything beforehand. Extension: Single-Value Rasters v. Multi-Band Imagery This week’s extension is not really an extension per se, but a short piece of information on using rasters. As you’ll have seen, we’ve used two different approaches in mapping rasters using tmap. The first - and our main approach, including today - is using the tm_raster(). We use this function when we either want to apply a fixed colour to our raster or a colour palette to a specific variable. As you’ve read earlier, a raster dataset normally only contains one layer, i.e. one variable. Hence when we want to map a raster, we use the tm_raster() and provide the layer name for mapping. In our examples, this has been layer and var1.pred, for example. However, in some circumstances, such as with satellite imagery and, if you remember last week, our OpenStreetMap basemap raster, we will want to use the tm_rgb(). This is because these types of rasters, instead of having a single layer, actually consist of three bands: a band with a red value, a band with a green value and a band with a blue value. This is known as “multi-band” imagery. To visualise these data correctly, we therefore need to use the tm_rgb() function in order to stack our three bands together to create the appropriate visualisation. We can visualise each band independently of one another, however, if you try it out with your OSM basemap from the previous week, you’ll see that you end up with either a nearly all red, green or blue image! We don’t have much time in our module to cover satellite imagery, but if you’d like to learn more about using satellite imagery with R, I recommend checking out this tutorial by a fellow lecturer at CASA on calculating the Normalised Difference Vegetation Index. Alternatively, you can also check Esri’s help information on Rasters and Raster Bands here. Learning how to use satellite imagery can be a really useful skillset, particularly as this type of data is being increasingly used human geography applications - as well as, of course, its more traditional applications in physical and environmental geography. Recap - Rasters, Zonal Statistics and Interpolation This week, we’ve looked at raster datasets and how we use the raster library to manage and process them. Specifically, we looked at using map algebra to apply mathematical operations to rasters, using local, global, focal and zonal approaches and how we use map algebra on either an individual or combination of rasters. We then looked at how we can use different interpolation methods to generate raster data from point data. These techniques are split into two categories, deterministic and geostatistical methods. The two approaches have their various advantages over one another - from efficiency in processing to accounting for spatial autocorrelation within our data. Which technique you use will be determined by your application and analysis requirements. Understanding how to interpolate data correctly is incredibly important. Whilst in most instances you will be working with vector data, especially where government statistics and administrative boundaries are involved, there are also plenty of use cases in which you will need to generate raster data from point data, as we have done today. You now know the basics of working with raster datasets as well as that you can create your own raster datasets by using interpolation and geostatistical methods such as kriging to predict a given phenomenon in unmeasured locations. One thing to remember is that when we use interpolation, we are looking at a value at a given point and predicting values for non-sampled points. When we use kernel density estimation, we are looking at the density of the points themselves and estimating likely density of the points themselves for non-sampled locations. Make sure you do not use the two techniques interchangeably - as they have very different purposes! Learning Objectives You should now hopefully be able to: Use, analyse and visualise raster data in R confidently. Utilise map algebra to analyse two or more raster datasets together. Utilise vector and raster data together using zonal statistics. Explain what interpolation is and the different techniques we can use. Implement different geostatistical techniques in R. Utilise vector and raster data together using zonal statistics. Acknowledgements This page is adapted from GEOG0114: Principles of Spatial Analysis: Raster data and geostatistics by Dr Joanna Wilkin (this Workbook’s author) and Dr Justin Van Dijk at UCL (the author of the week’s practical within GEOG0114) and Interpolation in R by Manuel Gimond. The datasets used in this workshop (and resulting maps): Lloyd, C. D., Bearman, N., Catney, G, Singleton, A. and Williamson, P. (2016) PopChange. Liverpool: Centre for Spatial Demographics Research, University of Liverpool. London Air Pollution Data © 2018, Environmental Research Group, Imperial College London. Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] "],["geodemographic-classification.html", "10 Geodemographic Classification", " 10 Geodemographic Classification Welcome to Week 10 - our final official week - in Geocomputation! This week, we’ll be covering Geodemographic Classification. In previous weeks, we’ve looked at how we can map single variables to specific administrative and analyse their distribution to understand local characteristics of an area. Geodemographic classification takes this one step further to create a more complex understanding of these characteristics - we take a selection of variables about our administrative areas and run an algorithm that clusters these areas into people of certain “similarities”. There’s a lot more behind the process, but ultimately our result is a map at the chosen spatial scale with these defined clusters that we give “names” based on their similarities. This week, we’ll introduce you to a few of the Geodemographic Classifications that are available to you to use as open data for England as well as look to create our own “GDC” for a different area in England, Cambridgeshire (county) and Peterborough (city), which are located approximately 50 miles north of London. Within the Extension, we’ll look at a few additional Indices that you should be aware of moving forwards in Geocomputation, including revisitibg the Index of Multiple Deprivation and understand its constituent domains as well as the Public Transport Accessibility Layer. Week 10 in Geocomp This week’s content introduces you to Geodemographic Classification. We have two areas of work to focus on: Open Geodemographic Classifications Geodeomographic Classification Methodology Geodemographic Classification in R Extension: Understanding Index Construction via IMD This week’s content is split into 4 parts: Workshop Housekeeping (10 minutes) [What is Geodemographic Classification] (30 minutes) A Typical Geodemographic Classification Workflow (30 minutes) Implementing Geodemographic Classification in R (60 minutes) Extension: Additional Indices for use within Geocomputation (10 minutes) This week, we have 2 short lectures within this week’s main workshop content - we do not have any assignments this week. Learning Objectives By the end of this week, you should be able to: Explain what is a Geodemographic Classification (GDC). Access and utilise already existing GDCs. Describe the typical GDC workflow and its limitations. Implement a GDC classification in R. Recall several pre-created GDCs and Indices that are available to use in spatial analysis. We’ll look to create our GDC for our new area of focus: the county of Cambridgeshire and the connected/associated city of Peterborough. (A long story short - Peterborough is in the county of Cambridgeshire, but in 1998, the city became autonomous of Cambridgeshire county council as a unitary authority. It does continues to form part of the county for ceremonial purposes, including administrative data purposes - as a result, we treat the two as a single spatial entity, although recognise Peterborough as autonomous. Who said administrative boundaries were easy?!). To complete this analysis, we’ll be using two new datasets: 2011 Census Data for Cambridge and Peterborough: a csv containings an aggregated representation of each different census variable for each Output Area (pre-prepared for us!). Output Area Boundaries: Output Area (OA) administrative boundaries for Cambridge and Peterborough. Due to the complexity in creating a GDC from scratch, we’ll be using already prepared data to conduct our analysis. Workshop Housekeeping Let’s get ourselves ready to start our lecture and practical content by first downloading the relevant data and loading this within our script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk10-geodemographic-classification.r. At the top of your script, add the following metdata (substitute accordingly): # Creating a Geodemographic Classification of Cambridge and Peterborough Script # started March 2021 NAME Dependencies (aka libraries) Now we’ll install the libraries we need for this week. In addition to those libraries you should now be familiar with, we will need to install and use: Formula: for correlation analysis Hmisc, preferably version 0.5-2: for more under-the-hood spatial data management fpc: for visualising cluster assignment (you may have this installed from our use of DB-Scan previously) plotrix: to create our radial plots Remember to use the install.packages(\"package\") command in your console. Within your script, add the following libraries for loading: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) library(RColorBrewer) library(Formula) library(Hmisc) library(fpc) library(plotrix) Remember to select the lines of code you want to run and press CMD (Mac)/CTRL(Windows) + Enter/Return - we won’t remind you to run each line of code in the remainder of the practical sessions. Datasets for this week This week, in a rarity for Geocomputation, we will be used pre-processed data for our Geodemographic Classification. You’ll need to download three datasets: Dataset File Type Link Output Areas for Cambridgeshire and Peterborough shp Download Prepared Classification Variables for Cambridgeshire and Peterborough csv Download Variable Look Up Table csv Download Once downloaded, copy over these files into your data –&gt; raw –&gt; gdc folder (i.e. create a new gdc folder!). This is all the data we’ll need for this week’s practical. What is Geodemographic Classification? Put simply, geodemographics is the ‘analysis of people by where they live’ (Harris et al. 2005) and involves the classification of geographical areas based on the socioeconomic characteristics of their residents (Cockings et al, 2020). To summarise the general geodemographic classification “GDC” workflow succintly, a set of variables are selected by the researcher that represent the characteristics by which the neighbourhoods should be classified, whether looking at demographics, including age, ethnicity, and family or household structure, or socio-economic characteristics, such as employment status and occupation, education and qualification levels, income or income proxies. These variables are provided at an aggregated spatial scale, representative of the neighbourhoods that will be studied (e.g. Output Areas, Lower Super Output Areas, Workplace Zones). An algorithm is then used to cluster neighbourhoods together that have similarities across these variables. The researcher then assigns these groups of neighbourhoods identities by giving them labels (names) and associated “pen profiles” that qualitatively describe the variables seen within the cluster. In most cases, the final output of a GDC is a map highlighting these different clusters and their location with one another. This output can then be used in further analysis and within other applications. Lecture: An Introduction to Geodemographic Classification This week’s first lecture provides you with a general introduction into GDCs and their history, applications and limitations. Lecture slides | Watch on MS stream The lecture is further expanded upon below, with examples of the various open GDCs mentioned in the lecture for our area of interest, Cambridgeshire and Peterborough. GDCs and the role of geography A GDC entails representing the individual and collective identities that are manifest in observable neighbourhood structure (Longley 2012). As a result, it has traditionally fallen into the nomothetic approach to geographic enquiry - emphasizing the shared social, economic, and demographic characteristics of different types (or classes) of neighborhoods, independent of their locations relative to unique places. As such, a type of neighborhood may be widely scattered across a territory or jurisdiction. Geodemographic profiles thus provide summary indicators of the commonalities of social structure that link different locations (Longley, 2017). These “spatial” commonalities however do not become evident within the GDC process until the outputs are mapped - and this spatial distribution can then be used to help create the identities (e.g. city elites, rural retirees or inner-city families). Instead, the role of “space” per se within GDCs is part of much of the theory that underlines the success of GDCs to “work” in the first place. As we know as geographers and spatailly-enabled thinkers, “everything is related to everything else, but near things are more related than distant things” (Tobler, 1970) - and this often stands true in terms of the formation of communities. An underlying concept in sociology is homophily, where there is a tendency of individuals to associate and bond with those who are similar to themselves. This, alongside Tobler’s Law, can help us conceptualise why similar people and households and co-locate, forming communities and neighbourhoods that are relatively homogenous. Furthermore, once an area is established, it encourages others of similar backgrounds and characteristics to also move to the same area - and benefit from whatever “conditions” attracted the original households, whether it is low house prices, proximity to certain instiutions or even planned housing projects. Thus, whilst space will ultimately be a factor in the cluster results, it is not taken into account during the clustering as a variable itself! This is one important thing to remember, particularly after the previous few weeks where we have looked at clustering algorithms that do take into account space as a direct varaible, such as DB-Scan. #### Applications of GDCs {-} GDCs are a useful means on segmenting the population into distinctive groups in order to effectively channel resources. Such classifications have been effective deductive tools for marketing, retail and service planning industries due to the assumed association between geodemographics and behaviour. For instance, typically a classification at the broadest level may distinguish cosmopolitan neighbourhoods with high proportions of young and newly qualified workers from suburban neighbourhoods with high proportions of settled families. Whilst most geodemographic products are built within the commercial sector and sold by vendors, increasingly more open source alternatives are available, particularly for the UK where GDCs are a prominent area of research within Quantitative Geography departments. (Open) GDCs in the UK (and its divisions) The UK is a hotbed of GDC research, with much of it occuring at UCL (and its incredibly productive PhD students within the CDRC, spearheaded by Professor Paul Longley) and also the various other Universities, where many of these former PhD students have moved to and set up their own CDRC partnering research groups. Other active GDC institutions include the Office for National Statistics (ONS), who were responsible for (with the University of Leed) the first openly available GDC in 2005, and the University of Southampton, which focuses on the “workplace” version of the more general GDC available for the UK and/or England and Wales (see below). Prior to the ONS and their GDC, the majority of GDC research and application in the UK occured “behind closed doors” with commercial firms such as Experian and CACI Limited creating and utilising their own GDCs (MOSAIC and ACORN respectively) for in-house applications. Now, particularly with the push for Open Data in the UK since the beginning of the millenium (and all the data portals we’ve seen pop up since data.gov.uk was first online in 2010), we have access to a huge amount of data that is enabling research into new types of GDCs, from the general purpose and original “Output Area Classifcation” to the newly created Internet User Classification, all of which are openly available and free to download and use. We utilise this in our workbook today, where I’ve created maps of several of the openly available GDCs for our area of interest today, Cambridgeshire and Peterborough. Output Area Classification The Output Area Classification (OAC) GDC was the original GDC created by the ONS and the University of Leeds. The first OAC was created using 2001 census data at the output scale. Each area was assignment one of seven clusters, based on the socio‐economic attributes of the residents of each area. The classification used cluster analysis to reduce 41 census variables to a single socio‐economic indicator (Vickers and Rees, 2007). The OAC was updated for 2011 census data by a team of researchers across several universities and published in 2016. It summarizes the social and physical structure of neighborhoods using data from the 2011 UK Census, with an updated methodology that also included a user engagement exercise. The 2011 OAC comprises 8 Supergroups, 26 Groups, and 76 Subgroups (Gale et al, 2016). The OAC supergroups for Cambridgeshire and Peterborough area are shown here: Note, there is a specific colour scheme to use with the Output Area Classification (and many of the GDCs!) which is not applied here. ## Reading layer `E47000008&#39; from data source `/Users/Jo/Code/GEOG0030/data/raw/gdc/Output_Area_Classification_2011/Combined_Authorities/E47000008/shapefiles/E47000008.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 2540 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 501879.2 ymin: 236168.1 xmax: 571833.4 ymax: 318013.3 ## CRS: 27700 ## Warning: The shape opa_shp is invalid. See sf::st_is_valid The data used in the map above is provided by CDRC 2011 OAC Geodata Pack by the ESRC Consumer Data Research Centre; Contains National Statistics data Crown copyright and database right 2015; Contains Ordnance Survey data Crown copyright and database right 2015. In addition to the above general Output Area Classification for the entirety of the UK, an additional GDC was created specifically for London and its respective output areas, known as the London Output Area Classification (LOAC). You can find the LOAC here on the CDRC website (alongisde interactive versions of all the maps displayed in this workbook). These classifications were conducted at Output Area scale, which usually consists of approximately 100 individuals and 40 households. Scales of Analysis As you should know, normally, I would not advocate running an analysis or aggregation at this scale (with the Lower Super Output Area as my recommended minimum scale), but GDCs are often created on data of this scale of resolution. Despite this availability, you should however be cautious and conscientious with your results as, although the data is aggregated and thus anonoymised, this fine resolution can still mean that those living in the OAs you are using can often “identify” themselves within the data and ultimately your resulting categorisation of them. I still very much advocate that any analysis directly to do with public health should always occur at least the LSOA scale. Classification of Workplace Zones A relativley new GDC to enter into the UK’s growing collection is a set of GDCs developed by researchers at the University of Southampton (my PhD alma mater!) and the ONS. With their Classification of Workplace Zones, (COWZ) the researchers developed a novel classification based on the characteristics of workers and workplaces within local areas at two spatial scales: 1) England and Wales and 2) the UK (i.e. England, Wales, Scotland and Northern Ireland). The COWZ-EW is a classification of Workplace Zones for England and Wales. The COWZ-UK is a classification of Workplace Zones for the UK. The GDCs used data from the 2011 censuses from England and Wales, Scotland and Northern Ireland to create the two GDCs. The GDCs are created at a unique spatial scale, known as the Workplace Zones (hence the title, Classification of Workplace Zones). Unlike the different levels of Output Areas, there is no hierarchical structure. There are 60,709 Workplace Zones in the UK. The openly available classification provides important new insights into the characteristics of workers and workplaces at the small area level across the UK (or England and Wales), which will be useful for analysts in a range of sectors, including health, local government, transport and commerce (Cockings et al, 2020). The England and Wales Classification of Workplace Zone for Cambridgeshire and Peterborough area is shown here: ## Reading layer `COWZ_EW_2011_BFC&#39; from data source `/Users/Jo/Code/GEOG0030/data/raw/gdc/Classification_of_Workplace_Zones_EnglandWales/Shapefiles/COWZ_EW_2011_BFC.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 53578 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 82672 ymin: 5342.699 xmax: 655598.3 ymax: 657534.1 ## CRS: 27700 ## Reading layer `E47000008&#39; from data source `/Users/Jo/Code/GEOG0030/data/raw/gdc/Internet_User_Classification/Combined_Authorities/E47000008/shapefiles/E47000008.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 487 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 501877.8 ymin: 236172.9 xmax: 571830.7 ymax: 318016.5 ## CRS: 27700 The data used in the map above is provided by the 2011 COWZ-EW Geodata Pack by the University of Southampton; Contains public sector information licensed under the Open Government Licence v2.0. Contains National Statistics and Ordnance Survey data © Crown copyright and database right 2015. Interestingly enough, we can see much of Cambridgeshire and Peterborough are designated as rural workplace zones, although we can certainly see the two major cities (Cambridge and Peterborough), with it’s suburbs and “top jobs”. Also interesting to identify the manufacturing and distributions areas! In addition to the above general Classification of Workplace Zonesn for the entirety of the UK, an additional GDC was created specifically for London and its respective output areas, known as the London Workplace Zone (LWZC) (Singleton and Longley, 2019). You can find the LWZC here on the CDRC website (alongisde interactive versions of all the maps displayed in this workbook). Internet User Classification Another new GDC entry is the CDRC Internet User Classification (IUC). The IUC is a bespoke geodemographic classification that describes how people residing in different parts of Great Britain interact with the Internet. For every Lower Super Output Area (LSOA) in England and Wales and Data Zone (DZ) (2011 Census Geographies), the IUC provides aggregate population estimates of Internet use (Singleton et al. 2020) and provides insights into the geography of the digital divide in the United Kingdom. “Digital inequality is observable where access to online resources and those opportunities that these create are non-egalitarian. As a result of variable rates of access and use of the Internet between social and spatial groups (..), this leads to digital differentiation, which entrenches difference and reciprocates digital inequality over time (Singleton et al. 2020).” As you’ll see below, the IUC is created at the Lower Super Ouput Area scale. We can look the various IUC categories applied within Cambridgeshire and Peterborough and their respective locations. ## Reading layer `E47000008&#39; from data source `/Users/Jo/Code/GEOG0030/data/raw/gdc/Internet_User_Classification/Combined_Authorities/E47000008/shapefiles/E47000008.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 487 features and 1 field ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 501877.8 ymin: 236172.9 xmax: 571830.7 ymax: 318016.5 ## CRS: 27700 The 2018 Internet User Classification. ESRC CDRC. Contains National Statistics data Crown copyright and database right (2017); Contains Ofcom data (2016). Contains CDRC data from Data Partners (2017). It seems the city of Cambrige has a wide variety of internet users, with Cultural Creators, Professionals, the Youthful Urban Fringe and Mainstream users prominent within the area - a pattern we’d likely expect. A Typical Geodemographic Classification Workflow Looking through our various GDCs for Cambridgeshire, we can see that there is indeed some really interesting (and if not, common sense!) clustering to our different supergroups and workplace zones. GDCs can provide insight to the complex characteristics of local areas through a single measurement - but how do we reduce many variables into a single classification scheme? And how do we determine the labels for the resulting classifications? Lecture: Geodemographic Classification Methodology The second of this week’s lecture provides you with an indepth step-by-step workflow on how to create a GDC. Lecture slides | Watch on MS stream The GDC workflow presented in the above lecture is summarised below. Creating a Geodemographic Classification As you would have heard in our lecture above, there are seven main steps to creating a GDC. 1) Choose elements / factors The first step to creating a geodemographic classification is considering what data to include and at what granularity. Finer level data will allow you to capture more intricate variations and reduce any issues of ecological fallacy. However, we also require a good number of useful variables in order to effectively segment neighbourhoods. 2) Select associated quantifiable variables To capture these elements or factors in your GDC, you’ll need to translate them into quantifiable variables that you can find within a specific data source, such as the census, at the scale at which you want to analyse them. 3) Standardise variables To reduce the effects of unbalanced base population sizes across each of the small area units (Output Areas) the variables all need to be transformed into percentages. Furthermore, so that erratic values within variables do not inadvertently dominate the clustering process, the input variables need to be standardised so that they each contribute an equal weight. Standardising the data will also make the final outputs much easier to interpret. 4) Measure variables for association (multi-collinearity) It is useful to test for associations between the final selection of variables. Variables that are collinear would essentially be conveying very similar distributions. This could give a particular phenomena a higher weighting in the final classification. To check for multicollinearity, create a correlation matrix for the dataset and remove one variable from those pairs of variables that are correlated. Try to remove the variables that are correatled with other variables to leave those most unique in your selection. 5) Choose clustering method There are several clusering algorithms that can be used to cluster variables for a GDC, but generally, most tend to use the K-Means algorithm. It is a top-down approach whereby the number of cluster groups is predefined. The algorithm seeks to reduce the sum distance between each data point and their respective cluster centre and works iteratively to produce the final cluster groupings. 6) Choose number of clusters There is no right answer to choosing the correct number of clusters. Even making judgements using some guidance on criteria involves a level of subjectivity. Your task as a researcher is to choose an appropriate number of clusters for your geodemographic classification. You should aim to make sure your clusters are as homogeneous as possible and that each cluster group should be distinct from the other groups. The groups should also be as evenly sized as possible. In addition, to each of these, we must also consider the compositions of the cluster groups. It is important that each of the characteristics of each cluster are distinguishable and relatable to real-life neighbourhood types. 7) Interpret findings, test and finalise them Before any efforts are made to visualise the data, it is important you understand what it represents. The cluster centres indicate the coordinates of the centroid for each cluster group once the k-means had reached its optimum solution. It, therefore, is a good indicator of the average characteristics of each group based on the n variables that were included in the original model. Having selected the number of clusters for the final model, there are several ways of visualising the results of a k-means clustering to help with labelling our clusters. We can create radial plots or cross-variable tables that depict the average variable score for each cluster type. In addition, we can map the location of our clusters to help understand the spatial aspects of our clusters (alongside our own local knowledge). Formulating names for geodemographic groups is not easy and be contentious. Essentially the names are derivative of the researcher’s perceptions of the cluster centres and sometimes also consider their geographic distributions. Names should be informative, short and memorable. They should also not make unnecessary assumptions or be derogatory in any way. Have a look at the names for the GDCs shown above and see what you think - do you think they meet all of these requirements? Summarising the GDC workflow These seven steps summarise the process of creating a GDC. Whilst it appears relatively straight-forward, behind each step will be significant research and researcher-guided decisions on the various subjective aspects of the methodology (e.g. deciding variables, number of clusters etc.). We’ll have a look at the required decisions by putting this all into action in today’s practical, creating a GDC for Cambridgeshire and Peterborough. Whilst this tutorial will take you through all the necessary “technical” steps behind creating a GDC, do remember, we’ve ultimately made the methodological decisions for you at each step (informed by our own research and knowledge), which provides us with a “shortcut” to what ultimatel is a very complex methodology. Implementing Geodemographic Classification in R In today’s practical, we will classify areas in Cambridgeshire and Peterborough based on their demographic and socio-economic characteristics, using k-means clustering. The clustering will group cases based on their statistical similarity as exerted by the inputted variables. As you’ve heard, GDC is a very effective means of reducing large multivariate databases into a singular, but informative, indicator. Cases in the same group may be similar in terms of their geodemographic compositions, but they may not be geographically proximal as the technique is aspatial. This practical will take you through all the steps necessary to create a classification: from data selection, preparation, clustering and eventually interpretation. Note, as we’re creating a GDC for a more localised area (i.e. not the whole of England and Wales or the UK), we’ll see some differences between our classification and that of the general 2011 Output Area Classification. Although this provides you with a pipeline in which to create your own GDC, you will see that the classification builder must make several analytical and subjective decisions in order to produce an optimum classification for a particular purpose (which we’ve done for you). Therefore, if and when creating your own “original” GDC, it must be substantiated thoroughly in research. Hence I would very much encourage you to use an existing GDC in your Gecomputation coursework (and not create your own). Of course, if you’d like to create one during your dissertation, this will be a discussion with your supervisor. 1) Choosing Our Elements The first aspect of any GDC is to choose the elements we want to classify. This includes “who” (individuals, adults) we’re classifying (or what, when it comes to households, OAs or postcodes, for examples), for what purpose (general, bespoke) and the scale at which we will represent them (OAs, LSOAs, workplace zones etc). In our case, we will build a general purpose classification based on the Output Area Classification referenced above, using 2011 census data of indivduals referenced to the OA scale (instead of the OA) - but in our case, for only the Cambridgeshire and Peterborough area. To enable this, we’ll use our 2011 census data for classification and the 2011 Output Area Administrative Boundaries to map our results. Let’s go head and load our administrative boundaries and look at our area under investigation in a little more detail, Cambridgeshire and Peterborough. Load Cambridgeshire and Peterborough (CAP) OA shapefile into R and plot the LSOAs: # Load Cambridgeshiere and Peterborough LSOA shapefile cap_oa &lt;- st_read(&quot;data/raw/gdc/cam_and_pet_boundaries.shp&quot;) ## Reading layer `cam_and_pet_boundaries&#39; from data source `/Users/Jo/Code/GEOG0030/data/raw/gdc/cam_and_pet_boundaries.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 2541 features and 5 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 501880 ymin: 236169.2 xmax: 571835.3 ymax: 318015.5 ## CRS: 27700 # Plot LSOA file plot(cap_oa$geometry) There are 2541 CAP OAs in total, of which we can already see a huge variety in their size. The county has two major cities, Peterborough (in the north) and Cambridge (smaller entity in the South). Beyond these cities, as evident in the structure of the LSOAs, there are smaller towns, but also some OAs that are quite large due to the more rural nature of the county. 2) Select associated quantifiable variables Now we know the elements we’re looking to classify, the next step in our GDC workflow is to select our variables that we want to use in our classification. Whilst the 2011 Census collected hundreds of variables, the 2011 Output Area Classification we looked at earlier was actually produced with 67 variables. The variables were selected on their informativeness and uniqueness i.e. a variable had to be comprehensive enough to simplify the complexity of the phenomena, but not too comprehensive so that variables are distinguishable. To keep things simple for our classification, we will simply use the same 59 variables from the 2011 Output Area Classification. These variables are detailed in the ONS’s detailed methodology guide here. We already have these variables stacked within our cam_and_pet_oac11_vars.csv - let’s go ahead and load the dataset. Load the cam_and_pet_oac11_vars.csv in your data/raw/gdc folder: # Load variable table Remove the first column as it only contains ID numbers cap_oac_variables &lt;- read_csv(&quot;data/raw/gdc/cam_and_pet_oac11_vars.csv&quot;)[, -1] ## Warning: Missing column names filled in: &#39;X1&#39; [1] # Get dimensions of resulting df dim(cap_oac_variables) ## [1] 2541 71 You should have 2541 observations with 71 fields. Let’s go ahead and look at ourdata. Display the first five rows of our cap_oac_variables data frame: # Return the first five lines of our data frame head(cap_oac_variables) ## # A tibble: 6 x 71 ## OA Total_Population Total_Households Total_Dwellings Total_Household… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E000… 290 119 131 131 ## 2 E000… 287 122 128 128 ## 3 E000… 187 97 99 99 ## 4 E000… 179 78 83 83 ## 5 E000… 278 117 122 122 ## 6 E000… 345 139 145 145 ## # … with 66 more variables: Total_Population_16_and_over &lt;dbl&gt;, ## # Total_Population_16_to_74 &lt;dbl&gt;, Total_Pop_No_NI_Students_16_to_74 &lt;dbl&gt;, ## # Total_Employment_16_to_74 &lt;dbl&gt;, ## # Total_Pop_in_Housesholds_16_and_over &lt;dbl&gt;, ## # Total_Population_3_and_over &lt;dbl&gt;, k001 &lt;dbl&gt;, k002 &lt;dbl&gt;, k003 &lt;dbl&gt;, ## # k004 &lt;dbl&gt;, k005 &lt;dbl&gt;, k006 &lt;dbl&gt;, k007 &lt;dbl&gt;, k008 &lt;dbl&gt;, k009 &lt;dbl&gt;, ## # k010 &lt;dbl&gt;, k011 &lt;dbl&gt;, k012 &lt;dbl&gt;, k013 &lt;dbl&gt;, k014 &lt;dbl&gt;, k015 &lt;dbl&gt;, ## # k016 &lt;dbl&gt;, k017 &lt;dbl&gt;, k018 &lt;dbl&gt;, k019 &lt;dbl&gt;, k020 &lt;dbl&gt;, k021 &lt;dbl&gt;, ## # k022 &lt;dbl&gt;, k023 &lt;dbl&gt;, k024 &lt;dbl&gt;, k025 &lt;dbl&gt;, k026 &lt;dbl&gt;, k027 &lt;dbl&gt;, ## # k028 &lt;dbl&gt;, k029 &lt;dbl&gt;, k030 &lt;dbl&gt;, k031 &lt;dbl&gt;, k032 &lt;dbl&gt;, k033 &lt;dbl&gt;, ## # k034 &lt;dbl&gt;, k035 &lt;dbl&gt;, k036 &lt;dbl&gt;, k037 &lt;dbl&gt;, k038 &lt;dbl&gt;, k039 &lt;dbl&gt;, ## # k040 &lt;dbl&gt;, k041 &lt;dbl&gt;, k042 &lt;dbl&gt;, k043 &lt;dbl&gt;, k044 &lt;dbl&gt;, k045 &lt;dbl&gt;, ## # k046 &lt;dbl&gt;, k047 &lt;dbl&gt;, k048 &lt;dbl&gt;, k049 &lt;dbl&gt;, k050 &lt;dbl&gt;, k051 &lt;dbl&gt;, ## # k052 &lt;dbl&gt;, k053 &lt;dbl&gt;, k054 &lt;dbl&gt;, k055 &lt;dbl&gt;, k056 &lt;dbl&gt;, k057 &lt;dbl&gt;, ## # k058 &lt;dbl&gt;, k059 &lt;dbl&gt;, k060 &lt;dbl&gt; We can see that the first few fields contain information about different population sizes (total population, households, dwellings etc) - these are the values we’ll use to standardise our variables as per the GDC workflow. After these fields, we then have our variable fields, denoted as k0XX. These codes are not incredibly useful for us as first-time GDC creators - but they are used due to the fact that some of the variables have long names and asa result, it is much easier to refer to them by their codes. To get an understanding of what these variables related to, we can use our look-up table to see what these codes represent. Load our look-up table: # Load variable lookup table var_lookup &lt;- read_csv(&quot;data/raw/gdc/2011_OAC_Raw_kVariables_Lookup.csv&quot;) Let’s take a look at what our variables represent. Print the contents of the VariableName field: # Print VariableName from lookup table var_lookup$VariableName ## [1] &quot;Total Population&quot; ## [2] &quot;Total Number of Households&quot; ## [3] &quot;Total Dwellings&quot; ## [4] &quot;Total Household Spaces&quot; ## [5] &quot;Total Population aged 16 and over&quot; ## [6] &quot;Total Population aged 16 to 74&quot; ## [7] &quot;Total Population aged 16 to 74 who are not students in Northern Ireland&quot; ## [8] &quot;Total Persons Employed aged 16 to 74&quot; ## [9] &quot;Total Population living in Households aged 16 and over&quot; ## [10] &quot;Total Population aged 3 and over&quot; ## [11] &quot;Persons aged 0 to 4&quot; ## [12] &quot;Persons aged 5 to 14&quot; ## [13] &quot;Persons aged 25 to 44&quot; ## [14] &quot;Persons aged 45 to 64&quot; ## [15] &quot;Persons aged 65 to 89&quot; ## [16] &quot;Persons aged 90 and over&quot; ## [17] &quot;Number of persons per hectare&quot; ## [18] &quot;Persons living in a communal establishment&quot; ## [19] &quot;Persons aged over 16 who are single&quot; ## [20] &quot;Persons aged over 16 who are married or in a registered same-sex civil partnership&quot; ## [21] &quot;Persons aged over 16 who are divorced or separated&quot; ## [22] &quot;Persons who are white&quot; ## [23] &quot;Persons who have mixed ethnicity or are from multiple ethnic groups&quot; ## [24] &quot;Persons who are Asian/Asian British: Indian&quot; ## [25] &quot;Persons who are Asian/Asian British: Pakistani&quot; ## [26] &quot;Persons who are Asian/Asian British: Bangladeshi&quot; ## [27] &quot;Persons who are Asian/Asian British: Chinese and Other&quot; ## [28] &quot;Persons who are Black/African/Caribbean/Black British&quot; ## [29] &quot;Persons who are Arab or from other ethnic groups&quot; ## [30] &quot;Persons whose country of birth is the United Kingdom or Ireland&quot; ## [31] &quot;Persons whose country of birth is in the old EU (pre 2004 accession countries)&quot; ## [32] &quot;Persons whose country of birth is in the new EU (post 2004 accession countries)&quot; ## [33] &quot;Main language is not English and cannot speak English well or at all&quot; ## [34] &quot;Households with no children&quot; ## [35] &quot;Households with non-dependant children&quot; ## [36] &quot;Households with full-time students&quot; ## [37] &quot;Households who live in a detached house or bungalow&quot; ## [38] &quot;Households who live in a semi-detached house or bungalow&quot; ## [39] &quot;Households who live in a terrace or end-terrace house&quot; ## [40] &quot;Households who live in a flat&quot; ## [41] &quot;Households who own or have shared ownership of property&quot; ## [42] &quot;Households who are social renting&quot; ## [43] &quot;Households who are private renting&quot; ## [44] &quot;Occupancy room rating -1 or less&quot; ## [45] &quot;Individuals day-to-day activities limited a lot or a little (Standardised Illness Ratio)&quot; ## [46] &quot;Persons providing unpaid care&quot; ## [47] &quot;Persons aged over 16 whose highest level of qualification is Level 1, Level 2 or Apprenticeship&quot; ## [48] &quot;Persons aged over 16 whose highest level of qualification is Level 3 qualifications&quot; ## [49] &quot;Persons aged over 16 whose highest level of qualification is Level 4 qualifications and above&quot; ## [50] &quot;Persons aged over 16 who are schoolchildren or full-time students&quot; ## [51] &quot;Households with two or more cars or vans&quot; ## [52] &quot;Persons aged between 16 and 74 who use public transport to get to work&quot; ## [53] &quot;Persons aged between 16 and 74 who use private transport to get to work&quot; ## [54] &quot;Persons aged between 16 and 74 who walk, cycle or use an alternative method to get to work&quot; ## [55] &quot;Persons aged between 16 and 74 who are unemployed&quot; ## [56] &quot;Employed persons aged between 16 and 74 who work part-time&quot; ## [57] &quot;Employed persons aged between 16 and 74 who work full-time&quot; ## [58] &quot;Employed persons aged between 16 and 74 who work in the agriculture, forestry or fishing industries&quot; ## [59] &quot;Employed persons aged between 16 and 74 who work in the mining, quarrying or construction industries&quot; ## [60] &quot;Employed persons aged between 16 and 74 who work in the manufacturing industry&quot; ## [61] &quot;Employed persons aged between 16 and 74 who work in the energy, water or air conditioning supply industries&quot; ## [62] &quot;Employed persons aged between 16 and 74 who work in the wholesale and retail trade; repair of motor vehicles and motor cycles industries&quot; ## [63] &quot;Employed persons aged between 16 and 74 who work in the transport or storage industries&quot; ## [64] &quot;Employed persons aged between 16 and 74 who work in the accommodation or food service activities industries&quot; ## [65] &quot;Employed persons aged between 16 and 74 who work in the information and communication or professional, scientific and technical activities industries&quot; ## [66] &quot;Employed persons aged between 16 and 74 who work in the financial, insurance or real estate industries&quot; ## [67] &quot;Employed persons aged between 16 and 74 who work in the administrative or support service activities industries&quot; ## [68] &quot;Employed persons aged between 16 and 74 who work in the in public administration or defence; compulsory social security industries&quot; ## [69] &quot;Employed persons aged between 16 and 74 who work in the education sector&quot; ## [70] &quot;Employed persons aged between 16 and 74 who work in the human health and social work activities industries&quot; There are a couple of things to note here. As you can see, it does make sense to manipulate the variables by their codes (their names can be quite long!). Even more importantly, the variables have different statistical units in which they were measured. Whereas some variables regard to the entire population within OA, some of them refer to households or just a part of the population (e.g. those over 16 years of age). This information is important as we need to take the population size into account; analysing raw statistics without taking population size into account would distort the results. For this reason, each variable needs to be standardized with the correct population group, to ensure consistency across the areas. Standardising our variables, in terms of area and measurement scale, are our next steps in our GDC workflow. Before we standarise our variables, we’ll extract just the three columns that we’ll need from our lookup table - the variable code, respective name and also the statistical unit at which the variable is collected. Refine variable look-up table: # Extract required three columns var_lookup &lt;- var_lookup %&gt;% dplyr::select(VariableCode, VariableStatisticalUnit, VariableName) 3) Standardise variables Before we proceed with our GDC analysis, we first need to standardize the variables between areas. To do this, each variable needs to be divided by the appropriate population for a given area. Before converting raw numbers into proportions, let’s take a look on the frequency of each statistical unit. Count the different types of statistical units within our variables: # Tally the different types of statistical units of our variables count(var_lookup, VariableStatisticalUnit) ## # A tibble: 12 x 2 ## VariableStatisticalUnit n ## &lt;chr&gt; &lt;int&gt; ## 1 Density 1 ## 2 Dwelling 1 ## 3 Employment_16_to_74 16 ## 4 Household 9 ## 5 Household_Spaces 5 ## 6 Not_Students_16_to_74 4 ## 7 Person 20 ## 8 Person_16_Over 8 ## 9 Person_16_to_74 2 ## 10 Person_3_Over 2 ## 11 Pop_Household_16_Over 1 ## 12 SIR 1 We have two variables that we do not need to convert into percentages or proportions because they are already ratios and not raw counts. Firstly, we have a density, in terms of the proportion of persons per hectare. Secondly, there is the Standardized Illness Ratio (SIR), which, as its name suggests, is a ratio. We will still need to go ahead and transform the remaining variables to standardise them by area - luckily our approach will essentially leave these values alone and only normalise those that have a suitable denominator. To do this, we will use a ‘double-loop’ (a loop within a loop) that takes each variable in our cap_oac_variables_std data frame and then standardises it by its appropriate denominator. To faciliate this, we first change the field names of the “potential” demoninators within the cap_oac_variables_std data frame to match the same spelling as those within the var_lookup data frame. This will enable us to use the lookup table within our loop to look up the right denominator to be used with each variable - and then standardise each variable by its respective denominator within the cap_oac_variables_std data frame. In other words, our loop will: Take a variable field within our cap_oac_variables_std data frame Use the variable field name, i.e. the variable code, to look up that specific variable in our var_lookup data frame From the var_lookup data frame, retrieve the statistical unit associated with the data frame Return to our cap_oac_variables_std data frame and divide the variable field by the values held in the respective statistical unit field contained (within the cap_oac_variables_std data frame) for each observation (i.e. OA) in our dataset. This loop therefore makes sure each variable is standaridsed by the appropriate denominator. Standardise our variables by the appropriate denominator. # Duplicate our cap_oac_variables for data standardisation cap_oac_variables_std &lt;- cap_oac_variables # Change the names of the &#39;potential&#39; denominators within our # cap_oac_Variables_std to match up with the ones used in the lookup table These # columns are the 9-11 columns in the cap_oac_variables data frame colnames(cap_oac_variables_std)[2:11] &lt;- c(&quot;Person&quot;, &quot;Household&quot;, &quot;Dwellings&quot;, &quot;Household_Spaces&quot;, &quot;Person_16_Over&quot;, &quot;Person_16_to_74&quot;, &quot;Not_Students_16_to_74&quot;, &quot;Employment_16_to_74&quot;, &quot;Households_16_and_over&quot;, &quot;Person_3_Over&quot;) # For each denominator in our cap_oac_variables_std data frame for (denom in colnames(cap_oac_variables_std)[2:11]) { # Create a list of associated variables for that given statistical unit (j) stat_unit_var_list &lt;- var_lookup[11:70, ] %&gt;% filter(VariableStatisticalUnit == denom) %&gt;% dplyr::select(VariableCode) # For each (oac) variable in our statistical unit list We call it oacvar simply # because we cannot use simply var or variabe in our code These are &#39;reserved&#39; # variable names used by R, and not &#39;temporary&#39; variable names we can use for (oacvar in stat_unit_var_list$VariableCode) { # Take this variable from the list (oacvar) and divide it by the required # statistical unit (denom) Store this result and overwrite preexisting values for # that variable cap_oac_variables_std[, oacvar] &lt;- cap_oac_variables_std[, oacvar]/cap_oac_variables_std[, denom] } # End 2nd loop } # End 1st loop If you check your resulting cap_oac_variables_std data frame, you should see that we have much smaller numbers in our variable values - we’ve successfully normalise our variables by area. We can go ahead and remove the denominator fields from our cap_oac_variables_std data frame as we no longer need them. Remove the denominator fields from our cap_oac_variables_std data frame: # Remove denominators from the data frame, as we no longer need them cap_oac_variables_std &lt;- cap_oac_variables_std[, -c(2:11)] We’ve now standardised our variables by area - but we also need to standardise them by “themselves” to place them on the same measurement scale. Before we do this, we want to transform our data to account for outliers in each variable’s distribution; specifically, some variables are subject to presence of very high value outliers (e.g. k008, k022, k034). To reduce the impact of these outliers, we will employ Inverse Hyperbolic Sine Transformation (IHS). We’ll use this transformation approach as it was found the most effective for creating OAC2011. There are other transformations that could be employed; for instance, the original OAC2001 utilized simple logarithmic transformation. The formula for IHS is as follows: \\[ \\log(x_{i} + (x_{i}^2 + 1)^{1/2}) \\] We’ll go ahead and transform our data to reduce the impact of outliers. Transform variables using the IHS: # Transform variables using IHS for every variable (bar the OA code!) cap_oac_variables_std[, -1] &lt;- log(cap_oac_variables_std[, -1] + sqrt(cap_oac_variables_std[, -1]^2 + 1)) Finally, before undertaking cluster analysis, we will standardize the variables into the same “measurement scale”. While standardization between areas ensures that population size is taken into account, standardization between variables allows for an easier comparison of variables with one another and an easier interpretation of results. The method that we will use is range standardization, which compresses the values into a range of 0 to 1. Standardise our variables using the range standardization approach: # First create a function for calculating range range_function &lt;- function(x) { (x - min(x))/(max(x) - min(x)) } # X =&gt; apply function (FUN) on the entire data frame but the first column MARGIN # =&gt; function will be applied over columns (1 indicated rows) FUN =&gt; function to # be applied over the data cap_oac_variables_std[, -1] &lt;- apply(X = cap_oac_variables_std[, -1], MARGIN = 2, FUN = range_function) Great - we now have our variables ready for analysis. 4) Measure variables for association (multi-collinearity) Before we go ahead and run our cluster analysis, we first need to check whether any of our variables are strongly correlated with one another or not. Whilst the purpose of the classification is to deal with the multidimensionality of the input data, the more variables we use, the harder it is to obtain efficient clusters. As a result, including variables that are similar to one another adds additional complexity without improving our understanding and/or explanation of the resulting clusters. We therefore should test our variables for multicollinearity and determine which should or should not be included within our final cluster analysis. To test for multi-collinearity, we can create a correlation matrix to test for association; within R, we can use the rcorr() function within the Hmisc package you installed earlier. Create a correlation matrix of our variables: # Create a correlation matrix using the rcorr function Only include the variables # in our matrix! cor &lt;- rcorr(as.matrix(cap_oac_variables_std[2:ncol(cap_oac_variables_std)])) We could print out the correlation matrix, but as it’s quite big (60 columns x 60 rows x 2 dimensions [correlation coefficient and p-value]), we’ll utilise a different approach. We will filter the matrix based on the correlation coefficient values. Before that, we will apply a flattenCorrMatrix() function found on STHDA that will transform the matrix to a long format that will ease our processing of the resulting matrix. Flatten our correlation matrix: # Flatten our correlation matrix flattenCorrMatrix &lt;- function(cormat, pmat) { ut &lt;- upper.tri(cormat) data.frame(row = rownames(cormat)[row(cormat)[ut]], column = rownames(cormat)[col(cormat)[ut]], cor = (cormat)[ut], p = pmat[ut]) } # Apply the function and present only rows that have the correlation coefficient # higher than 0.7 and arrange it by the coefficient values= flattenCorrMatrix(cor$r, cor$P) %&gt;% filter(cor &gt; 0.7) %&gt;% arrange(desc(cor)) ## row column cor p ## 1 k039 k055 0.8596370 0 ## 2 k022 k023 0.8561901 0 ## 3 k012 k020 0.8065253 0 ## 4 k010 k041 0.7734631 0 ## 5 k027 k041 0.7731676 0 ## 6 k010 k031 0.7659194 0 ## 7 k010 k027 0.7353053 0 ## 8 k015 k023 0.7308312 0 ## 9 k031 k041 0.7263128 0 ## 10 k030 k034 0.7127536 0 ## 11 k027 k031 0.7052124 0 ## 12 k039 k059 0.7023037 0 As a rule of thumb, variables for which correlation coefficient is greater than 0.8 are considered to be highly correlated. In our case, we have 3 cases of highly correlated variables: k012 (% of people who are white) and k020 (% of people born in the UK or Ireland) k039 (% of people aged over 16 whose highest level of qualification is Level 4 qualifications and above) and k055 (% of employed people between 16 and 74 who work in the information and communication or professional, scientific and technical activities industries) k031 (% of households who own or have shared ownership of property) and k041 (% of households with two or more cars or van) Moreover, 2 variables that have a correlation higher than 0.7 with at least 2 other variables: k004 (Persons aged 45 to 64) k023 (% of people whose main language is not English and cannot speak English well or at all) With this in mind, and checking the remaining correlations for “uniqueness”, we’ll exclude the following 5 variables: k020, k039, k041, k004 and k023. Remove those variables that are not unique from our final cluster data frame: # Remove 5 variables from our final data for cluster analysis cap_oac_cluster_data &lt;- cap_oac_variables_std %&gt;% dplyr::select(-c(k020, k039, k041, k004, k023)) We now finally have our data ready for analysis. 5) Choose clustering method Once we have our list of variables ready, we can proceed with cluster analysis. Importantly, the segmentation will depend on both clustering method and number of clusters. We’ll go ahead and use our K-Means clustering algorithm to segment our data. As you will have heard in the lecture, the method partitions objects into a pre-defined number of ‘seeds’ (\\(k\\)) that are randomly placed in multidimensional space in a way that a point is assigned to the nearest seed. Once every object is allocated to a cluster seed, the means of clusters are calculated and observations are re-assigned again to the nearest mean (Petersen et al., 2011). The process continues until the convergence is met, where data points do not change their cluster allocation, indicating minimised within-cluster heterogeneity. Here’s how this process looks: How the K-Means Algorithm works To run cluster analysis, we will use kmeans() function from the stats library. To run the algorithm, we provide the function with all the data bar the OA codes and also stipulate the number of clusters we’d like the algorithm to find. Run the k-means clustering algorithm on our final data frame of variables: # Exclude OA variable kmeans_data &lt;- cap_oac_cluster_data[, -1] # Select number of clusters, we&#39;ll choose 9 No_clusters = 9 # Ensure the consistency of the results Because each run of the k-means function # divides the observations into slightly different clusters, we want to make sure # that ours will be the same every time we run it set.seed(16) # Run k-means clustering Km &lt;- kmeans(kmeans_data, No_clusters, nstart = 25, iter.max = 1000) # Extract results as a vector with cluster scores for each OA KmClusters &lt;- as.matrix(Km$cluster) KmClusters &lt;- as.data.frame(KmClusters) # Create a table of the cluster allocation table(KmClusters) ## KmClusters ## 1 2 3 4 5 6 7 8 9 ## 282 471 199 79 447 233 159 522 149 We can already see we have some disparities in cluster size - with cluster 4 being significantly smaller than the other 8 clusters. Before we go ahead and map these allocations, we should assess the fit of the clustering and whether 9 was an appropriate cluster number amount. 6) Choose number of clusters We grouped our observations into 9 clusters. Why 9, and not 8 (like in 2011 OAC) or even 10? There is really no right number of clusters. We can make some judgements on the appropriateness of each number of clusters, but the final number of clusters with your classification is ultimately still subject to your own decision. Nevertheless, generally speaking we want our clustering to achieve: Distinctiveness of each cluster group Relatively high homogeneity (as high as possible) of cluster groups Even distribution of cluster groups Moreover, the size of the cluster should have enough cases to be meaningful, although not too many so that its homogeneity is poor. As a rule of thumb, we aim for each cluster to have between 4% and 20% of total number of observations. Let’s find out how the cluster allocation looks in our case. First, we’ll look at how many OAs each of our clusters represent as a percetnage. Extract the percentage of OAs each cluster represents: # Extract OA percentage of each cluster prop.table(table(KmClusters)) ## KmClusters ## 1 2 3 4 5 6 7 ## 0.11097993 0.18536009 0.07831562 0.03109012 0.17591499 0.09169618 0.06257379 ## 8 9 ## 0.20543093 0.05863833 As you can see, there is one cluster which consists of less than 4% observations, but the other clusters look alright. Now, let’s check how homogeneous the clusters are in terms of their variation from . To assess their homogeneity, we look at the Within Cluster Sum of Squares (WCSS) is a measure of the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares. Clusters that have higher values exhibit greater variability of the observations within the cluster. We therefore want to have clusters that demonstrate lower WCSS and we can use this analysis to helps us determine the appropriate number of clusters. Essentially we can do a trial and error approach and calculate this WCSS value for multiple cluster numbers. The idea is to use the ‘elbow’ approach where you look for a characteristic bend in the statistic - such bend implies that the additional group does not significantly minimise the WCSS. Calculate and plot the WCSS of our current cluster results: # Calculate WCSS of our cluster data wcss &lt;- (nrow(kmeans_data) - 1) * sum(apply(kmeans_data, 2, var)) # Perform a cluster analysis with a subsequent number of clusters (from 2 to 15) # and retrieve the WCSS value for each for (i in 2:15) { wcss[i] &lt;- sum(kmeans(kmeans_data, centers = i)$withinss, maxiter = 200) } # Plot the results to these analysis to determine if there is an elbow plot(1:15, wcss, type = &quot;b&quot;, xlab = &quot;Number of Clusters&quot;, ylab = &quot;Within groups sum of squares&quot;) To choose a number of clusters, we use the ‘elbow’ approach, in which we’re looking for a number of clusters where an additional group does not minimise the WSS by a significant amount. It seems that between 8 or 9 clustesr our WCSS value starts to level off - as a result, we’ll continue with our original choice of 9 clusters. You can of course choose a different value if you’d like for classifying our data. To visually assess the fit of the clustering, we can go ahead and use the plotcluster() function from the fpc library, which plots the one-dimensional data for each of our clusters. Plot our clusters by the one-dimension “location” of our data: # Plot our clusters in one-dimension plotcluster(kmeans_data, Km$cluster) From our plot, we can see why Cluster 4 is one of our smallest clusters - the data is not exactly “compact” - and might reveal some outliers in our classification that we may need to consider. This could flag that our chosen variables are not as useful as we’d hoped they would be. The only real approach to understanding this is to finally map our clusters in their geographic locations. 7) Interpret findings, test and finalise them As we have examined the fit of the clusters, it is now time to plot the cluster assignment over the area of interest. Firstly, let’s add the clusters allocation to the final datasets and merge the dataset with the spatial data. Add cluster allocation to our final cluster data (i.e. with OA codes!), rename our cluster field and then join to our OA administrative boundaries: # add the cluster group to the dataset (treat it as a factor, rather than # intiger) cap_oac &lt;- as.data.frame(cbind(cap_oac_cluster_data, as.factor(Km$cluster))) # change name of the variable names(cap_oac)[ncol(cap_oac)] &lt;- c(&quot;class&quot;) # merge with boundary data cap_oac_shp &lt;- left_join(cap_oa, cap_oac, by = c(oa11cd = &quot;OA&quot;)) We can now go ahead and plott our final classification - to help with our interpretation, we’ll specify the colour palette we want tmap to use, as we have done in other practicals. Specify our colour palette and map our GDC: # Specify our palette for our clusters MyPalette &lt;- c(&quot;#8dd3c7&quot;, &quot;#ffffb3&quot;, &quot;#bebada&quot;, &quot;#fb8072&quot;, &quot;#80b1d3&quot;, &quot;#fdb462&quot;, &quot;#b3de69&quot;, &quot;#fccde5&quot;, &quot;#d9d9d9&quot;) # Map our cluster classifications tm_shape(cap_oac_shp) + tm_fill(&quot;class&quot;, palette = MyPalette) + tm_compass() + tm_layout(frame = FALSE) ## Warning: The shape cap_oac_shp is invalid. See sf::st_is_valid As you can see, the majority of space is taken up by our second cluster. Interestingly, it is not the most numerous cluster in the classification (refer back to the cluster allocation table) - but, due to the size of the OAs, it seems to dominiate our classificatin visually. It’s highly likely that cluster 2 has something to do with those living in rural areas! Whilst the rural areas within the county is relatively homogenous, you can certainly notice that there is greater variability across the clusters within the more urban areas of Cambridge and Peterborough. Let’s go ahead and focus on Cambridge to analyse its classification further, Filter our spatial data frame before mapping to map only those OAs in the city of Cambridge: # Map only Cambridge OAs tmap_mode(&quot;plot&quot;) tm_shape(cap_oac_shp %&gt;% filter(lad11cd %in% c(&quot;E07000008&quot;))) + tm_fill(&quot;class&quot;, palette = MyPalette) + tm_compass() + tm_layout(frame = FALSE) # See the cluster allocation for Cambridge summary(cap_oac_shp %&gt;% filter(lad11cd %in% c(&quot;E07000008&quot;)) %&gt;% dplyr::select(class)) ## class geometry ## 9 :141 MULTIPOLYGON :372 ## 3 :138 epsg:27700 : 0 ## 7 : 62 +proj=tmer...: 0 ## 6 : 21 ## 2 : 6 ## 8 : 4 ## (Other): 0 For those of you unfamiliar with the city of Cambridge, it is obviously quite a diverse city - and we can tell this just from looking at our GDC. As you can imagine, the University tends to dominate the city in terms of its city centre population, which is likely to be our cluster 4, located in the north-west of the city. We then have more industrial areas, such as cluster 6, where the airport is located near Teversham. In comparison to the rest of the county, we also have no (potentially rural!) OAs within our clusters - not too much of a surprise there! I’d highly recommend using the interactive version of tmap and setting the alpha of our polygons to a semi-transparent to investigate our GDC further. In comparison, we can map the city of Peterborough and compare the clusters within and near the city. Filter our spatial data frame before mapping to map only those OAs in the city of Peterborough: # Map only Peterborough OAs tmap_mode(&quot;plot&quot;) tm_shape(cap_oac_shp %&gt;% filter(lad11cd %in% c(&quot;E06000031&quot;))) + tm_fill(&quot;class&quot;, palette = MyPalette) + tm_compass() + tm_layout(frame = FALSE) # See the cluster allocation for Peterborough summary(cap_oac_shp %&gt;% filter(lad11cd %in% c(&quot;E06000031&quot;)) %&gt;% dplyr::select(class), maxsum = 9) ## class geometry ## 1:100 MULTIPOLYGON :604 ## 2:112 epsg:27700 : 0 ## 3: 1 +proj=tmer...: 0 ## 4: 74 ## 5: 29 ## 6:130 ## 7: 39 ## 8:117 ## 9: 2 You can see that Peterborough is even more diverse than Cambridge - it has nearly all of our cluster groups present within our classification (except cluster 4!), including what is likely to be rural areas. Moreover, it seems that they are more evenly spread, with 4 of them having around 100-120 observations. Interestingly, the least numerous groups here are the ones that are most numerous in Cambridge. This simply means that Peterborough and Cambridge are quite different from each other. Giving Identities to Our Clusters Whilst spatial distribution can help us understand certain characteristics about our clusters (e.g. urban, rural, suburbs), we can also look at the variables that constitute our various clusters to help identify a common cluster identity. To do this, we can utilise the cluster centres, as they indicate the coordinates of the centroid for each cluster group once the k-means had reached its optimum solution. They are, therefore, a good indicator of the average characteristics of each group based on the \\(n\\) variables that were included in the original model. To understand our cluster centres and their relation to the overall dataset, we need to calculate the difference between them and the global mean of the dataset. Once we calculate this difference, we will be able to understand quantitatively how unique each group is relative to the whole sample. Calculate the difference between our cluster means and the global mean of our whole dataset: # Extract the cluster centres from our K-Means output Store asa dataframe KmCenters &lt;- as.data.frame(as.matrix(Km$centers)) # Calculate the global mean for each variable Store as a data frame global_mean &lt;- data.frame(lapply(KmCenters, mean)) # Creates empty data frame that has variables&#39; names as row names And will store # the clusters as fields clusters_mean &lt;- data.frame(row.names = colnames(cap_oac)[2:(ncol(cap_oac) - 1)]) # For each cluster in our total number of clusters for (i in 1:No_clusters) { # Retrieve the cluster&#39;s center characteristics class_means &lt;- KmCenters[i, ] # And calculate difference from the mean for each of our varalbes diff_from_global_mean &lt;- as.data.frame(t(global_mean - class_means)) colnames(diff_from_global_mean) &lt;- c(paste(&quot;Cluster_&quot;, i, sep = &quot;&quot;)) # Store the results for each cluster group clusters_mean &lt;- cbind(clusters_mean, diff_from_global_mean) } # Create global mean line clusters_mean$mean_zero &lt;- 0 You can go ahead and open the clusters_mean variable using the View(clusters_mean) command in your console - you’ll see it is a large table that is likely going to take some time to analyse for our 55 variables and it is not immediately clear what our groups represent. One approach to make this interpretation easier is to create charts to visualise the characteristics of each cluster group. In the example below we will create radial plots, using the first group as our example. Plot the radial plot of our first cluster group: # Creats a polygon(p) radial plot for cluster 1 radial.plot(clusters_mean[, 1], labels = row.names(clusters_mean), boxed.radial = FALSE, show.radial.grid = TRUE, line.col = &quot;blue&quot;, radlab = TRUE, rp.type = &quot;p&quot;) This certainly is a little easier to understand than our table, as we can quite quickly see the differences in our variables across our cluster. We can also add our cluster mean zero line to our radial plots to make their interpretation easier. Add the zero mean line to our radial plot: # Reduce the size of grid and axis labels in upcoming plots par(cex.axis = 0.8, cex.lab = 0.8) # create a radial plot for the mean zero line radial.plot(clusters_mean$mean_zero, # plot global mean for each variable labels=rownames(clusters_mean), # variables names as labels start=0,clockwise=TRUE, # start from 3 o&#39;clock and go clockwise rp.type=&quot;p&quot;, # just line #these defines the aesthetics line.col=&quot;red&quot;,lwd=2, show.grid=TRUE, point.symbols=16, point.col=&quot;black&quot;,grid.col=&quot;grey&quot;, show.centroid=FALSE, mar=c(3,0,3,0), #specify margins of the plot radial.lim=range(-0.5,0.5)) # define limits of the grid # Creats a polygon(p) radial plot for cluster 1 # moves the grid labels to position 3 (centre to top) radial.plot(clusters_mean[,1], labels=row.names(clusters_mean), start=0, clockwise=TRUE, boxed.radial = FALSE, show.radial.grid = TRUE, line.col = &quot;blue&quot;, radlab = TRUE, rp.type = &quot;p&quot;, add=T) That helps a lot with identifying the most “unique” variables for our cluster. Obviously it would help to have our variables listed as their name rather than their code to help with their interpretation, but for each cluster, we can look directly at the varaibles that seem to make each cluster unique. In this case, k027 (Households who live in a detached house or bungalow) and k010 (Persons aged over 16 who are married or in a registered same-sex civil partnership) are substantially below the mean, whilst k029 (Households who live in a terrace or end-terrace house), k030 (Households who live in a flat) and k009 (Persons aged over 16 who are single). Just by looking at these five variables, we can start to build a identity of those living in these clusters. I’m pretty sure if you repeated the plots for cluster 4, you’d see our density variable coming into play within that cluster and its classification! If you would like, you can repeat these radial plots for each of your clusters and save them into variables. You can then create a facet map showing the 9 different clusters stacked together. Naming our clusters and their identity Once you begin to build a picture of the identity for each cluster through this variable analysis, the next step is to formulate a name that separates that cluster from the others within your classification. Formulating names for geodemographic groups is not easy and can be contentious. Essentially the names are derivative of the researcher’s perceptions of the cluster centres and sometimes also consider their geographic distributions. Names should be informative, short and memorable. They should also not make unnecessary assumptions or be derogatory in any way. Look at the radial plot for our cluster above - could you think of a name for this cluster quite easily? At this stage, I would certainly think not! But, once you’ve spent time with your classification and looking within and between your clusters, you will begin to identify what makes each cluster unique. However, one thing to note, is that it is also not uncommon for one of your cluster groups to represent the typical traits of the entire dataset. A group like this would demonstrate variable mean differences that are close to 0. To get an idea of the names and descriptions (pen portraits) that are used in geodemographic classification, scroll back to the examples given earlier - or head online and read some of the papers from the various authors who created these examples! The Geodemographics Classification Quiz Before you panic, no this is not an assignment, but rather an optional quiz that you can read through or (maybe!) even submit answers to in order to test your comprehension on this week’s topic of Geodemographics. You can access the quiz here. The quiz is quite long, so don’t feel like you need to answer the questions fully (it’s not marked, but I’m happy to give you feedback on your answers - just ask me to look at your specific quiz!). You can again just read through the questions and if you feel like you wouldn’t be able to answer them, it might be worth going back through the workshop’s material and revising the content presented. Extension: Additional Indices for use within Geocomputation We have utilised Index of Multiple Deprivation (IMD) on a few occasions - but have you yet consider what the Index is? According to those behind the Index, aka the Department for Communities and Local Government, the Index of Multiple Deprivation is the official measure of relative deprivation for small areas1 (or neighbourhoods) in England. The IMD ranks every small area in England from 1 (most deprived area) to 32,844 (least deprived area). To help with its interpretation, deprivation ‘deciles’ are published alongside ranks. Deciles are calculated by ranking the 32,844 small areas in England from most deprived to least deprived and dividing them into 10 equal groups. These range from the most deprived 10 per cent of small areas nationally to the least deprived 10 per cent of small areas nationally. It is common to describe how relatively deprived a small area is by saying whether it falls among the most deprived 10 per cent, 20 per cent or 30 per cent of small areas in England (although there is no definitive cut-off at which an area is described as ‘deprived’). The IMD is a relative measure as a result can only be used to describe the relative level of deprivation in an area. It brings together variables across seven domains, including: Income; Employment; Education; Skills and Training; Health and Disability; Crime; Barriers to Housing Services; Living Environment, which are assigned a different weighting. It can be used for: Comparing small areas acorss England (i.e. compare ranks) Identifying the most deprived areas (using the deciles or percent approach) Explore the different domains or types of deprivation that constitute the measure Compare relative change between versions It can’t be used to “quantify” how deprived a small area is - this means it can tell you if one area is more deprived than another but not by how much. Nor can you identify deprived people (i.e. encountering ecological fallacy issues), conversely look at how affluent an area is (it is a holistic measure of deprivation, not just wealth) or compare to other small areas in other UK countries (as they use different methodologies). All of the information above was extracted from this really useful guide on the IMD by the DCLG. There are also supplementary indices concerned with income deprivation among children (IDACI) and older people (IDAOPI). I would highly advise taking some time to read into the IMD as a good example of how to construct an Index. In addition to the Index of Mulitple, there are several other indices that you might be interested in looking at, for example, for your coursework. We’ve already come across the Public Transport Accessibility Levels (PTAL), developed by Transport for London (TFL). The PTALS are a detailed and accurate measure of the accessibility of a point to the public transport network, taking into account walk access time and service availability. The method is essentially a way of measuring the density of the public transport network at any location within Greater London. You can find out more about the index and download it directly for London here and here respectively - and interestingly enough, for those of you looking at something like this for your dissertation, it outlines a few of the limitations the index faces in measuring accessibility. A final index to be aware of is the recently created Access to Healthy Assets and Hazards (AHAH) by The Geographic Data Science Lab at the University of Liverpool. According to the CDRC website, the AHAH is a multi-dimensional index for Great Britain measuring how “healthy” neighbourhoods are derived from data on: Access to retail outlets (fast food outlets, pubs, off-licences, tobacconists, gambling outlets) Access to health services (GPs, hospitals, pharmacies, dentists, leisure services) Air quality (green space, air pollution) Access to natural environment (green spaces including parks and recreational spaces, blue space including rivers, canals and lakes). The aim of the index is to enable researchers and policy makers to understand which areas have poor environments for health and helps to move away from treating features of the environment in isolation to provide a comprehensive measure of neighbourhood quality. You can read more about the development of this index by Daras et al (2019) here and on the CDRC website here. Recap - Geodemographics This week, we’ve looked at geodemographics, which is the ‘analysis of people by where they live’ (Harris et al. 2005). As we’ve heard - and put into practice - a GDC involves the classification of geographical areas based on the socioeconomic characteristics of their residents (Cockings et al, 2020). To summarise the general geodemographic classification “GDC” workflow succintly, a set of variables are selected by the researcher that represent the characteristics by which the neighbourhoods should be classified. An algorithm is then used to cluster neighbourhoods together that have similarities across these variables. We, as researchers, assigns these result clusters identities by giving them labels (names) and associated “pen profiles” that will qualitatively describe the variables seen within the cluster. In most cases, the final output of a GDC is a map highlighting these different clusters and their location with one another. In almost every case, geodemographic groups should display some spatial pattern, particularly within urban areas; this is because of the tendency for social groups to cluster. From our practical, we looked at how we can create a geodemographic classification with a set number of distinctive groups for a specific local area - in our case, the county of Cambridgeshire and Peterborough. At each of the steps, we had to make analytical methodological decisions on how to proceed within the GDC workflow. In this practical, we used a “best guess” approach that followed previous literature to create our GDC, in order to demonstrate the techniques for you. In “proper” academic research, each step should be very carefully thought through in order to produce the optimum classification to be used for a particular purpose. For instance, the inclusion of a single additional variable may influence which cluster a particular case is assigned to, whilst we may see, as in our case with Cambridge, there are local processes at play that will ultimately shape the outputs of our clusters (e.g. our small cluster 4). If you are interested in GDC as a technique for your dissertation, I highly recommend further reading on the theory and statistics behind techniques used in geodemographic clustering. Learning Objectives You should now hopefully be able to: Explain what is a Geodemographic Classification (GDC). Access and utilise already existing GDCs. Describe the typical GDC workflow and its limitations. Implement a GDC classification in R. Recall several pre-created GDCs and Indices that are available to use in spatial analysis. Acknowledgements The practical for this week was created by Jakub (Kuba) Wyszomierski, a PhD candidate at UCL and Geocomputation PGTA, and can be found here. The practical has been expanded upon as and where necessary, thank you Kuba for this week’s work! The content of this page is also adapted from GEOG0114: Principles of Spatial Analysis: Geodemographics by Dr Joanna Wilkin (this Workbook’s author) and Dr Justin Van Dijk at UCL (the author of the week’s practical within GEOG0114) and Creating a Geodemographic Classification Using K-means Clustering in R by Dr Guy Lansley and Professor James Cheshire for the CDRC. The datasets used in this workshop (and resulting maps): Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] (Open Government Licence) Plus of course, the additional GDCs shown earlier on in the workbook and as first attributed in their relevant section. Output Area Classification: CDRC 2011 OAC Geodata Pack by the ESRC Consumer Data Research Centre; Contains National Statistics data Crown copyright and database right 2015; Contains Ordnance Survey data Crown copyright and database right 2015. England &amp; Wales Workplace Zone Classification: 2011 COWZ-EW Geodata Pack by the University of Southampton; Contains public sector information licensed under the Open Government Licence v2.0. Contains National Statistics and Ordnance Survey data © Crown copyright and database right 2015. Internet User Classification: Alexiou, A. and Singleton, A. (2018). The 2018 Internet User Classification. ESRC CDRC. Contains National Statistics data Crown copyright and database right (2017); Contains Ofcom data (2016). Contains CDRC data from Data Partners (2017). "],["spatial-regression-models-optional.html", "11 Spatial Regression Models (Optional)", " 11 Spatial Regression Models (Optional) This content is optional - we will aim to release this within two weeks at the end of term for use in your coursework and dissertation. "],["accessibility-network-analysis-optional.html", "12 Accessibility &amp; Network Analysis (Optional)", " 12 Accessibility &amp; Network Analysis (Optional) This content is optional - we will aim to release this within two weeks at the end of term for use in your coursework and dissertation. "],["for-loops-and-functions-for-automated-data-processing-visualisation-optional.html", "13 For Loops and Functions for Automated Data Processing &amp; Visualisation (Optional)", " 13 For Loops and Functions for Automated Data Processing &amp; Visualisation (Optional) This content is optional - we will aim to release this within two weeks at the end of term for use in your coursework and dissertation. "],["assessment-information.html", "Assessment Information Useful additional resources", " Assessment Information Geocomputation is assessed through two separate Assessments: Social Atlas: The first assessment will involve the completion of a spatial analysis project, based on the theory, concepts and application learnt during the module. For this coursework you are required to create a small “social atlas” on a topic or area that interests you. Exam: The second assessment will take the form of an Exam, the exact format to be confirmed. More information on your Assessments will be provided at the end of Week 5 (i.e. 12th February 2021). Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out that may be useful for your first Assessment: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 "],["extra-resources-for-help-with-quantitative-dissertations.html", "Extra resources for help with Quantitative Dissertations", " Extra resources for help with Quantitative Dissertations This page is provided for those on the Geography UG degree programme, who are looking to follow a quantitative approach to their dissertation. The page provides general guidance on how to think through preparing your Dissertation outline as well as a list of links to either data portals or potential datasets that are openly available and therefore may be of interest for your dissertations. The list is not exhaustive nor do you need to use data from this list - it is simply provided as a resource. This is applicable to the below videos - these are not part of your Practice Of Geography module, but simply additional content produced to help you get into the right mindset when it comes to developing your dissertation outline and proposal. Video Guidance on Quantitative Dissertations The following videos are provided simply as general guidance to help with completing your Dissertation Outline / Proposal. They are not mandatory for you to watch, nor are they exhaustive of everything you should be considering in your outline/proposal write-up. Thinking Through Your Dissertation Outline This video provides general guidance to help think through your dissertation outline. I also recommend watching it prior to scheduling a meeting with a member of staff as these are the questions they are likely to ask you, so please come prepared. Common Quantitative Approaches in Geographical Research This video outlines the three approaches you can take in quantitative research currently. However for an Undergraduate Dissertation I only recommend two out of the three approaches: Common Mistakes in Quantitative Dissertations This video lists common mistakes made when you start working on your dissertation. To be updated Fri 5th Feb Dataset Guidance Openly Available Datasets From the CASA0005 repository. This is by no means an extensive data list, but summarises data used within some of the practicals alongside a few additions that you might want to explore when sourcing data for your dissertation. You are not limited to these datasets for your dissertation. Google dataset search Tesco store data (London) NHS data (ready for R) US City Open Data Census nomis ONS geoportal UK data service ONS Edina (e.g. OS mastermap) Open Topography USGS Earth Explorer Geofabrik (OSM data) Global weather data (points) London data store Air b n b data NASA SocioEconomic Data and Applications Center (SEDAC) UN environmental data explorer World pop World pop github DIVA-GIS DEFRA US Cesus data TFL open data TFL cycling data EU tourism data NASA EARTHDATA Camden air action Kings data on air pollution Uber travel time data Eurostat London Tube PM2.5 levels Bike docking data in an R package UK COVID data R package for COVID data Tidy Tuesday data (although look for spatial data) Correct statistical tests Data from the CDRC UG students can apply to CDRC for some of their Safeguarded data. There is a process to access these datasets, detailed on CDRC website here. To access any CDRC safeguarded data, you will need to follow this process. It normally takes 4-5 weeks for your application to be granted. As part of the process, you will need to say in your application why you want that specific dataset and what you are going to do with it. You will also need to have at least thought about the ethical implications of using that data and provide this with your data application (alongside your standard ethics application). In terms of specific datasets avaiable, you can apply for: Bicycle Sharing System Docking Station Observations CDRC Modelled Ethnicity Proportions - LSOA Geography NHS Hospital Admission Rates by Ethnic Group and other Characteristics Local Data Company - SmartStreetSensor Footfall Data – Research Aggregated data Speedchecker Broadband Internet Speed Tests FCA Financial Lives Survey - currently the 2017 survey, the 2020 survey may be available around May. There is also a substantial amount of open data available via the CDRC. In this case, you can just register on the site and download. This includes the CDRC Residential Mobility Index, a population ‘churn’ dataset, which has recently been reclassified from Safeguarded to Open. Other Data Lists Awesome public datasets have a wide range all data (some geographic, some not). Robin Wilson has authored one of the most extensive data lists that I’ve come across. "],["week-2-practical-alternate-using-agol-for-population-mapping.html", "Week 2 Practical Alternate: Using AGOL for Population Mapping", " Week 2 Practical Alternate: Using AGOL for Population Mapping For this week’s Practical Alternate, we’ll be using ArcGIS Online.The instructions below outline how to complete the same processing as the Q-GIS practical conducts. It is also includes the all extra information included in the Q-GIS tutorial about Attribute Joins and Classification Schemes. One thing I would recommend is to watch the two videos within the practical: a short introduction to Q-GIS and an introduction to Attribute Tables and Properties. These are not included within this practical. A short introduction to ArcGIS Online Feel free to skip this part and head straight to the Sign Up to ArcGIS Online section. What is ArcGIS Online? ArcGIS Online (AGO) is Esri’s “Software-as-a-service” GIS offering, that enables you to conduct some basic (as well as some quite advanced!) spatial analysis, as well as create interactive maps for sharing with others. It has some very similar features to Esri’s GIS Desktop software (ArcMap and ArcPro) discussed in last week’s lecture, but it does not have all of their capabilities, for example, it is not a tool I would use to create paper maps/ones for use in publication. In contrast, it does offer a lot of web interactivity, as we’ll see when we share our maps with one another at the end of the practical. It also has some really useful analysis tools that are quick and easy to use, in compared to their counterparts in the Desktop software, such as creating something called “drive-time” or “network” buffers – we’ll have a look at these next week when looking at spatial properties. The Esri Ecosystem AGO is just one of the may additional tools Esri offers. Their entire ecosystem of products is huge - you can see their list of products here. Whilst many of the products and/or extensions are created for specific industries and purposes, there are other web-based tools that I can recommend you looking into during your time on this course, to at least be aware of the capabilities moving forward. The first would be ArcGIS StoryMaps, where you can create a webpage a bit like the ones you are using for these workshops, but also integrate any maps you make within the page as well! In addition to StoryMaps, Esri has its own survey collector application – ArcGIS Survey123. Within this application, you can create online forms to collect spatial and non-spatial data – which can then be directly used as inputs within AGO or StoryMap applications. You might see why I call this an “ecosystem” – Esri have constructed their software, tools and applications to work well together and sync across their respective platforms (e.g. web, desktop and mobile)! You just need to be able to afford the license to use them in the first place – we have an educational license which enables ArcMap usage, whilst Esri (as you’ll see) offers AGO for free for non-commercial purposes. Using ArcGIS Online – limitations to be aware of! AGO is a very useful solution to conducting GIS and spatial analysis within the Esri ecosystem when you, as an analyst, are in a scenario where computing resources may be restrictive (and therefore downloading Q-GIS, or Esri’s ArcMap or ArcPro is not a good idea) but internet access is ok – or if, for example, you own a Mac and do not want to split your hard drive to install a Windows operating system, or, finally, when Virtual Machine alternatives may not meet your needs. One thing to flag before we get started with AGO though, is that the platform does simplify some aspects of the traditional GIS workflow – for example, defining your Coordinate Reference Systems and Projection Systems (CRS/PS). This will be an issue in next week’s practical - but I will address this in more detail then. Another aspect of using AGO instead of Desktop software is that your data is ultimately hosted on the AGO server, rather than on your hard drive. One critical aspect of GIS is to practice good file management, including establishing a good use of folders and data storage protocols, so you know where to access your data and where your outputs from any analysis are stored. Normally in Desktop GIS, or even in R-Studio, you would establish a project folder, and within this folder create folders for your data, scripts and outputs (e.g. maps, figures). With ArcGIS specifically, you can use geodatabases to store any spatial data you use or create, whilst R-Studio can create a project in which your work will be saved. QGIS in comparison will rely primarily on your use of folders. For AGO, your data and layers will be managed in their server, under your content page - so in a way you still need to organise your files somewhat. Sign Up to ArcGIS Online With all of this in mind, let’s get ourselves set up to continue with the practical! First head to: https://learn.arcgis.com/en/become-a-member/ and fill in your details as below: By signing up here, you will become part of the Learn ArcGIS organisation, which Esri has created to help support teaching of GIS online for non-commercial purposes, i.e. what we’re doing here! Once you’ve clicked on Join, you’ll need to go authorise your account from your UCL email. The sign-up box may not disappear (it did not for me), but check your emails first before clicking on Join again! Once you’ve authorised your account, you’ll be taken to the ArcGIS online home screen – feel free to navigate around the website yourself before starting the practical. Practical Instructions Open your ArcGIS Online (AGO) home webpage and click on the Map tab. This is the main interface we will use to import and analyse our data and is a light version of a traditional Desktop GUI-GIS. Save your map as Population Change in London. You can add as many tags as you like – I used: population | London | analysis. Let’s go ahead and start adding data to our map. Click on the Add button and select Add Layer from File: 3. You should then see the following pop-up: As you can see from the instructions, AGO requires the shapefile to be provided in the zipfile format, rather than the individual files. As a result, what we need to do is navigate to our raw data folder and compress our wardLondon_ward` shapefile to create a zipped version. For now, close down the pop-up. Navigate to your boundaries folder in your file management system and then to 2011 folder. Select all files related to the London_ward shapefile and right-click and select compress or archive (depending on your Operating System): 6. Back in AGO, click back on the Add button and select Add Layer from File. Navigate to your London_ward zipfile and select this as your file to import. + Click the option to ‘Keep original features’ and then import the layer. You should see the data appear on your map as such: The data is current styled according to the different names in our Name field. Before we go ahead and import our population data, let’s first change the symbolisation of our dataset to only a Single Symbol. In the Change Style option appearing on the left of the screen, select the option to show the Location (Single symbol). All of your wards should now be displayed in a single colour – but we would prefer to see them as simple grey polygons with a black outline. Click on the Options button hovering over the Single Location box that should have appeared. Click on the blue Symbols button – for FILL, select a light grey colour, for OUTLINE, choose a colour of your choice and make sure to reduce the transparency of your lines. Once you are happy with your symbolisation, click through (i.e. click the OKs and DONEs) until you are presented with the main screen. You should now see your Ward data in the main map canvas of AGO - you should also see what looks like a table of contents on the left-hand side which now contains the layers for the London_Ward data and the base map. If you hover over the layer, you’ll see the various options we have through AGO to interact with our dataset. These options include: Displaying the legend (i.e. how the data is symbolised) Show table (i.e. displaying the Attribute Table) Change style (i.e. return to the Symbology options you were just using) Perform analysis (i.e. what we’ll use to perform different types of analysis on our layers, including our attribute join) More options (i.e. other tools you might want to use, such as zooming to a layer or saving your content) We’ll utilise a few of these options over the practical. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using AGO right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 ward shapefile to create our Ward Population dataset. To do this, we need to add the 2011 population data to our map. In AGO, import the 2011 population csv from your working folder by using the Add data button as before. Note for csvs, the population data can be imported as the original file and there is no need to zip it. For the csvs, add the layer just as a table. Now we have it loaded on our map, we can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. Use the Show Table option to open the Attribute Tables for both the Ward and Population data layers. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. To create an Attribute Join in AGO, you need to click on the Perform Analysis button when hovering over the London_Ward dataset and then open the Summarise Data drop-down to find the Join Features tool. Click on the Join Features tool and add the appropriate inputs for each step (again make sure you get your target and join layer and their respective fields correct and also select to keep all target features): 6. Click Run Analysis! AGO will then return to the original layer screen and create your new layer! It might take a little time for AGO to create this join - just be patient. But, if, after ten minutes, your join has still not worked, you may download the complete 2011 ward population dataset here. It is provided as a zipfile, which you’ll then need to add/upload to your AGO map. Once AGO has finished processing, the next thing we would like to do with this dataset is to style it by our newly added Population field to show population distribution around London. Hover over your new layer, and then click the Symbology button. Next in Choose an attribute to show choose our POP2011 (population) field. AGO will automatically style your data for you as Counts and Amounts (Size), which is a useful way to view our dataset. This approach is also known as Proportional Symbols. We can see from just this initial styling that there are some differences in population size across our wards. You can click on the Options button to find more ways of altering how your data is currently styled. This also provides you with a histogram of the data (even though it is on its side!) to see how our data is distributed. As we can see, our dataset shows a normal Gaussian distribution. Understanding our data’s distribution is really important when it comes to thinking about how to style and visualise our data as well as understanding what sort of analysis techniques we can apply to our data – more on this next week. Alternatively to the Size option, you can also create a choropleth map from our dataset. Navigate back to the Change Style menu of the Symbology tab (this may involve clicking done to exit the previous menus). Click on Counts and Amounts (Color) – you’ll see the map change automatically to a choropleth map. We can change the colour scheme of our map, as well as the way in which AGO displays the data either via the Theme dropdown or by clicking the Classify Data box. The latter provides you with more control over the data’s classification scheme and details the different types of classification schemes you can use with your data: We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on Natural Breaks and change it to 7 classes. You may also want to reduce the transparency. Then click OK and then DONE. :::note A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using AGO, which makes it a good playground to see how certain parameters or settings can change your output. ::: You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Empty wards in the City of London In our Q-GIS tutorial, we would now go through the steps to exporting the data. When using AGO, we do not need to worry about this at the moment - make sure you save your map, and if you would like you can save your final Layer to your AGO content. To do this: Click on the More Options button when hovering your item and select Save Layer. Name your layer London_Ward_Population_2011 and add a few tags. Click create item. This layer should then appear in your AGO content. When looking at your layer in the AGO Content page (not the Map page we have been using), if you publish your layer, you will created a hosted layer than you can then download as a Shapefile for use within Desktop software etc from the AGO website. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Zip/compress the respective Ward dataset prior to adding it to AGO Add the respective Ward dataset as the zipped file Load the respective Population csv Join the two datasets together using the Join Features tool. Style your data appropriately. Save your joined dataset as a layer within your AGO content. To then make accurate visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with AGO, so for now, if you want, you can leave your symbolisation to the default settings. Alternatively, if you would like to standardise your classification breaks, you’ll need to return to the Classify Data option within the Symbology tab and manually change your breaks here. If you have any issues with AGO and joining your datasets (i.e. the processing takes longer than 10 minutes each), you can download the remaining pre-joined files here. You will need to download, then upload these datasets to style them appropriately for the next step. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint from AGO: Click on Print –&gt; Map with Legend and either take a screenshot or use the File –&gt; Export as PDF and then trim your PDF to the map. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) ArcGIS Online, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. "],["week-3-practical-alternate-using-agol-for-crime-mapping.html", "Week 3 Practical Alternate: Using AGOL for Crime Mapping", " Week 3 Practical Alternate: Using AGOL for Crime Mapping For this week’s alternate practical, we will continue to use AGOL as our main GIS system to process and analyse our data, similar to the Q-GIS practical. This week, compared to last, we will however be dealing with two main ‘compromises’ in our use of AGOL vs. Q-GIS that you will need to be aware of: 1. Projections Within Q-GIS, as you will see if you read through the main practical (which I highly advised doing), setting the Project and Data CRSs are an essential step in successfully analysing and visualisng spatial data correctly. In our case, our practical data uses two CRS - BNG for the administrative boundaries and WGS84 for the crime data. As a result, in the main practical, we use a tool within Q-GIS to reproject our crime data into the same CRS as the administrative boundaries, i.e. BNG. AGOL, in comparision, uses WGS84/Mercator as default CRS for all its data mapping and visualisation - and can only be altered if you change the basemap to a dataset that is in your desired CRS/PS (although in their Beta version, it appears that there will be more user choice over choosing projections). AGOL will convert our data, such as our Administrative Boundaries (which are in British National Grid) “on the fly” to WGS84 - so we do not need to reproject it. However, this will mean we may forgot this step in the future - for example, when using R-Studio instead; therefore it is important to recognise that this aspect of our GIS workflow is missed in this tutorial. 2. Map-Making &amp; Visualisation AGOL also has relatively limited capacity for map-making. As a result, for this practical, I would recommend using a mixture of your output from AGOL alongside either a graphic software or even PowerPoint to make final additions that are needed to your map. You’ll see these recommendations below as my proposed workaround. Detailed cartography is one of the key advantages that Q-GIS and ArcGIS have over the use of programming tools, such as R-Studio. As you’ll see in future practicals, we can still make excellent maps in R-Studio, it just takes a little more time and experience than the “speed” of the traditional GIS software. With all that being said, we still have plenty of data analysis to learn - so let’s get started! Practical Instructions We now have our datasets downloaded and ready to process - we simply need to get them loaded onto our AGO map. Open your ArcGIS Online (AGO) home webpage and click on the Map tab. Save your map as Crime Analysis in London. You can add as many tags as you like – I used: crime | London | analysis. Let’s go ahead and start adding data to our map. Ward Population We already have our ward_population_2019.shp dataset complete from last week, so we can go ahead and add this directly to the map. Click on the Add button and select Add Layer from File: Add your `ward_population_2019.shp’ to the map using the Add -&gt; Add Layer From File tool. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Borough Population We, as yet, do not have a borough_population_2019.shp. To create our Borough population shapefile, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Add the London_Borough_Excluding_MHW.shp file from the 2011 boundary data (in your raw data folder) to your map. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Add the borough_population_2019.csv you have just created from your working folder to your map. This can just be added as a csv, but remember to add just as a table. Join the two datasets together using the Join Features tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the London_Borough dataset. Crime Data We now are ready to load and map our crime data. We will now add our all_theft_2020.csv from our raw folder - we will load this exactly like our previous population csv but this time, when presented with the option, we need to add the point coordinates to map our crime data as points. Before we can load our data, we actually need to do one final step of data cleaning (compared to the Q-GIS tutorial). Unfortunately AGOL cannot handle all of the data from 2020 - so we need to reduce the size of our dataset. For now, we will analyse theft crime for March in 2020. Open your all_theft_2020.csv from our raw folder in your number editing software, and extract all rows that the field Month is equal to 2020-03. I do not mind how you do this, but just make sure to save to a new CSV called: march_theft_2020.csv into your working folder. Once you have extracted this smaller dataset: Click on Add -&gt; Add Layer From File. AGOL should automatically detect the Longitude and Latitude columns and map your data. You may have an error message, but you can ignore this for now. Unlike Q-GIS, we do not need to reproject our data when using AGOL as the software has done this for us - we can, as a result, move on to the next step - counting the number of crimes in each of our Wards and Boroughs respectively. Counting Points-in-Polygons with AGOL The next step of our analysis is incrediby simple - as AGOL has an in-built tool for us to use. We will use the Aggregate Points tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the March_theft_2020 point dataset to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Hover over the March_theft_2020 point dataset and click the Perform Analysis button. Next, click on the Summarise Data option and then Aggregate Points. Set up your query as follows: Point Layer: march_theft_2020 Aggregation areas: borough_population Add Statistics: Field = UID | Statistic = Sum Result Layer Name: borough_march_theft No need to add anything to Option 4 (group by) Click Run Analysis Note, the processing for the borough level will take around 5 minutes to process. Once complete, re-run the same process for the Ward scale - note this will take even longer to process (approximately 10 minutes). Calculating Crime Rate in AGOL Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. We therefore now want to add a Crime Rate statistic to our dataset - we want to normalise our crime count by our population data. Note, if your processing did not work OR is still processing after 10 minutes, you can find two pre-processed Ward and Borough shapefiles with population and crime count here. Let’s go ahead and calculate our Crime Rate statistic. To do this in AGOL, we actually need to access the Symbology menu. Click on the Change Style / Symbology button whilst hovering over your borough dataset that now contains your crime count. In 1: Choose Attribute, click on the drop-down next to the currently selected attribute, scroll to the bottom of the list and click on New Expression: A new pop-up window should appear - this is where we’ll add a new expression to calculate our crime rate. Edit the Custom name to crime_rate. In the expression box, remove the current comments. Add the expression: ($feature.crimecount/$feature.POP2019)*10000 You can double-click on the fields on the right of the box to add these if you want. Click on OK. You’ll now have a new field populated with the crime rate for each borough. Whilst you’re still in the Style tab, go ahead and change the styling to show the crime rate for each borough by creating a choropleth map: Click on Counts and Amounts (Colour) to access the correct style option. You can click on the Classify check box to change the type of classification scheme and the number of classes. Once you’re happy with your styling, click through the OKs and Dones to return to the main AGO map. Now you just need to repeat the above steps for your Ward crime data and we’ll have our maps ready to export. Remember to uncheck the box next to your borough layer, so this data does not show through on your ward map (and make sure you ward map has not shown through on your borough layer for that matter!). As an FYI, we won’t export our data from AGOL as I’ll provide you with the final shapefiles in Week 5 for the practical that week. Just remember to save your map once you’ve exported your maps as instructed below. Making our Crime Rate Maps for analysis in AGOL As stated at the top of this practical, AGOL does not have a huge amount of flexibility when it comes to cartography - so we’ll need to get a bit inventive. To create maps to submit for your assignment, these are the steps I recommend: Click on Print -&gt; Map with Legend and either take a screenshot or use the File -&gt; Export as PDF. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w3. To add the various map components, open up PowerPoint - preferably find a slide size that is wider than it is taller. Insert your two maps onto your slide, placing them side by side. Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We will use PowerPoints text box and shape features to replicate this on our slide. We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on PowerPoint: Add a north arrow: choose an arrow from PPTs shapes and draw it pointing upwards (i.e. north on your map) Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Feel free to customise your font etc. to give the final maps a good aesthetic. Once you have added these properties in, you should have something that looks a little like this: You’ll notice I got a bit creative with cropping my maps in various ways to try to create a similar format to the one I made in the Q-GIS tutorial. The only thing I haven’t managed to add is a scale bar as this would require accuracy in digitising that we do not have in PPT. This is as close as we can get with creating maps using AGO - it would be great if we could edit it further but this is the constraint with using an online tool. Export map Now we have our maps put together, we are finally ready to export our map! Export your slide as a PNG. Remember to also save your original slide. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in AGOL. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. In AGOL, you may find an option that does this for us. If not, you can use the New Expression builder to calculate these values. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "]]
