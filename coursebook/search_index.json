[["index.html", "Geocomputation 2020-2021 Work Book Module Introduction Learning Objectives", " Geocomputation 2020-2021 Work Book Module Introduction Welcome to this year’s Geocomputation module, a course that introduces you to both the principles of spatial analysis and the use of programming for data analysis. Over the next ten weeks, you’ll learn about the theory, methods and tools of spatial analysis whilst implementing small research projects, first using Q-GIS, and then using the R programming language within the R-Studio software environment. You’ll learn how to find, manage and clean spatial, demographic and socio-economic datasets, and then analyse them using core spatial and statistical analysis techniques. The course is an excellent precursor for those of you interested in a career in (spatial) data science! The course will consist of approximately 10 lectures, 10 self-led practicals, 5 seminars (held online) and 5 coding help sessions (held online), further details of which are provided on the next page, Module Information. For now, if you’ve not watched the Introduction video on Moodle, you can catch up below: Geocomputation Introductory Video Remember you must have joined our Geocomputation Team on Microsoft Teams to be able to watch our lecture videos - instructions are provided on Moodle. Learning Objectives As you’ll have read in the Module Catalogue entry, the main learning objectives for the module are as follows: Understand the ways in which digital representations of the observable world are created, and how representations of neighbourhood communities are built from publicly available Open Data. Gain practical experience of the use of analytical methods to profile small areas of London. Understand the nature of geographic data, and the concepts of spatial autocorrelation, modifiable areal units and neighbourhood classification. Understand the sources and operation of uncertainties in the creation of geographic representations, and the importance of generalisation, abstraction and metadata. Gain practical experience of software, map design and visual communication. Develop practical skills in data acquisition and analytics, which may be useful in the planning of dissertations. We hope that you’ll learn many other things during the module and it inspires you to think about how you might use spatial analysis, GIS and programming in your future career! Getting in touch during the module The module is convened and taught by Dr Joanna Wilkin - you can contact her at j.wilkin [at] ucl.ac.uk or, for online office hours, you can book a half hour slot using MS Bookings. The module is further supported by two Postgraduate Teaching Assistants: Jakub (Kuba) Wyszomierski and Nikki Tanu. They will host coding help sessions on the alternative weeks to our scheduled seminar sessions. Acknowledgements Putting together a workbook such as this is no easy feat - but it’s also something that after a little time with R-Studio, you’d be able to produce! The reason for this, as we’ll repeat throughout the course, is that there is an incredible amount of resources available online that can help you learn the skills required to produce a website like this (e.g. using Git with R, using GitHub to host websites, R-Markdown, basic CSS styling). These skills firmly fall outside of the requirements for this course, but something you can build on in your spare time and over your future career. Believe it or not, we as lecturers are also always still learning - particularly, as you’ll find if you continue in spatial data science, the tools and technology available to us is continuously changing! Content-wise, the lectures and practicals for this course are all original this year to the Geography Department at UCL. There is some overlap between this course and the Principles of Spatial Analysis module that is run at the Master’s level, e.g. the extensions offered in several of the practicals. Aesthetics-wise, the R package and analysis artwork used within this book has been produced by allison_horst, whilst much of the artwork used in information boxes has been produced by Desirée De Leon, as well as by Jo. You can find Allison’s images on the stats illustration GitHub repository and Desirée’s on the rstudio4edu GitHub repository. Yihui Xie’s Authoring Books with R Markdown and rstudio4edu’s book, A Handbook for Teaching and Learning with R and RStudio were key resources in the creation and editing of this book. In addition, the CASA0005 practical handbook (by Dr Andy MacLachan and Adam Dennett) alongside our own Principles of Spatial Analysis practical handbook (by myself and Justin van Dijk) served as inspiration for the structure and formatting of this book. For some practicals, additional acknowledgements are made at the end where code or inspiration has also been borrowed! Noticed a mistake in this resource? Please let us know through the GitHub issues tab, send us a message over MS Teams, or contact us by e-mail. "],["module-information.html", "Module Information Self-guided learning for Geocomputation Reading List Troubleshooting Module Content Feedback: Weekly and End of Module", " Module Information If you’ll read one page of this entire workbook in depth, please make sure it is this one!. The following page outlines exactly how we hope you will engage with online learning for this course. Running a practical-based course online is not easy - for both lecturers and students alike! To help, we’ve tried to break down our content into short chunks, using a mix of recorded lecture videos, recorded practical and coding videos, as well as recommended reading and even short explanations of our own. Self-guided learning for Geocomputation The majority of your learning this year for Geocomputation will be ‘self-guided’ - however this is not to say, you’ll be learning alone. We’ll be running fortnightly seminars to check-in on your progress and discuss what you’ve learnt in our small groups (attendance is recorded), whilst also encouraging you to attend (optional) Help/ Study Group sessions on the weeks the seminars are not held. In addition, you’ll have small Assignments to complete, that we’re (time-permitted) hoping to provide you with small feedback on, either during the seminars or through discussion forums (held either on Teams or Moodle, depending on requirements, instructions will be provided as and when). Geocomputation Timetable A typical fortnight for Geocomputation will look something like this: Tuesday 10am: New content is released. Friday 5pm: Post/Send Assignment submissions for online feedback (Optional, but recommended) Monday/Tuesday (allocated slots): Help/Study Group, run by Jakub and Nikki - get help on your practical work if you’ve been unable to submit your work, bring articles you’ve read for discussion, discuss your feedback, and catch-up with friends! Tuesday 10am: New content is released. Friday 5pm: Post/send assignment submissions for seminar (if required) Monday/Tuesday (allocated slots): Seminar, run by Jo - discuss content for the last two weeks, bring articles you’ve read for discussion, discuss your feedback, and ask questions! And repeat! There will of course be a break for Reading Week where we will set you a short Coding Challenge to complete ready for the seminar at the start of the second half of term. In addition to our scheduled fortnightly help sessions, there will also be the weekly Coding Therapy classes run by PhD students within the Department to help you with coding issues if you get stuck and can’t wait for our help sessions. Please do use these classes in addition to our Help Sessions - they are there to help you with any module that requires any type of programming, not just Geocomputation, as well as your Dissertation! We will also provide some additional help sessions in the first two weeks of the Easter break to help you with your first Assessment. The hours for this are TBC. What will I be learning each week? As you’ll soon find out, you will learn a lot of different things in Geocomputation - from spatial analysis techniques to understanding how to write code to process and analyse data, as well as how to organise your investigation and use statistical and spatial analysis to answer research questions. To help, we have broken the course into three main sections: Foundational Concepts Core Spatial Analysis Advanced Spatial Analysis We hope this helps with the various learning curves you’re about to embark on - as outlined and explained further in Week 1’s content. The topics covered over the next ten weeks are: Week Date Section Topic Online Session 1 11/01/2021 Foundational Concepts Geocomputation: An Introduction N/A 2 18/01/2021 Foundational Concepts GIScience and GIS software Seminar 3 25/01/2021 Foundational Concepts Cartography and Visualisation I Help/Study Groups 4 01/02/2021 Foundational Concepts Programming (for Statistical Analysis) Seminar 5 08/02/2021 Foundational Concepts Programming for Spatial Analysis &amp; ESDA Help/Study Groups READING WEEK READING WEEK READING WEEK CODING CHALLENGE - - 6 22/02/2021 Core Spatial Analysis Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Seminar 7 01/03/2021 Core Spatial Analysis Analysing Spatial Patterns II: Clusters Help/Study Groups 8 08/03/2021 Core Spatial Analysis Rasters, Zonal Statistics and Interpolation Seminar 9 15/03/2021 Advanced Spatial Analysis Geodemographics Help/Study Groups 10 22/03/2021 Cartography &amp; Visualisation Cartography and Visualisation II Seminar The lectures and practicals of this course only form a part of the learning process. You are expected to undertake wider reading (see the Reading List below), particularly to help with your second assessment. In addition to our course, there are many other online resources and tutorials that can help expand on the topics and content we cover - whilst it is not necessary for your assessments, you are encouraged to go beyond our recommendations and fully engage with applied GIS research, methods and visualisation techniques. Following certain ‘movers and shakers’ in the GIScience / Spatial Data Science world is one approach to learn more about what’s happening in the field and might prove inspirational for your dissertation ideas later this year. Reading List We link to books and resources throughout each practical. The full reading list for the course is provided on the UCL library reading list page for the course. Alternatively, you can always easily find the link to the Reading List in the top right of any Moodle page for our module, under “Library Resources”. This Reading List will be updated on a weekly basis, in preparation for the week to come, so you may see some weeks without reading for now. But please check back at the start of each week as the lecture, seminar and/or workshop material is released for that week to check for new readings. All reading for that week will be provided by the time your learning materials are released - so you will not need to check the reading list for updates as the week progresses. Troubleshooting Module Content Spatial analysis can yield fascinating insights into geographical relationships. However, at times it can be difficult to work with - particularly when we combine this with learning how to program at the same time. You will get lots of error messages and have software crash, you’ll end up with bugs in your code that are difficult to find, and you may spend a whole day trying to track down a single dataset. But the rewards of learning how to do all of this (particularly with this year’s emphasis on this online research for your dissertations) will become apparent. To bring in my first of potentially several cycling references, a well-known quote from Tour De France (1986, 1989, 1990) winner Greg Lemond: “It never gets easier, you just go faster.” Resonates quite well with spatial analysis and programming! Even after years of programming, we can still forget the syntax to a for loop or question what kernel density estimation actually shows, but you will - by the end of the next ten weeks - know how to find out the answers to these problems faster! Beyond the help sessions mentioned above, if you need specific assistance with this course please: Attend the fortnightly GIS/coding help sessions (from Week 3) to ask questions directly to Jakub or Nikki. Post in the respective tech-help or r-help channels within the Geocomputation Team. Ask a question at the end of a seminar (time-permitting) Check the Moodle assessment tab for queries relating to the assessment (more information will be provided in Week 5) Attend the Coding Therapy sessions that are run on a weekly basis Due to the size of the class we will only reply to tech and R help messages on Teams so all students can see the discussion. If you have a personal matter in relation to completing the course then please speak to or email Jo. We’d also encourage you to monitor the tech-help or r-help channels and contribute to/answer questions as/if you can! Creating a small community across our course will help all of us in the long-run. If after pursuing all these avenues you still need help, you can book into our office hours. These meetings are to discuss a geographical concept in relation to the material/assessment or for any personal matters relevant to the completion of the module. Additional Online Help &amp; Resources We are here to help you work through these practicals but even we do not know everything. Therefore, it’s a good idea to become familar with online sources of help, such as: Stack Exchange RStudio community QGIS documemtation R documentation ArcGIS help pages Ultimately, if you are struggling to use R don’t worry…here is some advice from a tweet and interview with Hadley Wickham, chief scientist at RStudio… You're doing it right if you get frustrated: if you're not frustrated, you're (probably) not stretching yourself mentally — Hadley Wickham (@hadleywickham) 11. Februar 2015 It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. We highly advocate taking a break if you get stuck - the Workbook will not disappear, and you can complete the content at your own pace! Expanding your R learning (to infinity and beyond!) In addition to our course, there are many online tutorials that can help with learning R, specifically: Free RStudio Education resources Codeacademy YaRrr! The Pirate’s Guide to R Feedback: Weekly and End of Module We can only make this course better through your feedback. We collect feedback: At the end of every practical. There is a link to an anonymous feedback form, please let us know if something is unclear and we will go over it in a future session. At the end of the module. The standard UCL feedback form will be available to fill in on Moodle. Acknowledgements Part of this page is adapted from CASA0005. "],["what-is-this-workbook.html", "What is this Workbook? Using this Workbook Workbook Functionality", " What is this Workbook? All course content, including lectures and practical material, will be contained within this Workbook (hence it’s name!). As outlined earlier, you will also need to be part of our Geocomputation Team to have access to the lectures within the workbook. Any official course requirements (e.g. submission links for assessments) will be on Moodle. Each week’s content will be uploaded to the Workbook for Tuesday 10am UK time after the third seminar or coding help sessions. Using this Workbook Alongisde the recorded lectures and practical instructions, key things to look out for in the Workbook are Assignments, which are short optional submissions, Key Reading(s) and Points of Information, including Learning Objectives, Tips and Recap. To help, we’ll try to highlight them as follows: Assignment Each week, you’ll have 1-3 short assignments where we would like you to submit either a response, map or code prior to our seminar session or have it ready to present during the session itself. Key Reading(s) The recommended readings for this week will be highlighted in a Reading Box as and when appropriate in the week’s content. You’ll be able to find direct links to them within the E-Reading list. Learning Objectives Each week, we’ll start with a highlight of the learning objectives we hope you’ll achieve through the practical and lecture content. Get Ahead Tips Tips for effective note-taking during the practicals such as recording the functions you end up using in our practicals and your understanding of the arguments that they require. Recap A recap at the end of each section or week - make sure you take a note of these and are confident that you understand the points addressed. Workbook Functionality To get the most out of this book spend a few minutes learning how to control it, in the top left of this webpage you will see this toolbar: These buttons will let you: control the side bar search the entire book for a specific word change the text size, font, colour propose an edit if you see a mistake that I can review view the webpage in the ‘raw’ RMarkdown format, we cover RMarkdown in the course information about shortcuts for this book and most others like it In addition the icon in the top right of the page takes you to the GitHub repository for this book, where the online files for the book are stored. Acknowledgements Part of this page is adapted from CASA0005. "],["software-installation.html", "Software Installation QGIS R and R-Studio ArcGIS Installation Issues", " Software Installation This course primarily uses the R data science programming language and we strongly advise you complete the assignment using it. We briefly touch upon QGIS in the first few weeks to give you a basic foundation in spatial analysis alongside the range of spatial software available. Please follow the instructions below before completing the first practical session (in Week 2) to install the software on your local computer. Alternatively, both software are available on UCL’s computers and therefore can be accessed through Desktop Anywhere - however depending on your internet connection, this may be slow to use and, as a result, a highly frustrating experience! As outlined below, we have an online version of R-Studio available for use, but as yet, we do not have one for Q-GIS. If you are unable to download Q-GIS for your own computer, please let us know through the form below. QGIS QGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. To get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html We recommend installing the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates. R and R-Studio R is both a programming language and software environment - in the form of R-Studio- originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks. R-Studio Setup 1 Search for and open RStudio. You can install R Studio on your own machine from: https://www.rstudio.com/products/rstudio/download/#download R studio requires R which you can download from: https://cran.rstudio.com/ RStudio is a free and open-source integrated development environment for R — it makes R much easier to use. If you are using a Mac and run into issues, firstly follow the instructions below then check out the [Mac R issues] section if the problem persists. R-Studio Setup 2 UCL students (and staff) can now also make use of R Studio Server. It’s RStudio on a webpage, so no installation is required. Access information will be provided on Moodle in Week 2. ArcGIS ArcGIS Pro (previously ArcMap) is the main commercial GIS software that you may have already used - or seen/heard about through other modules or even job aderts. We do not use ArcGIS Pro in our Practicals for several reasons: Computing requirements for ArcGIS Pro are substantial and it only operates on the Windows Operating System. For Mac users, using ArcGIS Pro (and ArcMap) would require using iether a Virtual Machine or “splitting your own harddrive” to install a Windows OS. It is proprietary software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license (around £100!) to continue using the software and repeat any analysis you’ve used the software for. Whilst ArcPro can use pure Python (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own ArcPy and ArcGIS API for Python packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others if they do not have their own ArcGIS license. Recent developments in the ArcPro software however does make it an attractive tool for spatial data science - it has cross-user functionality, from data analysts who like to use a tool called Notebooks for their code development, to those focused more on cartography and visualisation with in-built bridges to Adobe’s Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but for this course, we want to ensure the reproducibility of work (you’ll learn more about this in Week 1’s lectures). Therefore, the analysis for your coursework must be completed in R/R-Studio and QGIS (where permissible, see guidance in Week 5). Installation Issues If you have any issues with installing either Q-GIS or R, please let us know during Week 1 via the tech-help channel within the Geocomputation Team. PLEASE CONFIRM YOUR SOFTWARE INSTALLATION We would appreciate it if you can fill in this Installation Confirmation Form to confirm whether you have been able to install the relevant software by Friday 15th January 2021 5pm UK time. Acknowledgements Part of this page is adapted from CASA0005. "],["external-usage.html", "External Usage Issues / Contributions License Version", " External Usage All the required data to run this course or individual practicals is publicly available through the direct links provided in the practicals. For UCL students, access to the Geocomputation Moodle page will enable you to access the pre-formatted datasets, when appropriate. There are two main options to adopt the written content and practicals of this course: Adopt the course in its entirety by forking the repository on GitHub and Pulling to your local machine or simply download a .zip file containing the entire course. Adopt a single practical by downloading the .rmd file. You will still need to follow the instructions within each practical to download the data - and format it as appropriate. For external users, you are welcome to get in touch with Jo (see previous details) directly if you would like access to the formatted files or help in how to format them. Issues / Contributions To raise an issue simply log it on the GitHub issues tab for the repository. To propose an edit click on the edit symbol in the top tool bar (see [How to use this book]) and submit it for review. If you wish to contribute material or data then please contact the course convenor Jo Wilkin (details below). License If you use this material for teaching, research or anything else please let me (Andy) know via Twitter or email — j [dot] wilkin [at] ucl [dot] ac [dot] uk). This practical book is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. You are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material for any purpose, even commercially. However, you give appropriate credit, provide a link to the license, and indicate if changes were made. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. But, you do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. The code within this pracical book is available under the MIT license; so it is free to use (for any purpose) as long as you cite the source. Version This is version 1.0 of the Workbook. Acknowledgements Part of this page is adapted from CASA0005. "],["geocomputation-an-introduction.html", "1 Geocomputation: An Introduction Getting Ready for our Practicals", " 1 Geocomputation: An Introduction Welcome to the first week of Geocomputation! Week 1 in Geocomp This week’s content provides you with a thorough introduction into what is Geocomputation, outlining how and why it is different to a traditional ‘GIScience’ course. We set the scene for the remainder of the module and explain how the foundational concepts that you’ll learn about in the first half of term fit together to form the overall Geocomputation curriculum. We also outline how the course is a great step towards those interested in a career in (spatial) data science. For this week only, there is no practical per se, but you will need to complete a few practical tasks in preparation for our future practicals. We appreciate that to get to this point in our content, you will have read a lot on both the Moodle and Coursebook and we do have a few readings we’d like you to do in anticipation of next week’s seminar. There are however 3 assignments that we’d like you to complete, two of which involve setting up your computer ready for next week’s practical. Learning Objectives By the end of this week, you should be able to: Understand the differences between traditional GIScience and Geocomputation Explain what spatial analysis is and why it is important for Geocomputation Understand why we will use programming as our main tool for data analysis Know how you will access both of required software for this course: QGIS and R-Studio Establish good file management practices, ready for the module’s practical content, starting next week. What is Geocomputation? According to Lovelace et al (2020): Geocomputation is a young term, dating back to the first conference on the subject in 1996…[Geocomputation] is closely related to other terms including: Geographic Information Science (GIScience); Geomatics; Geoinformatics; Spatial Information Science; [Spatial Data Science]; and Geographic Data Science (GDS). Each term shares an emphasis on a ‘scientific’ (implying reproducible and falsifiable) approach influenced by GIS, although their origins and main fields of application differ. GDS, for example, emphasizes ‘data science’ skills and large datasets, while Geoinformatics tends to focus on data structures…Geocomputation is a recent term but is influenced by old ideas. It can be seen as a part of Geography, which has a 2000+ year history (Talbert 2014); and an extension of Geographic Information Science and Systems (Neteler and Mitasova 2008), which emerged in the 1960s (Coppock and Rhind 1991). Geocomputation is part of but also separate to the wider discipline of GIScience (and Systems). As geographers, particularly ones at UCL, you are likely to have come across GIScience in one of its many forms, including the use of GIScience software, known simply as GIS software, such as ArcGIS Pro or ArcMap. What differentiates Geocomputation from traditional GIScience is that it is: working with geographic data in a computational way, focusing on code, reproducibility and modularity. Lovelace et al, 2020 We would also add that its main focus is on the analysis of data, rather than wider technological and informational challenges that GIScience also addresses. Suggested Reading If you’d like to read where the above quote is from, you’re welcome to get ahead of Week 5’s reading by looking at Lovelace et al (2020) linked below. This is just suggested reading for this week - and may make a little more sense when we come to Week 5. But there’s always benefits in doing (and reading!) things twice. Book Chapter (10 mins): Lovelace et al, An Introduction to Geocomputation with R, Preface and Introduction. What is important to recognise is that Geocomputation benefits from many of the epistemological and ontological developments that were made in the 1960s onwards within GIScience to enable us now to process substantial amount of spatial data, geographic and non-geographic, at signficiant speeds and visualise our results accordingly. This includes how we capture, record and store the world around us in a digital format, how to take this data and turn it into insight and also the more technical issues of data formats, storing assigned metadata such as projections, and ensuring cross-compatibility across different GIS software and programming languages. Key Definitions Geographic refers to space on the earth’s surface and near-surface. Non-geographic can refer to other types of space, such as network and graph space. The use of spatial incorporates both geographic and non-geographic space. You’ll also see geospatial analysis mentioned which subset of spatial analysis applied specifically to the Earth’s surface and near-surface. Although there are subtle distinctions between the terms geographic(al), spatial, and geospatial, for many practical purposes they can be used interchangeably. Within this module, our focus will be on how we can analyse spatial data in a computational way to address specific research questions - we will try to focus on issues that often concern geographers, including socio-economic and environmental challenges, such as driving factors of crime and deprivation, inequalities in access to greenspace and food and health establishments, and exposure to environmental concerns, such as poor air quality. To achieve this, we need to draw on specific foundational concepts from GIScience, such as spatial data models and data interoperability (Week 2), Cartography and Visualisation, including map projections (Week 3), alongside traditional Data Analysis, including using Statistics (Week 4), and also learn how to Program effectively and efficiently, particularly when it comes to using spatial data (Week 4 and 5). The remainder of this week’s content provides you with a brief introduction into each of these foundational concepts for Geocomputation. Before you get started with the rest of this week’s content, however, we’d like you to make sure you’ve installed the software ready for next week. It should also serve as a good break between reading and watching our lecture videos. Assignment 1: Download Q-GIS and R-Studio software Your first assignment for this week is to complete the steps found in Software Installation and complete the Installation Confirmation Form once done. GIScience: A Short History Almost everything that happens, happens somewhere. Longley et al, 2015 Geographic information has an important role across a multitude of applications, from epidemiology, disaster management and demography, to resource management, urban and transport planning, infrastructure modelling and many more. With almost all human activities involving an important geographic component, understanding where something happens – and also why – can often be the most critically important piece of information when decisions need to be made that are likely to affect individuals, communities, our increasingly connected societies, as well as the environment and ecology that exist in the area of study. Current methods of analysing geographic information have its roots firmly within the discipline of Geographic Information Science (GIScience), which first came into prominence in the 60s and 70s as the first Geographic Information System (GIS) was conceptualised by the “Father of Geographic Information Science and Systems”: Roger Tomlinson. He formalised the ideas within his Doctoral Thesis here at UCL in 1974, under the title “The application of electronic computing methods and techniques to the storage, compilation, and assessment of mapped data”. Whilst the thesis is nearly fifty years old, much of its content remains extensively relevant to the problems faced by the collection and processing of geographic data and analysis of geographic information today. Furthermore, he identifies two important requirements for the success of GIScience: Within the discipline of geography, it is suggested that the mutual development of formal spatial models and geographic information systems will lead to future beneficial shifts of emphasis in both fields of endeavour. Tomlinson, 1974 For GIScience to work as a discipline, there was a need to focus on both the development of spatial modelling (i.e. how to represent and analyse real world spatial phenomena in digital systems as spatial data) and of geographic information systems (i.e. how this data is stored, managed, retrieved, queried and visualised as information). Much of Tomlinson’s work contributed to establishing both the spatial models and GISystems we use today - and UCL has remained active in the development of this knowledge, culminating in our course textbook by Professor Paul Longley et al, who you will find in our Department. The foundations of GIScience have been built upon, with fifty years of development in the digital collection, recording, management, sharing and analysis of geographic data and information. For spatial modelling, the discipline has seen researchers develop and implement new methods and techniques of spatial representation and analysis to augment and extend the capabilities of working with spatial data. For GIS, the discipline has spawned a new industry focusing on the (commercial) development of GIS tools, software and applications. These tools have enabled different types of GIS, from databases to analytical software to online data services and servers. Furthermore, for both to work in unison with one another, GIScience has seen the establishment of the Open Geospatial Consortium, which aims to provide consensus on the standards and codes used with geographic data, information, content and services. The following short lecture provides an introduction to GIScience, including the topics that we’ll cover in more detail in next week’s lecture and practical. What is GIScience: past, present and future Slides | Video on Stream In addition to the short lecture, and in preparation for next week’s seminar, please read the following two Book Chapters: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 1: Geographic information: Science, Systems, and Society. Article (15 mins): Albrecht, J. (2020). Philosophical Perspectives. The Geographic Information Science &amp; Technology Body of Knowledge. We’ll cover the history of GIS software in a little more detail next week. Spatial Analysis: An Overview “80% of all data is geographic.” A geographer Whilst no one really knows the origin of this urban legend within GIScience, it is a quote that has been heavily used across the GIScience industry to explain the importance of spatial analysis and the untapped potential of spatial data. It is incredible to believe that just over a decade ago, when I was in your exact position, there was a need to justify why studying, collecting and analysing geographic information was important. In just over a decade, we have now seen a substantial transformation where analysing geographic data is no longer a niche activity, but almost omnipresent to our personal lives and our society at large. Suggested Reading Article (3 mins): Forbes, 2020, Mapping the way forward: GIS is powering solutions to global challenges We now have an entire map in our pocket, which not only provides us with spatial analysis on the fly, but the device on which this map exists itself provides data to others to conduct spatial analysis on our own behaviours. Not only can we find out the best route for us to drive to our favourite park at the touch of a button, we can also passively inform others on how long it will take them to get there too. We all now actively create, use and analyse spatial data - whether we are aware of it or not! The question that has faced those working in GIScience - and now in fields and disciplines beyond - is how to formalise these analyses into identifiable and, importantly, rigorous methods and techniques. Over the course of this module, we’ll introduce you to some of the core and advanced analysis methods that will be essential to analysing spatial data, including geometric operations, spatial autocorrelation, spatial regression, cluster analysis, interpolation and network analysis. These methods have been developed by those working actively in GIScience and spatial analysis, such as Dr Luc Anselin and his development of spatial autocorrelation methods. What you’ll learn - and quickly find out through your own application of these analysis methods - is that understanding the theory and principles behind them is just essential as knowing how to implement them, either through GIS software or programming. The following short lecture provides an introduction to spatial analysis and the techniques you’ll come across in the following weeks. Spatial Analysis: A key component of GIScience and beyond Slides | Video on Stream Assignment 2: Spatial Analysis and the COVID-19 Pandemic For your second assignment this week, we’d like to you think about how spatial analysis has been used in the current pandemic (don’t worry, this is one of the few times we’ll reference it moving forward!). Prior to next’s week seminar (and preferably by Friday 5pm), we’d like you to submit a short description (100~ words or less!) of an application you may be using, or have seen in the news, where you think spatial analysis has been critical to its success. You don’t need to know exactly how spatial analysis is being used, but you’re welcome to make a guess - you can also submit a reference as well, if you’d like. Also - an application does not necessary mean a phone app, but can be a tool, website, or dashboard - or anything else you can think of that has a spatial component to it! Please submit your description here! Programming for Data Analysis Concurrent to the developments within GIScience and spatial analysis, particularly over the last twenty years or so, we have begun to see a growing dataficaton of our everyday lives, where we: “take all aspects of life and turn them into data.” Cukier &amp; Mayer-Schöenberg, 2013 Our personal use of digital sensors - from our mobile phone data, use of online social networks and fitness trackers, to our travel and credit card - has created a deluge of data, most commonly known as ‘big data’. What sets ‘big data’ apart from traditional data is these data are often substantial in their volume, velocity and variety - making them difficult to manage, store, process and analyse. The hope, however, has been with big data is that by harnessing and ‘wrangling’ ita, we will be able to derive new insight from this data that can help address real world challenges, from something as simple as Google’s Traffic alerts within its Maps application, to tracking food security in areas where access for surveys are unfeasible. For GIScience and spatial analysis, what is important to note is that this data deluge and resulting specialisation has created a new approach to the analysis of data that goes beyond traditional data analysis, known as data science, which has worked its way into analysis streams and lexicon of many industries, from commercial organisations to academic research institutions. What distinguishes data science from traditional data analysis is that data scientists are able extract knowledge from these substantial datasets by using an intersection of three skills: hacking skills (or computational skills), statistical analysis and domain expertise. These computational skills - in the form of 1) programming, 2) distributed computing and 3) large-scale (complex) analysis - often set these data analysts apart from their traditional counterparts. The increasing popularity of data science is having a signficant impact on how we “do” spatial anaysis as more and more data scientists become involved with spatial analysis as, for many of these datasets, location is a key component for its management, processing and analysis - after all, everything happens somewhere. Concomittantly, there has also been an growing availability and accessibility of other geographical data, such as satellite and UAV imagery, that have significant interest to those working in these computational fields, such as computer vision and machine learning, such as extracting building and/or roads from true-colour satellite imagery. As a result, the “world” of geographic information has transformed rapidly from a data-scarce to a data-rich environment (Miller and Goodchild, 2015) and has garnered significant interest from those who do not necessarily consider themselves as working within the GIScience discipline. Their involvement has increased the utility of computational tools, such as programming languages and data servers, to ensure that traditional programming, data analysis and statistical langauges, such as Python and R can incorporate and conduct spatial analysis. This has involved the creation of many GIScience and spatial analysis focused libraries or packages (to be explained further in Week 4) within these programming languages, that have enabled analysts and researchers to run specific techniques or algorithms (such as calculating a buffer around a specific point) but for substantially larger datasets than traditional software can normally handle. Whilst this adoption of spatial analysis and GIS by non-GIScience practitioners certainly has (and continues to have) its pitfalls (as you’ll see later on in the module), there is also a growing influence and appeal of data science to many working in GIScience (and its related fields) – including its focus on analysing large-scale datasets that may have the potential to study geographic phenomena at unprecedented scales and detail. Unlike GIScience of ten years ago, there is, as a result, a pertinent need to teach these computational skills - first in the form of programming - to you as future GIS analysts, researchers or even data scientists, alongside the theory and principles of spatial analysis and the wider GIScience knowledge base. As you might guess where this is going, our focus on Geocomputation is a first step in this direction - which you may build upon in your third year, following through with modules such as Mining Social and Geographic Datasets. The following short lecture outlines the key reasons why we should program for spatial analysis: Slides | Video on Stream Programmming vs. GUI-GIS If you’ve not programmed before, the learning curve to program can be daunting - and also very frustrating! To be honest, even when you know how to program, it can still be incredibly frustrating! But the benefits of being able to program your data processing and analysis using a Command Line Interface (CLI) program, compared to using a traditional Graphical User Interface (GUI) sotware, are substantial. This does not mean using a GUI GIS does not have purpose or its own advantages. In my opinion, GUI GIS are incredibly useful tools to understand the “spatialness” of your spatial data and your spatial analysis, particularly when looking at spatial operations and spatial neighbours. The scripting aspect of R/Python often shield or hide you from this spatiality, which when you’re starting out with GIS and spatial analysis, is also an important learning curve! Furthermore, with GUI GIS, if you are interested in making paper-based maps and establishing your own “James Cheshire and Oliver Uberti” coffee table map books, learning map-making in a GUI GIS can be incredibly helpful in terms of understanding the flexibility of styling, label placement etc. ArcGIS, for example, has bridges with Adobe and its Creative Suite catalogue of software, enabling you to easily format maps you’ve made in ArcGIS Pro within Illustrator and/or InDesign. As a result, for the first two weeks of practicals - and part of the final practical - we will use Q-GIS so you have a basic understanding of how to use the software, and can then develop your use of the software outside of our course if and when you need. To understand more about what spatial (geographic) data science is (and why we program!), please read our other two key readings for this week: Key Reading(s) Article (25 mins): Brunsdon and Comber, 2020, Opening Practice: Supporting Reproducibility and Critical Spatial Data Scinece Article (10 mins): Singleton and Arribas-Bel, 2019, Geographic Data Science, Geographic Analysis In addition, you can watch this short video from Carto, a major commercial organisation working in several aspects of spatial data science. It outlines these key skills you’ll need to learn to become a competent spatial data scientist, including an understanding of spatial data, which many data scientists often lack prior to engaging with spatial data. Carto’s ‘What is spatial data science’ video What’s next for us in Geocomputation We believe strongly that effective users of GI systems require some awareness of all aspects of geographic information, from the basic principles and techniques to concepts of management and familiarity with applications. Longley et al, 2015 pg.32 For the next few weeks, we’ll be taking a deeper look at many of these foundational concepts that will ultimately enable you to be able to confidently and competently analyse spatial data using both programming and GIS software. As you might guess, you’ll therefore be going on many learning curves over the coming weeks - some that may feel familiar (e.g. applying descriptive statistics) and others that are more challenging (e.g. learning how to write code and debug it as you find errors). To help with this, I highly recommend that you try to stay organised with your work, including taking notes and making yourself a coding handbook. I’d also list the different datasets you come across - and importantly, the scales and different projections you use them at - more on this in the next two weeks. Finally, you should also make notes about the different spatial analysis techniques you’ll come across, including the different properties they assess and parameters they require to run. Furthermore, over the next nine weeks, you’ll learn how to plan, structure and conduct your own spatial analysis using programming – whilst making decisions on how to best present your work, which is a crucial aspect of any type of investigation but of particular relevance to your dissertation. Establishing an organised file system, for both your data and your documents, is essential to working effectively and efficiently as a researcher, whether in Gecomputation, Spatial Data Science or any other application you might think of! To this end, we move to the final part of our content for this week: creating our folders to establish good File Management procedures. Getting Ready for our Practicals To get ready for our practicals, which start next week, we would like you to set-up a file management system as follows (either on your local computer or DesktopAnywhere VM) - this will also help ensure any code you use from Week 4 onwards works without issue (*theoretically!): Create a GEOG0030 folder in your Documents folder on your computer (most likely inside a UCL or Undergrad or Geography folder, and then again within a Year 2 folder - although we are not mindreaders here ;) ). Next, within your GEOG0030 folder, create the following subfolders: data lecture_slides maps qgis notes any other folder types you may think you need for this course (although you can of course add these as the module continues) Note the _ separating the two words in lecture_slides folder. PLEASE DO NOT LEAVE ANY GAPS INBETWEEN YOUR FOLDER NAMES (OR FILE NAMES). We will explain why in our seminar next week. Also note we do not use any capitals in our folder names. Within your data folder, create the following subfolders: raw working final If you’ve downloaded the lecture slides, move these into your lecture_slides folder. We’ll explain more about establishing good file management procedures in the seminar at the beginning of next week. Assignment 3 Follow the above guidelines to create your folders in your local system ready for our practicals to begin next week. And that’s it. You’re now ready to start our practicals next week. We look forward to meeting you all in our first seminars next week and address any questions you might have from this week’s content! Week 1 Recap This week, we’ve provided you with an introduction to the Foundational Concepts you’ll be coming across in our course as we train you to become competent spatial data analysist. You should now: Understand the differences between traditional GIScience and Geocomputation. Be able to explain what spatial analysis is and why it is important for Geocomputation. Understand why we will use programming as our main tool for data analysis. Know how you will access both of required software for this course: QGIS and R-Studio - or flagged this as an issue to us via the form! Have created your file system for GEOG0030 ready to practice good file management for the module’s practical content, starting next week. "],["giscience-and-gis-software.html", "2 GIScience and GIS software", " 2 GIScience and GIS software Welcome to Week 2 in Geocomputation! I hope you have your favourite caffeinated (or not!) beverage at hand and some good concentration music because this will be a longer than usual week of work to get through - but if you concentrate, take your notes, and complete our practicals, it will hold you in good stead as you progress along our course. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 3 will be shorter in content, therefore you will have time to catch up before the seminars at the start of Week 4. Week 2 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with GIScience and GIS software. Out of all our foundational concepts you’ll come across in the next four weeks, this is probably the most substantial to get to grips with - and has both significant theoretical and practical aspects to its learning. This week’s content is split into 6 parts: What is Representation? (5 minutes) Geographic Representation (25 minutes) Spatial Structure, Sampling and Scale (25 minutes) Spatial Data Models (45 minutes) Spatial Data File Formats (20 minutes) Practical 1: Exploring Population Changes Across London (1 hour) Videos can be found in Parts 2-5, alongisde Key and Suggested Reading and the first two of 3 assignments. Video content this week is a mixture of short lectures from myself, and two videos from YouTube. The two explanations from YouTube summarise the content presented in this workbook succinctly and with some really interesting examples. Using these videos have allowed me to spend more time on your practical - including ensuring there is a practical for those of you who cannot download Q-GIS. Part 6 is our Practical for this week, where you will be introduced to Q-GIS and apply the knowledge gained in the previous parts from Parts 1-5 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical. Learning Objectives By the end of this week, you should be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and their respective file types Know how to manage and import different spatial file types into a GIS software Learn how to use attributes to join table data to vector data Introduce you to the concept of an Administrative Geography (more next week!) As stated above, there is a lot to go through in this week’s content - but everything you will learn this week will provide you with a comprehensive background for the following weeks in the module. What is Representation? To be able to conduct any spatial analysis using our GIS tools or software, we first need to establish how we capture the geographical features, processes and/or phenomena that we wish to study as digital data that is readable by ourselves and by our computers. Coming to GIScience, at this point of time in its development, we often take this above statement for granted – we are, as mentioned last week, surrounded by geographical (and social) data, where much of this conceptual work has been done and, as a result, is often hidden from us as users of this data and/or technology. We can, for example, access data on every single train station in England - we can download this data directly from OpenStreetMap. But you’ll have a choice - depending on how you wish to represent your train stations, which is usually determined by your work purpose. For example, are you looking to show the distribution of stations over England? Do you therefore download the train stations as individual points that you could label with their name? Train stations in England represented as points on a map. Data from © OpenStreetMap and its contributors However, is this truly enough to represent a “train station” - surely, you might want to have the building instead, because this is the actual “station” itself? But then again, is this still enough? Do you need to have the components that constitute a train station - the railway tracks, the ticket office (or ticket stations more common now!), and even the waiting room - to truly represent every train station in England? In our case example, when looking only at the distribution of train stations, a point representation is likely to be sufficient enough - but this representation does not tell us much about the size and service area of each of the stations - or much else about them! We often do not think nor question the representations used to present data to us when we use or interact with spatial data - until, for example, we see something wrong that does not fit with our expectations or does not contain the information we want or expect for our purpose. Often, at times, representations can also be misleading, if the right information is not conveyed - or conveyed in the wrong way. However, as you’ll find out below, we often need to weigh up including too much detail in our representations, particularly if this detail is redundant to the information we wish to convey. We therefore use representations to convey information about something in the real world - but these representations almost always simplify the truth. We simply can’t fit every piece of information about the world around us within a representation - we have to select what bits are most important and relevant to what we are trying to convey. What this also means is that for these representations to mean something to us (i.e. can be interpreted), they need to fit this information into a standard form or model that we have come to expect in their representation. In our case above, we are able to associate a point on a map as a point of interest - and understand the distribution of the train stations thusly - because this has become the most dominant way to represent the location of an entity in a simple format on a map. As a result: the creation of these representations have required significant epistemological and ontological developments in order to turn the complexities of the world around us into information that we can understand. This includes: How to “view” the world around us in ways that lend themselves to be modelled by digital data. How to “sample” the world around us to be able to model these “views” as digital data. How to structure these models as digital data to facilitate their processing and analysis. How to create standardised formats to store and share these digital data across programs, software and computers. As a result, there are established rules, and classification schema (“models”) to how we represent geographic pheonomena and processes, which you will learn about today. Definitions Epistemological: ‘what we know and how we can know it’ - the theory of how a piece of knowledge has come into being, including the methods behind generating its “truth” and the validity of these methods, the belief in this truth, and the justification of holding these beliefs. Ontological: ‘studying what there is’ – questioning how we see our reality and categorise it in order to determine how things come into being. Geographic Representation To be able to convert the world around us into digital geographic data, we first need to understand how we can represent the features, processes and phenomena we may want to study. As Longley et al (2015) explain: “Representations help us assemble far more knowledge about the Earth than is possible on our own…They are reinforced by the rules and laws that we humans have learned to apply to the unobserved world around us.” As outlined above, increasingly due to our use of digital technology, this representation itself is rarely seen or really understood by the users of the data – only those creating the data are likely to ever see its individual elements and/or components. But behind the data that you’ll become familiar with over the course of this module, there are significant and specific decisions that have been made, which you should be aware of in order to understand these data (and their limitations) fully. One of the major developments in GIScience was the creation of representations that can capture the different types of geographic phenomena and processes around us - which could then ultimately be modelled and turned into digital data. These representations view the world in two fundamental ways: as discrete objects and as continuous fields. In summary, the discrete object view represents the geographic world as objects with well-defined boundaries, within larger objects of well-defined boundaries, in otherwise empty space, i.e. similar to our reference mapping schematisation. In comparision, the continuous field view represents the real world as a finite number of variables, that can each be defined at every possible position to create a continuous surface of the respective variable. The following short video outlines these in more detail, with examples: Understanding how to represent the world around us However, one thing to note is that many geographic phenomena have both object and field characteristics. When representing and modelling many features, the boundaries are not often clearly continuous or discrete. A continuum is created in representing geographic features, with the extremes being pure discrete and pure continuous features. Most features fall somewhere between the extremes. An example could be looking at edges of forest and trying to define their boundaries – does the boundary stop at the tree trunk or the diffuse layering of leaves? A recent tweet from MapMaker David - one of my Twitter follow recommendations! This question actually poses itself to even the most experienced of GIS-ers and cartographers! David Garcia (aka Mapmaker David) is a Filipino Geographer and Cartographer and someone who I would highly advocate following on Twitter. He often raises a lot of questions about the epistemological and ontological aspects of GIScience and their development from essentially cartography and the role this has in minimising indigenous knowledge - he also makes beautiful maps. This critical approach to GIScience is something that we’ll look into a bit more in Week 5, in order to have time to give this content due justice! Assignment 1: Discrete Objects and Continuous Fields Let’s think about spatial representation models in more detail. Below are links to four spatial datasets that I’d like you to think about whether they represent discrete objects or continuous fields. Click on each link and note down your answer - I’ll be asking for these in our seminar in Week 4: Dataset Spatial Model Type USA Tree Canopy Cover ? Global Land Cover ? OS Open Rivers ? World Population Density Estimate ? Ultimately though, continuous fields and discrete objects only define two conceptual views of geographic phenomena, but do not solve the problem of digital representation, i.e. how do we capture this representation using computers. A continuous field view still potentially contains infinite amount of information as it aims to defines the values of the variable at every point – and there are an infinite number of points in any defined geographic area. In contrast, discrete objects can also require an infinite amount of information in order to provide a full description (e.g. our train station dataset above!). Neither of these approaches are designed to deal with the limitations of computers and the need to store this representation digitally - for this, we need to understand the spatial structure of the phenomena or process at study alongside the scale at which we want to represent them in order to devise a sampling scheme behind our data creation. Spatial Structure, Sampling and Scale Why do we need to sample our data? Well – if we try to include everything in our representation, we’d end up with a map the size of the world, which would be pretty useless! This issue is quite eloquently expressed by the Argentine writer, Jorge Luis Borges, who made up a fictional short story of the issue of an Empire aiming to create a map that was so perfect it could represent the whole empire - because it was the size of the Empire itself, coinciding point for point. As a result, the map, whilst perfect, was useless and was offered up to the elements to essentially destroy by the following generations! A short story on the issue of representation in science, geography and map-making On Exactitude in Science Jorge Luis Borges, Collected Fictions, translated by Andrew Hurley. …In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast Map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography. —Suarez Miranda, Viajes de varones prudentes, Libro IV,Cap. XLV, Lerida, 1658 (Borges’ fictional character of “the time”) Find more here: There is No Perfect Map by Marcelo Gleiser (5 mins) and Why a 70 year-old short story goes to the heart of modern map making by Ian Delaney (3 mins). To be able to create accurate representations of our geographic phenomena and processes, we therefore need to find a way to sample our phenomena or process to reduce the information whilst still retaining the most important pieces of information. You have probably come across the concept of sampling before when it comes to surveys and statistics and the need to create samples from a population. In this case, whenever we look to derive an accurate sample from a population, we look to create a sample frame or scheme to extract statistically significant information. In your previous research experience, you may have come across the ideas of random, systematic and stratified sampling - and that you choose the sampling approach that most reflects the likely structure or distirbution of the population you are targeting to sample. We can think of converting our geographic representations into digital data as a similar kind of sample, in that the elements of reality that are retained are abstracted from the observable real-world in accordance with some overall design. Therefore, to create digital data from our representation, we need to design a way to sample it. To do this, we first need to understand the structure of the data in order to deduce a good ‘sampling strategy’. The next lecture in this workshop provides an introduction to how we can use the structure of spatial data to determine appropriate sampling schemes. Understanding the structure of spatial data to determine sampling schemes Slides | Video on Stream When looking at the representation of geographic phenomena as digital data, the scale and level of detail of that is needed for the analysis will therefore determine the spatial sample design and how we can then generalise from these measurements. As a result, scale and level of detail are key to building appropriate representations of the world. Assignment 2: Digitising the River Thames, London, U.K We can put these ideas into practice by thinking about how we could create our own digital data. Let’s take what should be a straight-forward example of digitising the River Thames in London. The River Thames in London. Image: Esri. We’re going to use a very light online tool that allows us to create digital data (and as you’ll see later in the workshop, export the data we create as actual raw files). Head to geojson.io - it should load directly, zoomed into London. In the bottom left-hand corner, select Satellite as your map option. Next, click on the Draw a Polyline tool: Now digitise the river - simply click from a starting point on the left- or right-hand side of the map, and digitise the whole river. Once you’re done, simply double-click your final point to end your line. You can then click on the line and select info to find out how long the line is. For this assignment, I’d like you take a screenshot of you final line. When you click on the line, you can use Properties to style the line to make it more visible, e.g. change the colour and the width of the line. Please then post your screenshot on a new slide in your respective group’s Powerpoint you can find here and add a text-box stating how long your line is (in Km) (don’t worry, you don’t need to add your name). We’ll look at each other’s digitisation attempts during our seminar this week – but the questions to think about are: How easy did you find it to digitise the data and what decisions did you make in your own ‘sample scheme’? How close together are your clicks between lines? Did you sacrifice detail over expediency or did you spend perhaps a little too long trying to capture ever small bend in the river? How well do you think your line represents the River Thames? In the activity above, we were looking at the river as a discrete field – imagine then if I asked you to find a way to collect data on and then digitise the air quality over the same area of London? How would you go about creating an appropriate sample scheme to accurately represent air quality – without spending too much time on collecting the data that is becomes almost redundant? In both of these scenarios, you are using your a priori knowledge of the spatial structure of the phenomena to determine your spatial sampling scheme. However, in some scenarios, we may not know this structure before sampling nor can you always control for all variations in all characteristics. When looking to record a phenomenom as digital data at a fine scale, i.e. a high spatial resolution, we need to ensure our sample scheme reflects the minimal variation in the spatial autocorrelation with a feature. To record digital data at a coarse scale, i.e. a low spatial resolution, we can be more flexible with our sample scheme – but should ensure it reflects larger changes within our phenomenom. Whilst ideally we would want to capture our representation in as fine scale as possible as this is likely to be the most accurate, this sometimes can be detrimental to our capturing and storage of the representation as digital data (see the next section). Ultimately, a sampling scheme will be a best guess: we must remember that GIScience is about representing spatial and temporal phenomena in the observable world, and because the observable world is complicated (and does not always adhere to Tobler’s principles), this task is difficult, error prone, and often uncertain. As a result, with any data you use from other sources, always remember to consider its quality, accuracy and precision in representing geographic phenomena. Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 2: The Nature of Geographic Data. Computational considerations of the impact of scale and sampling One final thing to note when it comes to sampling spatial data at various scales is that if we try to sample and study complex phenomena at fine spatial resolutions but over significant extents, we may ultimately create many issues from a computational perspective. Whilst we may be able to sample our spatial phenomenon at a increasingly fine detail (e.g. satellite imagery can now collect data at less than a meter precision), this data ultimately has to be stored digitally. As a result, when looking to use increasing levels of precision over vast scales in terms of spatial coverage/extent, we can inadvertently create substantially large datasets that computers can struggle to visualise and process. As a result, we need to be conscientious about the data we are trying to create and use - for example, the Ordnance Survey’s MasterMap topography layer contains 400 million individual features (i.e. records). Trying to load even a subset of this on your computer can often cause significant processing problems! Usually, this means you have a choice. You can study something at a fine resolution, but you’ll need to keep your spatial coverage small. In comparison, you can expand your coverage if you reduce the resolution of your data. This all depends on the computational processing capability and capacity you have at your disposal, as well as what you are trying to achieve with your analysis, i.e. what detail do you need to answer your research questions. In addition, generalising is a key approach within GIScience that focuses on removing detail that is unnecessary for an application, in order to reduce data volume and speed up our processing. There are many approaches to generalising spatial data, which we come across in more detail over the coming weeks including simplification, smoothing, aggregation and amalgamation. Ultimately, we need a priori information to inform our understanding of whether our sampling scheme and resulting digital data is suitable for our analysis, i.e. it is accurate enough without hindering processing power. Determining an appropriate sampling scheme and resulting method of capturing this representation as digital data will therefore be determined by the phenomenom at study – and the limitations of those using and processing the resulting data. Spatial Data Models We can now see how we convert the observable world around us into spatial representations – and how we then need to consider scale and level of detail, alongside spatial structure, to determine our spatial sampling scheme. The next step is to convert our sampled observations (how ever they are collected) into digital data. Digital data at its basics is a form of binary data entry: the representation system in digital computers uses only two numbers (0 and 1). As a result, “Every item of useful information about the Earth’s surface is ultimately reduced by a GI database to some combination of 0s and 1s.” Longley et al, 2015 To create our modern day digital geographic data, we need to devise spatial formats that can ultimately be ‘written’ (or rather, ‘coded’) using this binary entry. Many of these decisions formed much of Roger Tomlinson’s original body of work (and others!). In this thesis, he outlined how to capture “real world data elements” as digitised geometries (points, lines, polygons) and grids - and how to store them in a coded digital format: Tomlinson’s original proposal for coding spatial data formats. Image: Tomlinson, 1974 These formats are the basis to the two main spatial data models we use. These are called raster and vector data formats, which are explained in further detail in this short video: Raster and Vector Spatial Data The below text summarises what was presented in the above video. Raster Data Format A raster dataset is a pixel-based grid data format. For any variable studied, a grid is created within which each pixel represents a value or measure for the variable: A raster grid and pixel. Image: QGIS Raster data only contain a single “attribute” for the variable it represents – and the attribute will be coded according to the data measurement scale and attribute type (see below). Rasters are primarily stored as a type of image file, that is either geo-referenced (e.g. a GeoTIFF) or comes with an additional georeferencing file (normally called a World file). Vector Data Format In comparison, vector data contains geometries: the points, lines and polygons we’ve seen earlier in the workshop. To provide the “geographic” component of these geometries, they actual geometry itself is specified using a pair of coordinates, preferably assigned to a specific coordinate reference system (the below diagrams simply use a graph!): Vector data: points, lines (polylines) and polygons (on a graph). Image: mgimond. As you can see, the three types of vector geometries are: A point dataset, which will have at least a single pair of coordinates for each point (or more generally “record”) within its dataset A single line, which will have two pairs of coordinates, whilst a polyline (multiple lines connected together) will have a minimum of three pairs. A polygon, which will have a minimum of three pairs (forming some sort of triangle!). Alongside containing these geometries, a vector dataset can also contain multiple attributes for each the records it contains. These attributes are stored in what is known as an Attribute Table. An Attribute Table consists of a set of records/observations (the rows) and attributes/fields (the columns): An example of an attribute table in ArcMap. Source: Esri Each record within the dataset will refer to one point, polygon or line (polyline) and will contain a value for each attribute/field that is part of the dataset. This includes a geometry field, which will contain the coordinates required to map and display the dataset correctly within its Coordinate Reference System (CRS) - more on these next week. The use of “field” for attribute tables At this point, it is important to note that you should not confuse the use of field here with our previous use of field in terms of spatial representation models. ‘Field’ and ‘scale’, as you can tell, have many meanings when used in GIS – but the more you come across the terms within context, the easier you’ll find it to understand which meaning is being referred to! These attributes will be stored in the field as a specific attribute measurement scale and as a specific data type - depending on the variable or data that they represent. Attribute Data Measurement Scales and Types For any data, whether spatial or not, it will collected against a specific measurement scale and, in its digital form, be stored as a specific type of data type. This measurement scale is a classification that describes the nature of the information of the values assigned to the specific variables. Data can be: Measurement Scale Explanation Nominal Has labels without any quantitative value. Ordinal Has an order or scale. Interval Numeric and have a linear scale, however they do not have a true zero and can therefore not be used to measure relative magnitudes. Ratio Interval data with a true zero. In addition, data may also be: Measurement Scale Explanation Binary Can have only two possible outcomes, yes and no or true and false, etc. Image: Allison Horst For example, for our point data set of train stations mentioned earlier: A field that contains the name of each train station would be nominal data. A field that details the class of the train station, e.g. whether it is a mainline, secondary or tertiary line as a type of order or rank, would be ordinal data. A field that details the temperature of the train station in celsius would be interval data A field that details the number of tracks the station contains would be ratio data A field that details whether the station is operational or not could be binary data (a ‘yes’ or ‘no’ or ‘operational’ or ‘non-operational’) Depending on the measurement scale, the attribute data will be stored as one of several data types: Type Stored Values Character Formats Short integer -32,768 to 32,768 Whole numbers Long integer -2,147,483,648 to 2,147,483,648 Whole numbers Float -3.4 * E-38 to 1.2 E38 Real numbers Double -2.2 * E-308 to 1.8 * E308 Real numbers Text Up to 64,000 characters Numbers, letters and words Knowing your measurement scale and data type level are essential to working accurately and effectively with spatial data. If you inadvertently store a float (e.g. values of 1.021, 1.222, 1.456, 1.512, 1.888) as an integer, your number will be rounded (e.g. it would become: 1, 1, 1, 2, 2) which can impact the accuracy of your work. Conversely, while storing whole numbers (integers) as a float or a double would not have an accuracy issue, it will come at a computational cost in terms of storage space. This may not be a big deal if the dataset is small, but if it consists of tens of thousands of records the increase in file size and processing time may become an issue. Being aware of (and checking!) your data types can also help solve initial bugs when loading and trying to analyse or visualise data in both GIS software and programming. For example, one commmon issue with data types when using table data within Excel prior to ingesting your data a GIS software or program is that Excel often converts British National Grid coordinate codes (which are integers) into text - therefore, when you come to display your point data, for example, by their coordinates, this field is not readable by your software or program. You therefore need to force your program to recognise that field as a numeric field - we’ll come across this issue and ways to solve it in Week 5. In addition to these attributes that contain variable information that might be used for analysis or visualisation purposes, each record should contain its own ID that will be used for indexing purposes in both GIS software and programming. This can help you select certain rows for analysis or order your data. Finally, in some cases, a dataset may contain a unique identifier (UID) for each record that can be used for data management purposes. These UID can be used to match with another dataset containing the same UID. In this latter scenario, this helps us join data that we may download as table data (e.g. a spreadsheet of population numbers for the different wards in London) with spatial data (e.g. a spatial dataset that shows the outlines of the wards in London) to create a new spatial dataset that contains the population data as an attribute, ready for its analysis and/or mapping. We’ll see this in action in today’s pratical. Don’t worry if this is a lot to take in right now, we’ll be utilising a lot of what you are reading about here in practice in the coming weeks! Rasterising vector and vectorising raster One additional thing to know about vector and raster data is that, in some cases, it is possible for both data formats to represent the same geographic feature, process or phenomena – but how they do so will look very different: Differences in capturing and storing geographic phenomena as vector and raster data. Image: vebuso. And also each data models comes with both advantages and limitations: Summarising key advantages and vector and raster data. Image: vebuso. There are also tools within our GIS software and programming software that will allow us to convert between the two data formats. This can be of use when we wish to process data faster (e.g. rasterising vector data) or we wish to add attributes to what was a continuous field (i.e. vectorising raster) for analysis. There will, of course, be considerations and limitations when switching between data formats, such as loss of accuracy in either direction of conversion. The results of vectorising elevation represented as a Digital Elevation Model Left image: DEM, Right image: vector version. Image: Esri. You will find more information on Spatial Data Models and the raster and vector data formats in the following two chapters in the Geographic Information Science &amp; Systems (GISS) book: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 3: Representing Geography. Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 7: Geographic Data Modeling. Spatial Data File Formats The final part to our introduction to spatial data is understanding the different file formats in which spatial data is stored. There are a number of commonly used file formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them. Shapefiles Perhaps the most commonly used spatial data file format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications. A shapefile is not a single file, but a collection of files of which at least three are needed for the data to be displayed in GIS software. The files include: File Type Description Required? .shp Contains the feature geometry Mandatory .shx Index file which stores the position of the feature IDs in the .shp file Mandatory .dbf Stores all of the attribute information associated with the records Mandatory .prj Contains all of the coordinate system information. Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used. Optional, but important .xml General metadata Optional, but important .cpg Encoding information Can also be included .sbn Optimization file for spatial queries Can also be included When using shapefiles, it is good to get into a habit of creating zipped archives of your file that you can share with yourself and others – this means selecting all the related files, right-clicking and choosing to compress or archive your data. This creates a single ‘file’ to move, for example across folders, so you do not end up losing any of the files that are critical for the shapefile to display! Copying and pasting the .shp file alone is not enough! This is one of the main criticisms of the shapefile – it is easy to lose files and as a result render your data useless. Other GIS formats such as GeoJSON and the increasingly popular GeoPackage include all of this information in a single file, reducing this risk substantially of this happening. Despite these issues, the shapefile still remains an ever-popular GIS format, and one you’ll use the most in this course. On Twitter and want to see the love for shapefiles….have a look at the shapefile account: GeoJSON GeoJSON (Geospatial Data Interchange format for JavaScript Object Notation) is becoming an increasingly popular spatial data file, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file. Comparing Shapefile and GeoJSON file formats If you would like, you can explore a shapefile (.shp ) and GeoJSON (.geojson) in action - we’ll use the light digitising tool, that we used earlier to digitise the River Thames: Head to: http://geojson.io/#map=16/51.5247/-0.1339 Image: Digitised point, line and polygon examples. 2. Using the drawing tools to the right of the map window, create 3 objects: a point, line and a polygon as shown above. Click on your polygon and colour it red and colour your point green. Using the ‘Save’ option at the top of the map, save two copies of your new data – one in .geojson format and one in .shp format. Open your two newly saved files in a text editor such as notepad or notepad++ on your computer. For the shapefile you might have to unzip the folder then open each file individually. What do you notice about the similarities or differences between the two ways that the data are encoded? I won’t ask you about this in our seminar, but it’s a good way to start getting familiar with the actual structure of our data. If you do end up having issues with your datasets, this may give you an idea of where you might find out if there’s an issue with your raw data itself. Geodatabase A geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab. GeoPackage A GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move. Raster Data Most raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostationary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type. Other Data Formats The aforementioned file types and formats are likely to be the ones you predominately encounter. However there are several more used within spatial analysis. These include: Vector GML (Geography Markup Language —- gave birth to Keyhold Markup Language (KML)) SpatialLite PostGIS Raster Band SeQuential (BSQ) - technically a method for encoding data but commonly referred to as BSQ. Hierarchical Data Format (HDF) Arc Grid There are normally valid reasons for storing data in one of these other file formats, however you do not need to read or know about these for now! In the end, the variety of data formats can be a bit overwhelming. But don’t worry, most of the time in this course you’ll be using shapefiles, table (in the form of csvs) or raster data. Table Data: Comma Separated Values (.csv) v. Excel Spreadsheet (.xls) In addition to spatial data, you will find that in this module (and for your dissertations), you will download and use a lot of table (tabular/spreadsheet) data. When you download this data, you can first inspect that data in Excel or Numbers (or another spreadsheet application of your choice), prior to loading it into either GIS software or programming software (such as R-Studio). The reason why is a lot of the time, you will need to clean this dataset prior to using it within these software/programs. Often the data comes formatted with too many rows, additional formatting, or generally just a lot of additional stuff we just don’t need. We’ll take a deeper look at this need for cleaning in Week 4 as we tackle using R-Studio. One thing to note though is that there are differences between a csv and an Excel spreadsheet, particularly if the latter is contained in a Workbook. There are a few summaries of these differences available online and we will go over the differences in further detail again in Week 4. For now, please be aware that we will be using csv as our default table data format, so if you need to save anything at any point in our practical, please save your file as a csv. GIS Software - a more thorough introduction (moved to Week 5) As outlined last week, this week, we were going to provide you with a more thorough introduction to the different types of GIS software available to you - but we’ve decided that you’ve read/listened/learned enough about spatial data that more information on GIS software is not going to help. Instead, we’ll cover this in Week 5 and now move onto our Practical. If you’d like to get ahead, you can read the following chapter in GISS: Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 6: GI System Software. Practical 1: Exploring Population Changes Across London The first half of this workshop has given you an in-depth introduction into how we can represent the world around us and turn it into digital geographic data – and how we store this data from a technical perspective. The practical component of the week puts some of these learnings into practice with an exploration of population data within London. The datasets you will create in this practical will be used in Week 3 practicals, so make sure to follow every step and export your data into your working folder at the end. The practical component introduces you to attribute joins. You’ll be using these joins throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. A Typical Spatial Data Analysis Workflow When using spatial data, there is generally a very specific workflow that you’ll need to go through - and believe it or not, the majority of this is not actually focused on analysing your data. Along with last week’s “80% of data is geographic data”, the second most oft-quoted GIS-related unreferenced ‘fact’ is that anyone working with spatial data will spend 80% of their time simply finding, retrieving, managing and processing the data – before any analysis can be done. One of the reasons behind this need for a substantial amount of processing is that the data you often need to use is not in the format that you require for analysis. For example, for our investigation, there is not a ‘ready-made’ spatial population dataset (i.e. population shapefile) we can download to explore popuation change across England: Image: Alas a quick google search shows that finding a shapefile of England’s population is incredibly difficult! Instead, we need to go and find the raw datasets and create the data layers that we want. As a result, before beginning any spatial analysis project, it is best-practice to think through what end product you will ultimately need for your analysis. A typical spatial analysis workflow usually looks something like this: Identify the data you need to complete your analysis i.e. answer your research questions. This includes thinking through the scale, coverage and currency of your dataset. Find the data that matches your requirements - is it openly and easily available? Download the data and store it in the correct location. Clean/tidy the data - this may be done before or after ingesting your data into your chosen software/program. Ingest/load the data into your chosen software/program. Transform &amp; process the data - this may require re-projection (next Week), creating joins between datasets, calculating new fields and/or creating selections of the data that you want to work with (Week 5). Run Analysis on your data, whatever technique you are using. Visualise your data and results, including maps, graphs and statistics. Communicate your study and outputs - through good write-ups and explanations of your visualisations. As you can see, the analysis and visualisation part comes quite late in the overall spatial analysis workflow - and instead, the workflow is very top-heavy with data management. Wrangling data is often the most time-consuming part of any spatial analysis project! Image: Allison Horst Often in GIS-related courses, you’ll often be given pre-processed datasets ready to go ahead with analysing the data. Instead, we’re going to start cleaning (the majority of) our data from the get-go. This will help you understand the processes that you’ll need to go through in the future as you search for and download your own data, as well as deal with the data first-hand before ingesting it within our GIS software.Good thing you’ll be learning a lot about these aspects over the coming weeks! Setting the scene: why investigate population change in London? For this practical, we will investigate how population has changed over the last ten years in London. Understanding population change - over space - is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure. We can also see first-hand the impact of wider socio-economic processes, such as urbanisation, or, in the case of the predicted population movements currently, relocation of a certain demographic of urban dwellers to rural areas. For us, the aim for our practical is to actually create population data for London in 2011, 2015 and 2019 at the ward scale that we can use within our future analysis projects, starting next week. This data will be used in our future practicals to normalise certain data, such as the crime datasets for next week. Why do we need to normalise by population? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event, such as crime. We’ll look at this in greater detail next week. Finding our datasets In the U.K, finding authoritative data on population and Administrative Geography boundaries is increasingly straight-forward. Over the last ten years, the UK government has opened up many of its datasets as part of an Open Data precedent that began in 2010 with the creation of data.gov.uk and the Open Government Licence (the terms and conditions for using data). Data.gov.uk is the UK government’s central database that contains open data that the central government, local authorities and public bodies publish. This includes, for example, aggregated census and health data – and even government spending. In addition to this central database, there are other authoritative databases run by the government and/or respective public bodies that contain either a specific type of data (e.g. census data, crime data) or a specific collection of datasets (e.g. health data direct from the NHS, data about London). Some portals are less up-to-date than others, so it’s wise to double-check with the ‘originators’ of the data to see if there are more recent versions. For our practical, we will access data from three portals: For our administrative boundaries, we will download the spatial data from the London Datastore (which is exactly what it sounds like!). For population, we will download table data from the Office of National Statistics (ONS) (for 2019 data to represnt 2020) and the London Datastore (only contains these data until 2018). In our extension activity (available later this week), we will also download a gridded spatial dataset showing how population can be represented in the raster data format from the Worldpop research group at the University of Southampton. Download and process datasets The first step in our practical is to download and process our two main datasets: administrative geography boundaries and population. Administrative Geography Boundaries For our administrative boundaries, we’ll download the ‘Statistical GIS Boundary Files for London’ dataset(s) found in the London Datastore. Navigate to the datasets, here: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london. If you navigate to this page, you will find multiple choices of data to download. We wil need to download the all three zipfiles: statistical-gis-boundaries-london.zip, London-wards-2014.zip and London-wards-2018.zip. The first dataset contains ALL levels of London’s administrative boundaries. In descending size order: Borough, Ward, Middle Super Output Area / MSOA, Lower Super Output Area / LSOA, and Output Area / OA) from 2011. The second dataset contains an UPDATED version of the Ward boundaries, as of 2014. The third dataset contains an UPDATED version of the Ward boundaries, as of 2020. As we will be looking at population data for 2015 and 2020, it is best practice to use those boundaries that are most reflective of the ‘geography’ at the time; therefore, we will use these 2014 / 2018 ward boundaries for our 2015 / 2020 population dataset respecitvely. When downloaded, depending on your operating system, the zip may unzip itself (or you may need to do this manually). When open, you’ll find two folder options: Esri and MapInfo. These folders contain the same set of data, but simply in two data formats: Esri shapefile and MapInfo TAB. MapInfo is another proprietary GIS software, which has historically been used in public sectors services in the UK (and many councils still use the software!), although has generally been replaced by either Esri’s Arc ecosystem or open-source software GIS. The TAB format is the main format that the software uses for vector data, similar to Esri and its shapefile format. In your GEOG0030/data/raw/ folder, create a new folder called boundaries. Within this folder, create three new folders: 2011, 2014 and 2018. Copy the entire contents of Esri folder of each year into their respetive year folder within your new boundaries folder: Note, we do not want to add the additional Esri folder as a step in our file sytem. I.e. your file path should read: GEOG0030/data/raw/boundaries/2011 for the 2011 boundaries, and GEOG0030/data/raw/boundaries/2014 for the 2014 boundaries etc. We now have our Administrative Geography files ready for use. We will ingest these directly into Q-GIS and do not need to do any cleaning at this stage. What are wards and boroughs? A short introduction to Administrative Geographies. Put simply, administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. Administrative areas ensure that each public body has a clearly defined area of responsibility, which can be measured and budgeted for appropriately. They originate from the Roman era who used these geographies, usually defined by topographical and geographical features, to administer these regions including collecting the relevant tax from those living in these areas. These geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. The UK has quite a complex administrative geography (see more here), particularly due to having several countries within one overriding administration and then multiple ways of dividing the countries according to specific applications. For the majority of your practicals, we will be keeping it simple with a focus on London, which is divided into: Boroughs -&gt; Wards OR Boroughs –&gt; Middle Super Output Areas -&gt; Lower Super Output Areas -&gt; Output Areas. We’ll be looking at wards in our practical analysis – although even at this fine scale, the City of London is a little pesky and introduces complexities into our analysis, which we’ll see. We’ll learn more about Administrative Geographies next week. Population Datasets For our population datasets, we will use the ONS mid-year estimates (MYE). These population datasets are estimates that have been modelled based on the previous 2011 census count and then forecasted population growth (plus some additional data). They are released a year, with a delay of a year, i.e. we can only access data for 2019 at the moment, so we’ll use this as our most recent year. As the London Datastore only has these MYE for up to 2018, we’ll need to download the data from ONS directly. It’s always worth checking the ‘originators’ of the data to see if there are more recent versions. Navigate to the Ward level datasets: https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/wardlevelmidyearpopulationestimatesexperimental When you navigate to this page, you will find multiple choices of data to download. We will need to download the estimates for 2011, 2015 and 2019. Click to download each of the zipfiles. Choose the revised versions for 2015 and the (Census-based) on 2011 wards edition for 2011. In your GEOG0030/data/raw/ folder, create a new folder called population and copy the three spreadsheets into this folder. Now it’s time to do some quite extensive data cleaning. Cleaning our Population Datasets If you open up the 2011 ward spreadsheet in Excel (or another spreadsheet program: this could be Numbers or you can upload your data to use it with Google Docs, for example), you’ll quickly see that there are several worksheets to this workbook. We are interested in the Mid-2011 Persons. Click on the Mid-2011 Persons tab and have a look at the data. As you should be able to see, we have a set of different fields (e.g. Ward Code, Ward Name), including population statistics. Right now, we have too much data - so what we will want to do is simplify and extract only the data we need for analysis. For this, we need the total population (All Ages), alongside some identifying information that distinguishes each record from one another. Here we can see that both Ward Code and Ward Name suit this requirement. We can also think that the Local Authority column might be of use - so it might be worthwhile keeping this information as well. Create a new spreadsheet within your program. From the Mid-2011 Persons spreadsheet, copy over all cells from columns A to D and rows 4 to 636 into this new spreadsheet. Row 636 denotes the end of the Greater London wards (i.e. the end of the Westminster LA) which are kept (in most scenarios) at the top of the spreadsheet as their Ward Codes are the first in sequential order. Before we go any further, we need to format our data. First, we want to rename our fields to remove the spaces and superscript formatting. Re-title the fields as follows: ward_code, ward_name, local_authority and POP2011. One further bit of formatting that you MUST do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave this commas in our values, Q-GIS will read them as decimal points, creating decimal values of our population. There are many points at which we could solve this issue, but the easiest point is now - we will strip our population values of the commas and set them to integer (whole numbers) values. To format this column, select the entire column and right-click on the ‘D’ cell. Click on Format Cells and set the Cells to Number with 0 decimal places. You should see that the commas are now removed from your population values. Save your spreadsheet into your working folder as ward_population_2011.csv. We now need to copy over the data from the 2015 and 2019 datasets as well into their own csvs. Open the Mid-2015 ward population xls from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2015 Persons tab. We now need to copy over the data from our 2015 dataset to a new spreadsheet again. However, At first instance, you’ll notice that the City of London wards are missing from this dataset. Then if you scroll to the end of the London Local Authorities, i.e. to the bottom of Westminster, what you should notice is that the final row for the Westminster data is in fact row 575 - this means we’re missing nearly other LAs in addition to our COL LAs and we will need to determine which ones are missing and if we can find them in the 2015 spreadsheet. With this in mind, first copy from row 5 to the end of the grouped London Local Authorities, i.e. to the bottom of Westminster, for columns A to D into a new spreadsheet. Through a quick scroll of the Local Authorities, a.k.a as Boroughs, (and with the extensive knowledge that you will soon build about London Local Authorities!) we can quickly find that we are missing the wards for: Hackney Kensington and Chelsea Tower Hamlets. If we head back to the original 2015 raw dataset, we can actually find this data (as well as the City of London) further down in the spreadsheet. It seems like these LAs had their codes revised in the 2014 revision and are no longer in the same order as the 2011 dataset - oh, the joys of using data! Locate the data for the City of London, Hackney, Kensington and Chelsea and Tower Hamlets and copy this over into our new spreadsheet. Double-check that you now have in total 636 wards within your dataset. Remember to rename the fields as above, but change your population field to POP2015. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2015.csv. We now need to repeat this for our 2019 data. I wonder what surprises this dataset has in store for us! Open the Mid-2019 ward population spreadsheet from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2019 Persons tab. Let’s have a look at our data - once again, there’s a lot to take in - but what we’re interested is in columns A, B, and now D and G. Let’s follow the same process we used above to copy our data across. To make our processing easier, first hide columns C, E and F in our spreadsheet - right-click on the columns at select Hide. Next, copy the data from row 5 to the final row for the Westminster data for columsn A, B, D and G over into a new spreadsheet. Look at the total number of rows you’ve copied over. We can see that we have even fewer wards than the 2015 dataset - yikes! We need to go hunting again for our missing data in the 2019 dataset. For expediency, you need to find and copy over the data for: City of London Hackney Kensington and Chelsea Tower Hamlets (as per 2015) and Bexley Croydon Redbridge Southwark Perhaps now you see why so much time is spent on processing data for spatial analysis! Copy over the remaining wards for these Local Authorities/Boroughs. Once you’ve copied them over - you should now have 640 wards - delete columns C, E and F and rename the remaining fields as you have done previously. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2019.csv. You should now have your three population csv datasets in your working folder. We’re now ready to start using our data within Q-GIS. Using Q-GIS to map our population data We will now use Q-GIS to create population maps for the wards in London across our three time periods. To achieve this, we need to join our table data to our spatial datasets and then map our populations for our visual analysis. Because, as we have seen above, we have issues with the number of wards and changes in boundaries across our three years, we will not (for now) complete any quantitative analysis of these population changes - this would require significant additional processing that we do not have time for today. *Data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to Administrative Geographies. In our extension activity Extension: Population as a Raster Dataset we show how we can complete this calculation easily when we use raster data that has a standardised grid format.* If you do not have access to Q-GIS, please click here to go to the alternative option: Week 2 Practical Alternate: Using AGOL for Population Mapping Start Q-GIS If you are not familiar with the Q-GIS environment, please watch our short video that explains its main components: Let’s start a new project. Click on Project –&gt; New. Save your project into your qgis folder as w2-pop-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. We will explain CRSs and using CRSs in GIS software v. programming in more detail next week. We will first focus on loading and joining the 2011 datasets. Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your 2011 boundary files. Here, we will select the London_Ward.shp dataset: Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. We can take a moment just to look at our Ward data - and recognise the shape of London. Can you see the City of London in the dataset? It has the smallest wards in the entire London area. With the dataset loaded, we can now explore it in a little more detail. We want to check out two things about our data: first, its Properties and secondly, its Attribute Table. The following short video explains these main components to using spatial data within Q-GIS. Right-click on the London_Ward layer and open the Attribute Table and look at how the attributes are stored and presented in the table. Explore the different buttons in the Attribute Table and see if you can figure out what they mean. Once done, close the Attribute Table. Right-click on the London_Ward layer and select Properties. Click through the different tabs and see what they contain. Keep the Properties box open. Before adding our population data, we can make a quick map of the wards in London - we can add labels and change the symbolisation of our wards. In the Properties box, click on the Symbology tab - this is where we can change how our data layer looks. For example, here we can change the line and fill colour of our Wards utilising either the default options available or clicking on Simple Fill and changing these properties directly. Keep the overall styling to a Single Symbol for now - we’ll get back to this once we’ve added the population data. You can also click on the Labels tab - and set the Labels option to Single labels. Q-GIS will default to the NAME column within our data. You can change the properties of these labels using the options available. I’ll add a thin buffer to my labels and change the font to Futura and size 8. You can click Apply to see what your labels look like. In my case, incredibly busy!: As its very busy, you may actually want to remove the labels from your dataset for the remaining processing - but hopefully this helps you understand how to add simple labels to your data. We’ll show you some more complex approaches in Week 10. Click OK once you’re done changing the Symbology and Label style of your data to return to the main window. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using Q-GIS right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 shapefile. First, we need to add the 2011 population data to our project. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text Layer. Click on the three dots button again and navigate to your 2011 population data in your working folder. Your file format should be set to csv. You should have the following boxes clicked: Decimal separator is comma; First record has field names’ Detect field types; Discard empty fields. Q-GIS does many of these by default, but do double-check! Set the Geometry to No geometry (attribute only table). Then click Add and Close*. You should now see a table added to your Layers box. We can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datatsets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. Right-click on your London_Ward layer –&gt; Properties and then click on the Joins tab. Click on the + button. Make sure the Join Layer is set to ward_population_2011. Set the Join field to ward_code. Set the Target field to GSS_code. Click the Joined Fields box and click to only select the POP2011 field. Click on the Custom Field Name Prefix and remove the pre-entered text to leave it blank. Click on OK. Click on Apply in the main Join tab and then click OK to return to the main Q-GIS window. We can now check to see if our join has worked by opening up our London_Ward Attribute Table and looking to see if our wards now have a Population field attached to it: Right-click on the London_Ward layer and open the Attribute Table and check that the population data column has been added to the table. As long as it has joined, you can move forward with the next steps. If your join has not worked, try the steps again - and if you’re still struggling, do let us know. Now, the join that you have created between your ward and population datasets in only held in Q-GIS’s memory. If you were to close the program now, you would lose this join and have to repeat it the next time you opened Q-GIS. To prevent this from happening, we need to export our dataset to a new shapefile - and then re-add this to the map. Let’s do this now: Right-click on your London_Ward shapefile and click Export –&gt; Save Vector Layer as... The format should be set to an ESRI shapefile. Then click on the three dots buttons and navigate to your final folder and enter: ward_population_2011 as your file name. Check that the CRS is British National Grid. Leave the remaing fields as selected, but check that the Add saved file to map is checked. Click OK. You should now see our new shapefile add itself to our map. You can now remove the original London_Ward and ward_population_2011 datasets from our Layers box (Right-click on the layers –&gt; Remove Layer). The final thing we would like to do with this dataset is to style our dataset by our newly added population field to show population distribution around London. To do this, again right-click on the Layer –&gt; Properties –&gt; Symbology. This time, we want to style our data using a Graduated symbology. Change this option in the tab and then choose POP2011 as your column. We can then change the color ramp to suit our aesthetic preferences - Viridis seems to be the cool colour scheme at the moment, and we’ll choose to invert our ramp as well. The final thing we need to do is classify our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation. We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on the drop-down next to Mode, select Natural Breaks, change it to 7 classes and then click Classify. Finally click Apply to style your dataset. A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using Q-GIS, which makes it a good playground to see how certain parameters or settings can change your output. You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Load the respective Ward dataset as a Vector Layer Load the respective Population dataset as a Delimited Text File Layer (remember the settings!) Join the two datasets together using the Join tool in the Ward dataset Properties box. Export your joined dataset into a new dataset within your final folder. Style your data appropriately. To make visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with Q-GIS, so for now, you can leave your symbolisation to the default settings. Alternatively, to set all three datasets to the same breaks, you can do the following: Right-click on the ward_population_2019 dataset and navigate to the Symbology tab. Double-click on the Values for the smallest classifcation group and set the Lower value to 141 (this is the lowest figure across our datasets, found in the 2015 data). Click OK, then Click Apply, then Click OK to return to the main Q-GIS screen. Right-click again on the ward_population_2019 dataset but this time, click on Styles –&gt; Copy Styles –&gt; Symbology. Now right-click on the ward_population_2015 file, but this time after clicking on Styles –&gt; Paste Style –&gt; Symbology. You should now see the classification breaks in the 2015 dataset change to match those in the 2019 data. Repeat this for the 2011 dataset as well. The final thing you need to do is to now change the classification column in the Symbology tab for the 2015 and 2011 datasets back to their original columns and press Apply. You’ll see when you first load up their Symbology options this is set to POP2019, which of course does not exist within this dataset. And that’s it - you can now make direct visual comparisons against your three maps. As you’ll be able to see, population has grown considerably in the London wards and there is are a few spatial patterns to this. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint: Click on Project –&gt; Import/Export –&gt; Export to Image and save your final map in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) QGIS, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. Acknowledgements Part of this page is adapted from CASA0005 and Introduction to GIS by Manuel Gimond. "],["cartography-and-visualisation-i.html", "3 Cartography and Visualisation I", " 3 Cartography and Visualisation I Welcome to Week 3 in Geocomputation! Well done on making it through Week 2 - and welcome to what is a more practical introduction to GIScience where we will be focusing on: how to make a good map. It’s not quite as “light” as promised, but this and the previous week will hold you in good stead as you come to learn about more technical analytical techniques after Reading Week. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 4 will be shorter in content, therefore you will have time to catch up after the seminars at the start of Week 4. The seminar will go through aspects of this week’s work, so it will still be incredibly useful if you do not manage to complete everything we outline in this workshop. Week 3 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with Cartography and Visualisation, where we have three areas of work to focus on: Map Projections Data Visualisation The Modifiable Areal Unit This week’s content is split into 4 parts: Coordinate Systems and Map Projections (40 minutes) Effective Data Visualisation (40 minutes) The Modifiable Areal Unit Problem (20 minutes) Practical 2: Mapping Crime Across London Wards and Boroughs (1 hour) Videos can be found in Parts 1-3, alongisde Key and Suggested Reading. This week, your 1 assignment is creating the final output from our practical. Part 4 is our Practical for this week, where you will be introduced to using the Map Composer with Q-GIS and apply the knowledge gained in the previous parts from Parts 1-3 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical but we recommend reading through the Q-GIS practical as unfortunately we are unable to repeat everything within the AGOL practical. Learning Objectives By the end of this week, you should be able to: Explain what a Geographic Coordinate System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. We will build on the data analysis we completed last week and create accurate maps that show changes in crime across our London wards. Coordinate Systems and Map Projections Maps, as we saw last week, are representations of reality. But not only are they are designed to represent features, processes and pheonomena in their ‘form’, they also need to represent, with fidelity, their location, shape and spatial arrangement. To be able to locate, integrate and visualise spatial data accurately within a GIS system or digtal map, spatial data needs to have two things: 1. A coordinate reference system (often written as CRS) 2. An associated map projection A CRS is a reference system that is used to represent the locations of the relevant spatial data within a common geographic framework. It enables spatial datasets to use common locations for co-location, integration and visualisation. Each coordinate system is defined by: Its measurement framework Unit of measurement (typically either decimal degrees or feet/metres, depending on the framework) Other measurement system properties such as a spheroid of reference, a datum, and projection parameters Its measurement framework will be one of two types: Geographic: in which spherical coordinates are measured from the earth’s center Planimetric: in which the earth’s coordinates are projected onto a two-dimensional planar surface. For planimetric CRS, a map projection is required. This projection details the mathematical transformation to project the globe’s three-dimensional surface onto a flat map. As a result, there are two common types of coordinate systems that you will come across when using spatial data: 1. Geographic Coordinate Systems (GCS): a global or spherical coordinate system such as latitude-longitude. 2. Projected Coordinate System (PCS): a CRS which has the mechanisms to project maps of the earth’s spherical surface onto a two-dimensional Cartesian coordinate plane. These PCS are sometimes reference to as map projections, although combine both location and the projection in their use. Understanding Coordinate Systems Slides | Video on Stream In summary, a GCS defines where the data is located on the earth’s surface, whereas a a PCS tells the data how to draw on a flat surface, like on a paper map or a computer screen. As a result, a GCS is spherical, and so records locations in angular units (usually degrees). Conversely, a PCS is flat, so it records locations in linear units (usually meters): Visualising the differences between a GCS and a PCS. Image: Esri For a GCS, graticules are used as the referencing system, which are tied directly to the Earth’s ellipsoidal shape. In comparison, within a PCS, a grid is a network of perpendicular lines are used, much like graph paper, which are then superimposed on a flat paper map to provide relative referencing from some fixed point as origin. Your data must have a GCS before it knows where it is on earth. But, whilst theoretically projecting your data is optional, projecting your map is not. Maps are flat, so your map will have a PCS in order accurately draw the data. In most GIS systems, a default projection will be used to draw the map and therefore the system will project your data to match this projection. For example, if you do not specify the projection of the map or data, both ArcGIS and Q-GIS will draw your map and corresponding data using a pseudo Plate Carrée or ‘geographic’ projection. The Plate Carrée Projection This projection is actually just latitude and longitude represented as a simple grid of squares and called pseudo because it is measured in angular units (degrees) rather than linear units (meters). It is easy to understand and easy to compute, but it also distorts all areas, angles, and distances, so it is senseless to use it for analysis and measurement and as a result, before you start your work, you should choose a different PCS! Which CS you will choose will depend on where you are mapping: most often, you will not need to choose a GCS as the data you are using was already collected and/or stored in a pre-selected system. For example, all GPS receivers collect data using only one datum or coordinate system, which is WGS84. Therefore any GPS data you use will be provided in the WGS84 GCS. However, you will often need to choose your PCS: which PCS you use depends on where you are mapping, but also the nature of your map — for example, should you distort area to preserve angles, or vice versa? For example, if you are using GPS data from the U.K, it is likely that you will transform this data into British National Grid (a PCS). Understanding Map Projections Either CS provides a framework for defining real-world locations - however, when it comes to much of GIScience and spatial analysis work, we will use a PCS to help locate, project, analyse and visualise our data in 2D. To locate, project, analyse and visualise our data in 2D, the PCS has, through mathematical transformations known as map projections, transformed the surface of our three-dimensional earth into a two-dimensional map canvas (whether paper or digital). This ability to create a flat surface from a 3D sphere is however not so simple! From a classic geographical metaphor, the easiest way to think about this is to think about peeling an orange - how could you peel an orange to ultimately result in a flat (preferably square/rectangular - computers really like squares!) shape? Well, luckily, you don’t need to think too hard about it - as Esri’s resident cartographer John Nelson (another Twitter recommendation) has done it for us: Trying to flatten an orange - our earth - into a flat map. Images: John Nelson, Esri As he shows, to create just a flat version of our earth from the spheriod itself, it takes some very interesting shapes and direction maniuplation - let alone achieving a rectangle! (You can see the original blog post these images are taken from here.) To create a classic square or rectangular map that we are so used to seeing, we have to use other geometric shapes that can be flattened without stretching their surface to help determine our projection. These shapes are called developable surfaces and consist of three types: Cylindrical Conical Plane The three types of projection families: cyclindrical, conical and plane. Image: QGIS However when any of using these shapes to representing the earth’s surface in two dimensions, there is always some sort of distortion in the shape, area, distance, or direction of the data. This distortion is explained through Vox’s excellent video: Why all world maps are wrong We can actually test out this distortion ourselves. You can head to The True Size (https://thetruesize.com) and see how our use of the Web Mercator has skewed our understanding of the size of countries in respect to one another. In addition, I highly recommend looking through this short (2 minutes!) blog post where a keen mapper got creative with his own orange peel: Blog post: Visualising the distortion of web mercator maps with an orange peel, Chris M. Whong, Online here Different projections can therefore cause different types of distortions. Some projections are designed to minimize the distortion of one or two of the data’s characteristics. A projection could, for example, maintain the area of a feature but alter its shape. Our second short lecture explains how to think through choosing a map projection: Choosing a Map Projection Slides | Video on Stream As explained in our lecture, each map projection therefore has advantages and disadvantages. Ultimately, the best projection for a map depends on the scale of the map, and on the purposes for which it will be used. As the excellent Q-GIS Projection documentation explains: For example, a projection may have unacceptable distortions if used to map the entire African continent, but may be an excellent choice for a large-scale (detailed) map of your country. The properties of a map projection may also influence some of the design features of the map. Some projections are good for small areas, some are good for mapping areas with a large East-West extent, and some are better for mapping areas with a large North-South extent. When it comes to choosing your map projection, think about: Is there a default projection for your area of study (e.g. London and British National Grid)? What analysis are you completing? What properties are important to this analysis? At what scale and direction are you visualising your data? What is critical to remember though, is that map projections are never absolutely accurate representations of our spherical earth. As a result of the map projection process, every map shows distortions of angular conformity, distance and area. Why should we care about projection systems? In summary, the projection system you use can have impact on both analytical aspects of your work, e.g. using measurement tools effectively, such as buffers, alongside visualisation. It is usually impossible to preserve all characteristics at the same time in a map projection. This means that when you want to carry out accurate analytical operations, you will need to use a map projection that provides the best characteristics for your analyses. For example, if you need to measure distances on your map, you should try to use a map projection for your data that provides high accuracy for distances. Furthermore, you need to be aware of the CS that your data is in, particularly when you are using multiple datasets. In order to analyse and visualise data accurately together, they must all be in the same CS. Transforming/Reprojecting Data If you are using datasets that are based on different geographic or projected coordinate systems, you will need transform all your data to one singular system: these are known as transformations. Between any two coordinate systems, there may be zero, one, or many transformations. Some geographic coordinate systems do not have any publicly known transformations because that information is considered to have strategic importance to a government or company. For many GCS, multiple transformations exist. They may differ by areas of use or by accuracies. Accuracies will usually reflect the transformation method. A geographic transformation is always defined in a particular direction, like from NAD 1927 to WGS 1984. Transformation names will reflect this: NAD_1927_To_WGS_1984_1. The name may also include a trailing number, as the above example has _1. This number represents the order in which the transformations were defined. A larger number does not necessarily mean a more accurate transformation. Even though a geographic transformation has a built-in directionality, all transformation methods are inversible. That is, a transformation can be used in either direction. Moving for with CRS in Geocomputation Keep in mind that map projection is a very complex topic. There are hundreds of different projections available that aim to portray a certain portion of the earth’s surface as accurately as possible on a digital screen/flat paper. In reality, the choice of which projection to use will often be made for you. When it comes to geocomputation and spatial analysis, you need to choose your CRS carefully - thinking through what is appropriate for your dataset, incuding what analysis you are completing and at what scale. You will find there are specific recommendations by country and, fortunately for us, most countries have commonly used projections. This is particularly useful when data is shared and exchanged as people will follow the national trend. Often, most countries will utilise the relevant zone within the Universal Transverse Mercator. In addition, a great resource is Esri’s documentation on Choosing a Map Projection. The Tyranny of Web Mercator One thing to watch out for though is the general (over)reliance on what is known as the Pseudo-Mercator projection (EPSG:3857) by web applications such as Google Maps. The projected Pseudo-Mercator coordinate system takes the WGS84 coordinate system and projects it onto a square. (This projection is also called Spherical Mercator or Web Mercator.) This method results in a square-shaped map but there is no way to programmatically represent a coordinate system that relies on two different ellipsoids, which means software programs have to improvise. And when software programs improvise, there is no way to know if the coordinates are consistent across programs. This makes EPSG:3857 great for visualizing on computers but not reliable for data storage or analysis. Luckily for us in Geocomputation, for the majority of our work, we will be using the British National Grid for our mapping and analysis as we are focusing on analysis on London. In this week’s practical, we will look at how we can reproject our spatial data from one a GCS to a PRS (in this case WGS84 to OSGB1936). Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 4: Geo-referencing. Optional: The Power of the Map Maps and map projections have had a long and complicated history with our politics and geopolitics. For example, whilst maps have existed in many forms prior to the periods, we cannot ignore their signficant use for land acquisition and resource exploitation during the “Age of Discovery” and resulting colonialism eras. There is significant power embedded within a map and, even to this day, as we see with the use of the Mercator projection in web technology, a map can be a substantial propaganda tool when it comes to political issues. Google Maps, for example, has found itself at the centre of various border disputes across the world - resulting, in several occasions, with troop mobilisation and threats of war: By misplacing a portion of the border between Costa Rica and Nicaragua, Google effectively moved control of an island from one country to the other and was cited as the justification for troop movements in the region in 2010. The Washington Post, 2020 To further avoid this, Google has created a new techno-political approach within its Google Maps platform in that the world’s borders will look different depending on where you’re viewing them from. You can read more about this a recent article by The Washington Post: Google redraws the borders on maps depending on who’s looking (10 minutes). Maps therefore are never true representations of reality, but will always include some bias - after all, maps are still very much made by humans. Whilst we won’t cover this in any more detail in our lecture or practical content this week, we do hope you enjoy discussing these issues in your Study Group sessions. In addition, there are many excellent books on this power of maps, including Denis Wood’s The Power of Maps and follow-up, Rethinking the Power of Maps and Mark Monmonier’s How to Lie with Maps. These books all outline how both paper and modern digital maps offer opportunities for cartographic mischief, deception, and propaganda. If you’d like to avoid reading for a little longer, I would also highly recommend this excerpt from the “before your time” show, the West Wing, which summarises quite a few of the debates well: Effective Data Visualisation In addition to choosing the correct map projection for your spatial data and map, to visualise your data correctly as a map - for visual analysis and publishing - you need to consider: How you represent your spatial data effectively. How you present this data on a map that communicates your data and analysis accurately. We will first focus on the latter aspect and look at how you can achieve effective data visualisation, including how to make a good map as well as detailing the common cartographic conventions we’d expect you to include in your map. Then we look at common types of spatial data and focus on how we can accurately represent event and survey data that are commonly aggregated to areal units (such as the Administrative Geographies we came across last week) for use within choropleth maps. Cartographic Conventions Making a good map is a highly subjective process - what you think looks good versus what someone else thinks looks good maybe entirely different. That’s why there is a whole discipline out there on cartography - it’s also why good data visualisation skills are becoming essential within data scientist roles. As a result, I can highly recommend taking the Cartography and Visualisation module by Prof James Cheshire next year! At its most fundamental, a map can be composed of many different map elements. They may include: The main map Map graticules A legend (including symbols) A title A scale bar or indicator An orientation indicator, i.e. a North Arrow An inset map (to locate your map within a wider area) Data Source information Any ancillary information These elements are all part of the expected cartographic conventions, i.e. what should be included on/within your map in order to accurately convey all the information contained within your visualisation. Map elements. Image: Manuel Gimond However, not all elements need to be present in a map at all times. In fact, in some cases they may not be appropriate at all. A scale bar, for instance, may not be appropriate if the coordinate system used does not preserve distance across the map’s extent. Knowing why and for whom a map is being made will dictate its layout: If it’s to be included in a paper as a figure, then simplicity and restraint should be the guiding principles. If it’s intended to be a standalone map, then additional map elements may be required, such as customised borders, graphics etc. Knowing the intended audience should also dictate what you will convey and how: If it’s a general audience with little technical expertise then a simpler presentation may be in order. If the audience is well versed in the topic, then the map may be more complex. Ultimately, to make a good map there are several rules you can follow: Visual hierarchy: Making sure the most important elements are the most visible on the map (e.g. size, placement on map, colour scheme). Colour schemes: Keeping colour schemes simple (less than 12 colour at max) and representative of the data you are showing (more on this later) as well as suitable to all audiences (e.g. being aware of mixing colours indetectable to those colourblind/visually impaired) Scale bars and north arrows: Should be used judiciously! They are not needed in every map, nor do they need to be extremely large - just readable. I advise trying to locate the two together and keeping their design as simple as possible. Title and other text elements: Again, less is more! Never use “A map of…” in your title - we know it’s a map! Keep font choices simple and reflective of the topic you are mapping. Titles are not needed on maps with figure captions. Make legends readable - including simplifying their values. Utilise font size effectively to ensure communication of the most important aspects. The following short lecture explains in more detail how to make a good map: Cartographic Conventions and Effective Data Visualisation Slides | Video on Stream Representing Spatial Data The second aspect of creating effective maps is to ensure that you are representing the type of data you are using effectively and accurately. As we saw last week, spatial data itself is only a representation of reality. Some of the types of data we use can be very close representations of reality, such as ‘raw’ geographic data (including satellite imagery or elevation models), whilst other datasets, when used in maps, may be far abstract representations of reality. The different common types of spatial data you might come across in spatial analysis are outlined in the table below: Common Types of Spatial Data Data Type Examples Digital Representation ‘Raw’ Geographic Data Satellite Imagery LIDAR/RADAR imagery Environmental Measurements (e.g. elevation, air quality, water levels) Raster/Grids Coordinates / Point Data, with attributes Processed or Derived Spatial Data Geographic Reference Data (e.g. buildings, roads, rivers, greenspace) Gridded Population (Density) Data Digital Elevation Models Air Quality Maps Points, Lines and Polygons Raster/Grids (Spatial) Event (Count) Data Human Activities ( e.g. crime, phone calls, house sales) Scientific Recordings (e.g. animal and plant sightings) Coordinates / Point Data, with attributes Statistical Survey or Indicator Data Human Characteristics (e.g. demographic, socio-economic &amp; health information) Scientific Recordings (e.g. total animal counts, leaf size measurements) Voting Tabular Data, representative at a specific spatial aggregate scale, i.e. areal unit Whilst we will come across a variety of these types of spatial data on this course, our main focus for the first few weeks are looking at Event and Statistical data - because these are the two types of data that are primarily used within the most common data visualisation map tool: a choropleth map. Choropleth Maps At its most basic, a choropleth map is a type of thematic map in which a set of pre-defined areas is colored or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population density or crime rate. When using either Event Data or Statistical Data, we tend to aggregate these types of data into areal units, such as the Administrative Geographies we came across last week, in order to create these choropleth maps. Because we see choropleth maps in our everyday lives, choropleth maps, I would say, out of any type of map-based data visualisation are the maps most vulnerable to poor use and data representation. We often think it’s a simple case of linking some table data with our areal units and then choosing some pretty colour scheme… An Example Choropleth: London’s Wasted Heat Energy at the MSOA scale. The question is: do you think it looks good? What would you change? Image: Mapping London …However, within a choropleth map, many decisions need to be made in terms of thinking through their classification (categorical or continuous/graduated), the ‘class breaks’ used, as well as the type of colour schemes used. Furthermore, a key challenge to using choropleth maps is that often the areal units we use are not of equal area - as a result, we have to be careful in how we represent our chosen dataset. Showing population as a ‘raw’ geographic fact across London Wards as we did last week, for example, would actually be a big no-no in terms of mapping population. Instead, we would want to show the population density - by normalising our population by the area of each ward. What’s still missing from this map? London Ward Population Density 2019. Data: ONS Without taking these normalisation approaches, we can create incredibly misleading maps. At the most basic, our brain sees the larger areal units within our map as having more of whatever quantity we are representing, irrespective of thinking through the underlying area (and/or population) it is actually representing. This was common amongst the US election maps, for example, where many of the Republican states have a large landmass - but ultimately a low population. Therefore, when representing the results of the election as a categorical choropleth, it presents an overwhelming Republican landslide. However, as we all know, whilst the Party won the Electoral College vote, the Democrats actually won the Popular Vote by 3 million votes. Hence, when mapping by number of votes rather than state outcome, a different message is conveyed, as we see below. Alas, despite this difference in total votes, the US runs an Electoral College System and in the end, the winner is the winner of the Electoral College vote and no map coud or can change that! Different approaches to mapping the 2016 election result in different information communicated (L-&gt;R: Business Insider, Time, xkcd) Despite their various challenges, choropleth maps can be increidbly useful tools. We provide a more detailed introduction to how to create choropleth maps in the following lecture: An Introduction to Choropleth Maps Slides | Video on Stream The Modifiable Areal Unit Problem The final aspect of good map-making we will cover actually focuses on how we process and resultantly analyse our data when we aggregate individual event or statistical data to areal units. When using choropleth maps to represent aggregated data, there are three key analytical challenges you need to be aware of, in order to not fall into the “trap” of the first two, whilst also thinking about ways to address the allter. The are three key challenges: Ecological Fallacy (EF): EF occurs when you try to make inferences about the nature of individuals based on the group to which those individuals belong (e.g. administrative unit). This applies when looking at correlations between two variables when using administrative geographies or looking at averages within these units. Whilst your areal unit may represent the aggregation of individual level data, you can not apply your findings from the analysis of this map to the individuals directly. You can only apply your conclusions to the area that you have aggregated by, e.g. at the Ward scale. The Modifiable Areal Unit Problem (MAUP): Spatial data is scale dependent - when data are tabulated according to different zonal systems at different scales and are then analyzed, it is unlikely that they will provide consistent results - even though the same variables are used and the same areas are ultimately analyzed. As a result, the results from your analysis are only relatable to those precise areal units used. This variability or inconsistency of the analytical results is mainly due to the fact that we can modify areal unit boundaries and thus the problem is known as the MAUP. It is one of the most stubborn problems in spatial analysis when spatially aggregated data are used. Fundamentally, you cannot extrapolate your findings at one scale to another, i.e. any conclusions drawn at the Ward level in London cannot be applied to the Borough level, even though, for example, your wards may “fit” within the Borough scales. Boundary Issues: Spatial data does not have “boundaries” - the use of artifical boundaries such as Administrative Geographies are indiscriminate to the spatial prcoesses that may actually underline the distribution of these phenomena at study. As a result, simply using these boundaries per se can bring about different spatial patterns in geographic phenomena - or simply disregard them in their entirety. We have to use Administrative Boundaries with care and think about the underlying processes we are trying to measure to see if we can account for these discriminatory issues. In summary, whenever you conduct spatial analysis using areal units – you cannot infer about the individuals within those units nor can you assume your findings will apply at coarser scales. You also need to take into account the “decisive and divisive” nature the use of areal units can have on individual level data when aggregating. We will begin to look at MAUP in this week’s practical and Week 4’s seminar and continue accounting for and considering its impact over the next few weeks of our analysis. A more detailed introduction to Administrative Geographies As we read and saw last week, an administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. Generally, they contain 4-5 levels of administrative boundaries, starting at Level 0, with the outline of the country, Level 1, the next regional division, Level 2, the division below that etc. Each country will have a different way of determining their levels and their associated names – and when you start to add in differentiating between urban and rural areas, it becomes a whole new level of complexity. What is important to know is that these geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. For the U.K, we can access the spatial data of our Administrative Geographies from data.gov.uk (and a few other sources). Any country with their own statistics or spatial office should have these datasets available. If not, you can find data (for pretty much all countries) at gadm.org, which allows you to download and use the data for non-commercial purposes. As a note of interest at this point, in the U.K., it is generally understood that for publishable research, we do not analyse data at a smaller scale than something called the Lower Super Output Area (LSOA). There is another administrative unit below the LSOA, known as the Output Area, which (again due to ensure confidentiality of data) has a minimum size of 40 resident households and 100 resident people but for particular types of research, this level of detail can still lead to unintended consequences, such as households being identified within the data. Practical 2: Mapping Crime Across London Wards and Boroughs The first half of this workshop has given you an in-depth introduction into how we can create a successful map, including understanding map projections, cartographic conventions and issues faced with the analysis of aggregated data at areal units. The practical component of the week puts some of these learnings into practice as we analyse crime rates within London at two different scales. The datasets you will create in this practical will be used in the Week 4 practical, so make sure to follow every step and export your data into your working and final folders (respectively) at the end. The practical component introduces you to point-in-polygon counts. You’ll be using these counts throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. Setting the scene: why investigate crime in London? Over the next few weeks, we will look to model driving factors behind crime across London from both a statistical and spatial perspective. As Reid et al (2018) explain: Spatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime. Crime mapping allows researchers and practitioners to explore crime patterns, offender mobility, and serial offenses over time and space. Within the context of local policing, crime mapping provides the visualization of crime clusters by types of crimes, thereby validating the street knowledge of patrol officers. Crime mapping can be used for allocating resources (patrol, specialized enforcement) and also to inform how the concerns of local citizens are being addressed. Mapping crime and its spatial distribution is of significant interest to a variety of stakeholders - it also serves as a relatable and understandable geographical phenomena for learning different types of spatial analysis techniques as well as many of the ‘nuances’ analysts face when using this type of ‘event’ data. As a result, within this practical, we are actually going to answer a very simple question: Does our perception of crime (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the ‘ecological fallacy’ mistake of assuming patterns at the ward level are the same at the borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. To test this, we will use these two administrative geographies (borough and ward) to aggregate crime data for London in 2020. Here we will be looking specifically at a specific type of crime: the theft from a person. Finding our datasets As we saw last week, accessing data within the UK, and specifically for London, is relatively straight-forward - you simply need to know which data portal contains the dataset you want! Crime Data For our crime data, we will use data directly from the Police Data Portal, which you can find at https://data.police.uk/. This Data Portal allows you to access and generate tabular data for crime recorded in the U.K. across different the different Police Forces since 2017. In total, there are 45 territorial police forces (TPF) and 3 special police forces (SPF) of the United Kingdom. Each TPF covers a specific area in the UK (e.g. the \"West Midlands Police Force), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a speciic area, we need to know which Police Force covers the Area of Interest (AOI) for our investigation. When you look to download crime data for London, for example, there are two territorial police forces working within the city and its greater metropolitan area: The Metropolitan Police Force (The Met), which covers nearly the entire London area, including Greater London The City of London (COL) Police, which covers the City of London. The Met has no juridiction in the COL. You therefore need to decide if you want to include an analysis of crime in the City of London or not - we will in our current study. We’ll get to download this dataset in a second! Population Data After what we’ve learnt about above, we know that if we want to study a phenomena like crime (and aggregate it to an areal unit as we will do today!), we will need to normalise this by our population. Luckily, we already have our Ward Population sorted from last week, with our ward_population_2019.shp that should be currently sitting in your final data folder. If it is not, you can download our shapefile here. Remember to unzip it and, for now, store it in your final data folder. In addition to our ward level dataset, we also want to generate the same type of shapefile for our London boroughs, i.e. a borough_ward_population_2019.shp, utilising the same approach as last week, joining our population table data to our borough shape data. To do this, we need to know where to get both our required datasets from - luckily, you’ve already got borough shape data in your raw/boundaires/2011 folder. Therefore, it is just a case of tracking down the same Mid-Year Estimates (MYE) for London Boroughs as we did for the wards, which with the ONS’s central MYE database, this also won’t be too difficult! So let’s get going! Download and process datasets As outlined above, to get going with our analysis, we need to download both the population data for our boroughs and the 2020 crime data for our two police forces in London. Let’s tackle the population data first. 1) Borough Population Through a quick search, we can find our Borough Population table data pretty much in the same place as our Ward data - however it is a separate spreadsheet to download. Navigate to the data here. Download the Mid-2019: April 2020 local authority district codes xls. Open the dataset in your spreadsheet editing software. Navigate to the MYE2-Persons tab. Utilising your preferred approach, extract: Code, Name, Geography and All ages data for all London boroughs. For me, the simplest way is to add a filter to row 5, and from this filter, in the Geography column select only London Boroughs: You should have a total of 33 boroughs. Once you have your 33 boroughs separated from the rest of the data, copy the columns (Code, Name, Geography and All ages) and respective data for each borough into a new csv. Remember to format the field names as well as the number field for the population as we did last week. Save as a new csv in your working population folder: borough_population_2019.csv. 2) Ward Population As mentioned above, you should have a ward_population_2019.shp file within your final data folder. As we’ll be using this dataset in our practical, we would like to make sure that we keep a version of this data in its current state, just in case we make a mistake whilst processing our dataset. As a result, we should create a copy of this dataset within our working folder, that we can use for this practical. Copy and paste over the entire ward_population_2019.shp from your final data folder to your working data folder. Don’t forget to copy over ALL the files. 3) Crime Data We will now head to the Police Data Portal and download our crime data… …or maybe not! As I said at the start of last week’s practical: We’re going to start cleaning (the majority of) our data from the get-go. However, with our crime data, the processing that is required from you right now is exhaustive to do manually - and far (far!) easy to do using programming. Essentially, all of our data that we will download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: For our data processing therefore, you would need to merge all of this crime into a single csv. Now you could do this manually by copying and pasting each csv into a new csv (24 times) - or you can do it through a few lines of code. However, you’ve already read through a lot today, so we’ll save learning Command Line tools for next week, where we’ll find out just how quick it can be to merge csvs! Instead, you can find the pre-merged and pre-filtered spreadsheet here. Note, I filtered the data to only contain data on theft crime, rather than all types of crime in London. There are however a few caveats in our crime data, that we’ll explain below - but these might not become clear until you start using the raw dataset yourself next week. For now, make sure you have downloaded the london_crime_theft_2020 csv linked here. Copy this csv into a new folder in your raw data folder called: crime. Downloading and using crime data from data.police.uk To download data for all of London for 2020, you follow these simple steps: As you can see, it is a simple process of selecting the Police Forces and months for which you want data for - and then a csv for each of these will be generated. 1) Data Structure Once downloaded, you can open up the csv to see what the data contains. Each crime csv contains at least 9 fields: Field(s) Meaning Reported by The force that provided the data about the crime. Falls within At present, also the force that provided the data about the crime. Longitude and Latitude The anonymised coordinates of the crime. LSOA code and LSOA name References to the Lower Layer Super Output Area that the anonymised point falls into, according to the LSOA boundaries provided by the Office for National Statistics. Crime type One of the crime types used to categorise the offence. Last outcome category A reference to whichever of the outcomes associated with the crime occurred most recently. Context A field provided for forces to provide additional human-readable data about individual crimes. For us, the main fields we are interested include: Longitude and Latitude (for plotting as points) LSOA code/name (for aggregating to these units without plotting) Crime Type (to filter crime based on our investigation) 2) Location Anonymisation When mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime — not the exact place that it happened. This displacement occurs to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household. Interestingly enough, the police also convert the data from their recorded BNG eastings and northings into WGS84 latitude and longitude ( hence why we’ll need to re-project our data in this practical). 3) Coding of Crimes into 14 Categories Each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. We can use these crime types to filter our crime specific to our investigation - in our case theft. Now we have all our data ready, let’s get mapping! Using Q-GIS to map our crime data If you do not have access to Q-GIS, please click here to go to the alternative option: Week 3 Practical Alternate: Using AGOL for Crime Mapping Start Q-GIS Click on Project –&gt; New. Save your project into your qgis folder as w3-crime-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. Compared to last week, you should now know what EPSG:27700 means! Shortcut to CRS on Q-GIS To access and set the project CRS quickly in Q-GIS, you can click on the small CRS button in the bottom-right corner in Q-GIS: Now we have our Project CRS set, we’re now ready to start loading and processing our data. Load Ward Population data Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your ward_population_2019.shp in your working folder. Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. Load Borough shape and population data and join! We now need to create our Borough population shapefile - and to do so, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Load the respective Borough dataset as a Vector Layer (found in your raw data folder -&gt; boundaries -&gt; 2011 -&gt; London_Borough_Excluding_MHW.shp). Load the respective Population dataset as a Delimited Text File Layer (Remember the settings, including no geometry! This one is found in your working folder) Join the two datasets together using the Join tool in the Borough dataset Properties box (remember which fields to use, which to add and to remove the prefix - look back at last week’s instructions if you need help). Export your joined dataset into a new dataset within your working folder: borough_population_2019.shp. Make sure this dataset is loaded into your Layers / Added to the map. Remove the original Borough and population data layers. Load and project our crime data We now are ready to load and map our crime data. We will load this data using the Delimited Text File Layer option you would have used just now to load the borough population - but this time, we’ll be adding point coordinates to map our crime data as points. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text File Layer. With File select as your source type, click on the small three dots button and navigate to your all_theft_2019.shp in your raw -&gt; crime folders. Click on the .csv file of this dataset and click Open. In *Record and Fields Options**, ensure it is set to CSV, untick Decimal separator is comma and tick First record has field names, Detect field types and Discard empty fields. In Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84, a.k.a. the GCS of lat and lon! Click Add. But WAIT! We are using the wrong CRS for our project?! Surely, we need everything to be in BNG? As you click Add, you should see that you get a pop-up from Q-GIS asking about transformations - we read about these earlier and they are the mathematical algorithms that convert data from one CRS to another. And this is exactly what Q-GIS is trying to do. Q-GIS knows that the Project CRS is BNG but the Layer you are trying to add has a WGS84 CRS. Q-GIS is asking you what transformation it should use to project the Layer in the Project CRS! This is because one key strength (but also problem!) of Q-GIS is that it can project \"on the fly - what this means is that Q-GIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use. But you must note that this transformation is only temporary in nature and as a result, it is not a full reprojection of our data. Map Projections in Q-GIS The following is taken from the Q-GIS’s user manual section on Working with projections. Every project in QGIS also has an associated Coordinate Reference System. The project CRS determines how data is projected from its underlying raw coordinates to the flat map rendered within your QGIS map canvas. By default, QGIS starts each new project using a global default projection. This default CRS is EPSG:4326 (also known as “WGS 84”), and it is a global latitude/longitude based reference system. This default CRS can be changed both permanently, for example, to British National Grid for all future projects, or for that specific project, as we have done in our two practicals. QGIS supports “on the fly” CRS transformation for both raster and vector data. This means that regardless of the underlying CRS of particular map layers in your project, they will always be automatically transformed into the common CRS defined for your project. Behind the scenes, QGIS transparently reprojects all layers contained within your project into the project’s CRS, so that they will all be rendered in the correct position with respect to each other! These reprojections are only temporary and are not permanently assigned to the dataset it is reprojecting - only to the project. As a result, we should be aware of this when using data across different projects and/or GIS systems and always remember what the data’s original or “true” CRS is! This reprojection is also using computer memory, therefore, if you are to analyse large datasets (such as our crime dataset), it makes sense to reproject our data to have it permanently in the same CRS as our project. For now, let’s use the on-the-fly projection for now and utilise Q-GIS’s recommendation of the +towgs84=446.448.... transformation. This transformation should be built-in to your Q-GIS transformation library, whereas some of the more accurate options would need installation. For now, given the displacement of our data in the first place, this transformation is accurate enough for us! Click to use the +towgs84=446.448.... transformation and click through the OKs to return to the main Q-GIS screen. You should now see your crime dataset displayed on the map: We can test the ‘temporary’ nature of the projection by looking at the CRS of the all_theft_2020 layer: Right-click on the all_theft_2020 layer then select Properties -&gt; Information and then look at the associated CRS. You should see that the CRS of the layer is still WGS84. Yup, Q-GIS is definitely projecting our data on-the-fly! We want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the same CRS as our administrative datasets, i.e. British National Grid. This also means we’ll have the dataset to use in other projects, just in case. Back in the main Q-GIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows: Input Layer: all_theft_2020 Target CRS: Project CRS: EPSG: 27700 Reprojected: Click on the three buttons and Save to File to create a new data file. Save it in your working folder as all_crime_2019_BNG.shp Click Run and then close the tool box. You should now see the new data layer added to your Layers. Q-GIS can be a little bit buggy so when it creates new data layers in your Layers box, it often automates the name, hence you might see your layer added as Reprojected. It does this with other management and analysis tools, so just something to be aware of! For now, let’s tidy up our map a little. Remove the all_theft_2020 original dataset. Rename the Reprojected dataset to all_theft_2020. Now we have an organised Layers and project, we’re ready to start our crime analysis! Counting Points-in-Polygons with Q-GIS The next step of our analysis is incrediby simple - as Q-GIS has an in-built tool for us to use. We will use the Count Points in Polygons in the Analysis toolset for Vector data to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Click on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons. Within the toolbox, select the parameters as follows: Polygons: borough_population_2019 Points: all_theft_2020 (Note how both our data layers state the same CRS!) No weight field or class field Count field names: crimecount Click on the three dot button and Save to file: working -&gt; borough_crime_2020.shp Click Run and Close the box. You should now see a Count layer added to your Layers box. Let’s go investigate. Click the checkbox next to all_theft_2020 to hide the crime points layer for now. Right-click on the Count layer and open the Attribute Table. You should now see a crimecount column next to your POP2019 column. You can look through the column to see the different levels of crime in the each borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software. Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. To get our crime rate statistic, we’re going to do our first bit of table manipulation in Q-GIS, woohoo! With the Attribute Table of your Count layer still open, click on the pencil icon at the start of the row. This pencil actually turns on the Editing mode in Q-GIS. The editing mode allows you to edit both the Attribute Table values and the geometry of your data. E.g. you could actually move the various vertex of your boroughs whilst in this Editing mode if you like! When it comes to the Attribute Table, it means you can directly edit existing values in the table or create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake. Using the Editing mode is the correct approach to editing your table, however, it might not always be the approach you use when generating new fields and, as we all are sometimes, a little lazy. (This may be a simple case of “Do what I say, not what I do!”) Let’s go ahead and add a new field to contain our Crime Rate. Whilst in the Editing mode, click on New Field button (or Ctrl+W/CMD+W) and fill in the Field Parameters as follows: Name: crime_rate Comment: leave blank Type: Decimal number Length: 10 Precision: 0 Click OK. You should now see a new field added to our Attribute Table. What did all this mean? Understanding how to add new fields and their parameters rely on you understanding the different data types we covered last week - and thinking through what sort of data type your field needs to contain. In our case, we will store our data as a decimal to enable our final calculation to produce a decimal (an integer/integer is likely to produce a decimal) but we will set the precision to 0 to have zero places after our decimal place when the data is used. That’s because ultimately, we want our crime rate represented as an integer because, realistically, you can’t have half a crime! Calculating a decimal however will allow us to round-up within our calculations. The empty field has NULL populated for each row - so we need to find a away to give our boroughs some crime rate data. To do this, we will calculate a simple Crime Rate using the Field Calculator tool provided by Q-GIS within the Attribute Table. We will create a crime rate that details the number of crimes per 10,000 people in the borough. In most cases, a crime rate per person will create a decimal result less than 1 which not only will not be stored correctly by our crime_rate field but, for many people, a decimal value is hard to interpret and understand (yes, I know, but we are aiming to make maps that are accessible to everyone…). Therefore going for a 10,000 person approach allows us to calculate and represnt the crime rate using full integers for both our borough and ward scales as we’ll see later. This calculation was determined by a bit of a trial and error by me within this practical, so it is something you’d need to consider and change for future research you might do! Whilst still in the editing mode, click on the Abacus button (Ctrl + I / Cmd + I), which is actaully the Field Calculator. A new pop-up should load up. We can see there are various options we could click at the top - including Create a new field. Ah! So we could in fact create a new field directly from the field calculator which would help us combine these two steps in one and quicken our workflow! For now, in the Field Calculator pop-up: Check the Update existing field box. Use the drop-down to select the crime_ratefield. In the Expression editor, add the following expression: ( “crimecount” / “POP2019” ) * 10000 You can type this in manually or use the Fields and Values selector in the box in the middle to add the fields into the editor. Once done, click OK. You should then return to the Attribute Table and see our newly populated crime_rate field - at the moment, we can see the resulting calculations stored as decimals. Click on the Save button to save these edits - you’ll see the numbers turn to integers. Click again on the Pencil button to exit Editing mode. We now have a crime_rate column to map! Before moving to the next step, if you would like, go ahead and symbolise your boroughs by this crime_rate. Tips for Symbolisation When in the Symbology tab and after selecting Graduated as your symbolisation option, click on the histogram tab and load the values to see the distribution of your data. You can also edit the lines of the borough to a colour of your choice. You should also make sure your new borough crime rate layer has been renamed from the default Count layer name Q-GIS has given it. Rename your borough crime rate layer has been renamed from the default Count layer name to Borough Crime Rate. Great! We now have our Borough crime rate dataset ready for mapping and analysis - we just now need to repeat this process to have our Ward dataset. Repeat the above processes to create a crime_rate column within our Ward dataset ready for mapping and analysis. Tips for Repetition in Q-GIS Remember, you can use the field calculator straight away to shorten the field creation process by selecting to create a new field whilst completing the field calculation (still using the same parameters though!). One additional small tip is that in the middle box in the Field Calculator, you can load Recent field calculations and double-click on your prior calculation to automate the creation of the crime_rate calcuation! Now you have both datasets ready, it’s time to style the maps! Remember to use the Properties box to first symbolise your maps. Think through using the appropriate colour scheme - and perhaps have a look online for some examples, if you don’t want to use the defaults! Once you’re happy with their symbolisation, we’ll turn them into proper publishable maps using Q-GIS’s Print Layout. Making our Crime Rate Maps for analysis in Q-GIS and the Print Layout To create proper publishable maps in Q-GIS, we use what Q-GIS calls its Print Layout window (formely Map Composer). If you’ve ever used ArcMap, this is similar to switch the view of your map canvas to a print layout within the main window - but in Q-GIS’s case, it loads up a new window. From the main Q-GIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime_map_borough_ward A new window should appear. Oh great, another tool I need to learn how to use… well yes, but learning how to use the Print Layout window will help you make great maps. A short introduction to the Print Layout As the Q-GIS documentation on Print Layout explains: The print layout provides growing layout and printing capabilities. It allows you to add elements such as the QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. You can size, group, align, position and rotate each element and adjust their properties to create your layout. The layout can be printed or exported to image formats, PostScript, PDF or to SVG. Initially, when opening the print layout provides you with a blank canvas that represents the paper surface when using the print option. On the left-hand side of the window, you will find buttons beside the canvas to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item. On the right-hand side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. For our practical today, we’re most interested in the bottom panel as Layout will control the overall look of our map, whilst Item properties will allow us to customise the elements, such as Title or Legend, that we may add to our map. In the bottom part of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level, the number of selected items if applicable and, in the case of atlas generation, the number of features. In the upper part of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar. Getting started with creating our map Working with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. To start with creating a map, you use the Add Map tool to draw a box in which a snapshot of the current active map you have displayed in your Q-GIS main window will be loaded. Let’s try this now: Click on the Add Map tool and draw a box in the first half of our map to load our current map, in my case, Ward Crime Rate: Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc. As you can see, the map currently does not look that great - we could really do with zooming in, as we do not need all of the white space. With your map selected, head to the Items Properties panel and look for the Scale parameter. Here we can manually edit the scale of our map to find the right zoom level. Have a go at entering different values and see what level you think suits the size of your map. Keep a note of the scale, as we’ll need this for the second map we’ll add to our map layout - our borough map. Next, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map. In the same panel, note down the size of your map - we want to make sure the next map we add is of the same size. Note, if you need to move the position of the map within the box, look for the Move Item Content tool on the left-hand side toolbar. Once you are done, finally click on the Lock Layers and Lock Style for layers. By locking the Layers (and their symbology) in our map, it means we can change our data/map in our main Q-GIS window without changing the map in the Print Layout - as we’ll see in a minute when adding our Borough Crime Rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main Q-GIS window. Now we’ve added our first map to our Map Layout, we want to add a Legend for this specific map. Click on the Add Legend tool and again, draw a box on your map in which your legend will appear. As you’ll see, your Legend auto-generates an entry for every layer in our Layers box in the main Q-GIS application: In Item Properties, uncheck auto-update - this stops Q-GIS automatically populating your legend as it has done current, and enables you to customise your legend. First, let’s rename our Layer in the legend to: Ward Crime Rate (per 10,000 people). Next, we want to remove all othe Layers, using the - button We can also customise the Legend further, including type, size and alignment of font - go ahead and style your legend as you would prefer. Move the Legend to an appropriate part of the layout near your Ward Crime Rate map - resize if necessary. Now we are finished with the Ward map, we want to make sure we don’t change any aspect of its layout. To do so, we need to lock both the Map and Legend in the Items panel - this prevents us accidently moving items in our layout. Note, this is different to locking your layers in the Items Properties as we did earlier. In the Items panel, click the Lock checkbox for both our map and legend. Once locked, we can now start to add our Borough map. In the main Q-GIS window, uncheck your Ward_crime_rate layer and make sure your Borough_crime_rate layer is now visble. Return to the Print Layout window. Repeat the process above of adding a map to the window - this time, you should now see your Borough map loaded in the box (and you should see no changes to your Ward map). Place your Borough map next to your Ward map - use the snap grids to help. Set your Borough map to the same zoom level as your Ward map. Make sure your Borough map is the same size as your Ward map. Set your Borough to the same extent as your Ward map (extra neatness!). Add a frame if you want. Lock your layer and its symbology in the Items Properties once ready and the lock your layer in the Items panel. We now just need to add a second legend for our Borough map. If we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there is a difference in the values, we need to have two legends. The Extension Activity to this practical, on the other hand, may result in a single legend - as a clue to help complete the activity! Repeat the process as above to add a Legend for our Borough map. Remember to re-title the Legend to make it more legible/informative. Match the same formatting for a clean look. Once complete, lock these two items in the Items panel as well. Locking/Unlocking and Visibility If you hadn’t noticed in the Items panel, you have the ability not only to lock/unlock different items (unlock to edit any items again), but also turn on/off the visibility of your layers: Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on the left-hand tool bar: Add a scale bar: use the Item Properties to adjust the Style, number of segments, font etc.. Add a north arrow: draw a box to generate the arrow and then use the Item Properties to adjust. I typically place the two side by side - and you can select both, right-click and group so you can then treat them as a single item when moving them around the page. Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Once you have added these properties in, you should have something that looks a little like this: Export map We are finally ready to export our map! To export your map to a file go: Layout -&gt; Export as Image and then save in your maps folder as London_2020_Crime_Rate.png. You can also export your map as a PDF. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in Q-GIS. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. This is all possible using the field calculator in Q-GIS, but will require some thinking about the right expression. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "],["programming-for-data-analysis-using-r-and-r-studio.html", "4 Programming (for Data Analysis) using R and R-Studio", " 4 Programming (for Data Analysis) using R and R-Studio Welcome to Week 4 in Geocomputation! Well done on making it through Week 3 - and welcome to our introduction to using programming, in the form of R and R-Studio, for data analysis. This week is heavily practical oriented - with many aspects of your practical integrated at various points in the workshop - as well as, of course, a data analysis section towards the end. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. Week 4 in Geocomp Video on Stream This week’s content introduces you to the foundational concepts associated with Programming for Data Analysis, where we have three areas of work to focus on: General principles of programming How to use R and R-Studio effectively for programmatical data analysis The ‘Tidyverse’ philosophy This week’s content is split into 4 parts: An Introduction to Programming (40 minutes) Using R and R-Studio for Data Analysis (60 minutes) The Tidyverse Philosophy and Principles (40 minutes) Practical 3: Analysing Crime in 2020 in London (30 minutes) This week, we have a slightly different approach to our workflow structure, with a mixture of short lectures, instructional videos and activities to complete throughout each part. A single Key Reading is found towards the end of the workshop. This week, you have 1 assignment, which will be highlighted in the workbook. Part 4 is the main part of analysis for our Practical for this week, but you will find aspects of programming in Parts 1-3 that you will need to do in order to prepare our data for the final part. If you have been unable to download R-Studio Desktop or cannot access it via Desktop@UCL Anywhere, you will have access to our R-Studio Server website instead. Instructions on how to access this are provided below. Learning Objectives By the end of this week, you should be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R and produce a bar chart We will build on the data analysis we completed last week and look to further understand crime in London by looking at its prevalence on a month-by-month basis. An Introduction to Programming Programming is our most fundamental way of interacting with a computer - it was how computers were first built and operated - and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking - and not typing. However, programming and computer code underpin every single application that we use on our computers… or really any technological device. After all, programming is used for so many purposes and applications, that, we as users take for granted - from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages (and ways of using programming languages!) have developed - Wikipedia, for example, has a list of 50 different types of languages, although there is some overlap between many of these and some are used for incredibly niche activties. In general, the main programming languages that people focus on learning at the moment include: Top 10 programming languages and their applications according to DZone in 2017. Some can be used for a range of purposes – others are more specific, e.g. HTML for website building. There are also different ways in which programming languages work, which give some advantages over others. This is to do with how the code is written and ‘talks to the computer’ - behind our main programming languages there is something called a compiler that takes the code we write in our various programming languages and translates it into machine code, a.k.a. the code our computers know how to understand. This code is written purely in binary and, as a result, looks a lot different to the code we’ll be writing in our practicals (think a lot of 1s and 0s!). For some languages, this translation is completed when your code is compiled before it is run, i.e. the ‘compiler’ will look through all your code, translate to machine code and then execute the machine code according. These languages are known as compiled, or low-level languages and can, at first, be slow to write but are incredibly efficient when executing huge amounts of code (e.g. when creating software). They however require an understanding of things called registers, memory addresses, and call stacks and are, as a result, a lot more complicated to learn and use (and no, I personally do not know how to code in any low-level languages…nor do I particularly want to!). For other languages, such as R and Python, these fall into the interpreted language category. Here, each line of code is executed without a pre-runtime translation. In this case, a program called an interpreter reads each program statement and, following the program flow, then decides what to do, and does it. The issue with these high-level programming languages is that this approach can be costly in computational resources (i.e. processing time and memory space). As there is no pre-run time compilation, bugs are not found before the code is run but instead as the code is run - as a result (and what you might see happen in your own code), your computer can get stuck trying to execute code which is either completely unfeasible for your computer to execute (e.g. your computer cannot handle the size of data you are feeding it) or it ends up in a loop with no way out - except for you stopping the code. However, the advantage of using these languages is that their main focus is on things like variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts - all of which we’ll use within our module, believe it or not! These languages have a primary focus on usability over optimal program efficiency, which, when we’re just learning to code, are ideal for us in Geocomputation! Don’t worry if you don’t understand what any of this means, you will do by the end of this module! As we’re not taking a Computer Science degree here, we won’t go into any more detail about this, but suffice to say, there is a lot more to programming then what we’ll cover in Geocomputation. But what is important to recognise is that a lot of work went into creating the programming environments that we get to use today - and I, for one, am extremely glad I never had to learn how to write a compiler! If understanding a little more about compilers and machine code is of interest to you, the below video provides an accessible explanation - although you might want to come back to it at the end of the practical: How do computers read code? The Command Line Interface The most basic of programming we can use without installing anything on our computers is using the Command Line Interface(CLI) already built in, known as the shell. The shell is a simple program that lets you interact with your computer using text commands (Command Line Interface) instead of a point &amp; click Graphical User Interface (GLI). The shell simply takes your commands and provides them to the operating system of your computer. Each operating system has its own shell program: Mac / Linux = zsh (new) / bash (Bourne-again Shell) (previous) Microsoft = PowerShell (and a few others). For most operating systems, you can access the shell using a window-based program, known as a terminal emulator (TE). The default TE for Mac &amp; Linux users is: Terminal. The default TE for Windows users use Command Prompt (old) or Terminal (new). (If you remember my introductory lecture, this is how I used to have to interact with my first computer at a very young age of probably 5 when attempting to load a computer game!) The shell is an incredibly useful program - and if you take programming further in the future (e.g. analysis of big datsets through servers, running multiple scripts, dealing with version control, software engineering), it will become a tool that you’ll become incredibly familiar with. But for now, we just want to illustrate how cool it can be for us to be able to tell our computer to do things in a few lines of code - rather than having to click and point - particularly once you know how to use the CLI-shell and can remember specific commands. Let’s take a look by completing a simple task with our shell to tell our computer to do something - let’s do some file management for our practical today. Using the Command Line Interface On Your Computer As you may remember from last week’s practical, I provided you with our crime data in a single processed csv file. However, when you download the data from data.police.uk, the data is not so nicely formatted! Essentially, all of our data that we download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: To be able process this data easily in R, we want to move all of our files into a single folder. And believe it or not, it only takes a couple of lines of code to do so. Let’s take a look: Using the Command Line to copy and move files. Video on Stream As we’ve seen in the video above, it can really useful - and quick - to use our Shell to organise our data files prior to loading them in R-Studio. Just to proove this, the first piece of programming we will do today is use your built-in shell on your computer to repeat the same process and copy our crime data into a single folder. File Management using the Command Line Now you’ve watched how I copy the files over, let’s go ahead and do this ourselves. Head to data.police.uk and download all available crime data for London in 2020 (this may only include up until November) and for both Police Forces. The below video shows you how to do this. You should now have your data downloaded in a single main folder in your downloads (it may be given a ridiculously long coded filename). Copy this folder into your GEOG0030 -&gt; data -&gt; raw -&gt; crime folder using copy and paste / drag-drop (whatever your preference). Next, open up your shell on your computer. On mac, hold CMD and hit space to open up your search bar and type terminal, and a terminal window should appear. On windows, press the windows button and do the same. Alternatively search for shell or command prompt. With your shell open, we will navigate to your raw data folder and copy over the crime data into a single folder. To do so, we’ll use six main terminal commands: Common Shell/Terminal Commands pwd (both Mac and Windows): Print Working Directory - this will show us where we are in our computer’s file system. By default, you should be located at the “base” of your user file. dir (Windows) or ls (Mac): Lists all files in the directory you are currently located. cd (both Mac and Windows): Change into a new directory (i.e. folder) - this can be a single path ‘step’ or several steps to get to the directory you want. md (Windows) or mkdir (both Mac and Windows): Make a new directory (i.e. folder) where you are currently located. cp (both Mac and Windows): Copy content of a directory or specific file to a new location. This command can take wildcards to help search multiple folders at once, as we’ll see in our query. move (Windows) or mv (Mac): Move directory to a new destination. Let’s get going. In your shell, type the command pwd and press return. You should now see a file path appear under your command - the computer is telling you where you are currently located in your computer system. Next, type dir OR ls (OS-dependent) and press return. The computer now lists all of the folders in your directory that you can move into as a next step from your current folder. You now need to identify what folder your GEOG0030 work is contained in - and what your “path” is to get there as we now need to continue changing directories to get to our GEOG0030 folder. Next, type cd followed by the folder(s) you need to change into in order to get to your main GEOG0030 folder. Remember to press return to execute your code. In my case, the command is: cd Code. Each time you change folder, this folder is added to the file path next to your Prompt - have a quick look at this now. Keep going changing folders until you are in the folder that contains your downloaded crime data. Auto-Population of File Paths A tip is that your terminal can auto-populate your folder names for you when there is enough information for them to determine the unique folder. To do this, press tab on your keyboard. E.g. your crime folder is likely to be a long list of numbers and letters if you haven’t renamed it whilst copying it over to your raw folder. Therefore I recommend using this approach will save you time entering all these numbers. You can also change into this folder in one command, simply keep adding to your folder path as so: Type in (and use tab) cd GEOG0030/data/raw/crime/52c6b758bceaf2244fc1b6f93e85d7f00f234ccf/ and then press return. Note, if you are using a WINDOWS PC, you need to use a backslash (\\) in your file path, not a forward-slash. Note, you do not need a slash at the start of your file paths.* Once you are in the correct folder, we first want to make a new folder to contain all the crime csvs (without their current subfolder system): Type in mkdir all_crime and press return. If you now type dir or ls, you should now see the new folder listed within your current folder. Let’s go ahead and copy all of our csvs into this single folder. Type cp **/*.csv all_crime and press return. Again use a backslash ()) if on a Windows PC. This command uses the wildcard * to search for any file in any folder, as long as it has a .csv file type. Using wildcards is a very common programming tool and we are likely to use them in our practicals moving forward. You can also use them in searches on search engines such as Google! We can now change into our new folder and then list its contents to check that our files have moved across. Type cd all_crime and press return. Then type dir or ls. Check that you have all your files (either 22 or 24, depending on when you are completing this practical!). Great, we have our files all in a single folder which will make using them in R much easier. We’ll do one final thing - and that is move this folder out of this original and into the main crime folder. Still in the terminal, type: cd .. to take a step back into our police crime data folder. Next, to move our all_crime folder: (WINDOWS) move all_crime .. or (MAC): mv all_crime .. Finally, type cd .. and press return. You should now find yourself one step back in your file system in the main crime folder. We can check that our move worked by again listing the contents of the folder. Type dir or ls and press return. Check that your all_crime folder is now listed. ::: Great! You’ve just done your first bit of command line programming! Think about how quick was it to type those commands and get the files moved - rather than have to do all of that by hand. Of course it helped that I told you what commands to write - but the more time you spend with programming, the quicker (and more familiar) you will get at (with) coding and executing these commands. Command Line Paths Note, the use of .. in our two commands above means to take a step back in your file system path, as we did in both of these cases here (i.e. the ‘parent’ folder. In addition, two further commands to be aware of include: ~ (Mac) to denote the root or home directory, e.g. cd ~. In Windows, there is not really a shortcut for this. .: a single full-stop means “this folder”. The command line is just one aspect of programming - but we also want to have the ability to create and run scripts. Scripts are incredibily important for us when completing data anlaysis and, as such, we’ll look at the differences between the two as we start to use R/R-Studio for our data analysis today. One cool thing about the terminal is that we actually have the ability to create and run scripts just within the terminal itself. We can do this by opening up a text editor in our terminal to write a script in any programming language and then execute our script within the terminal. We execute our script by actually setting the terminal to that programming language and then calling the script. This all sounds extremely complicated - but it really is not once you’ve spent a bit of time working with the CLI. We can have a quick look here: Using the Command Line to create and run scripts (in Python). Video on Stream Whilst we could use this type of approach for data analysis for conducting actual spatial analysis, we won’t be doing so in Geocomptuation (you are probably happy to read!). This is because, quite frankly, the terminal is pretty limited in its display of maps and charts, a key output of our work here in Geocomputation, and general user-friendly functionality. In fact, we’d need to save our outputs to a file each time to go view them, which would end up being a pretty clunky workflow… Instead, what’s great is that we have several different types of software and Integrated Develoement Environments that bring the functionality of running scripts together with the visualisation capacity we like in our GIS software. For us in Geocomputation, our tool of choice for this is R-Studio. Using R and R-Studio for Data Analysis Before we go any further, what I want to make clear from this workshop - and the remainder of the module - is that programming using R and R-Studio is ultimately a tool we will use to complete specific tasks we need to do for our data analysis. There are a lot of different tools out there that you can use to achieve the same outcomes (as you’ve seen with Q-GIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools - more on this next week. With this tool though, there is a lot to learn about the principles and the theory behind programming languages. As evident above, whilst we could look at this in a lot of detail (there is a lot of theory behind programming which we just won’t cover - that’s for computer scientists), we will instead focus on the aspects most important to our use, which is covered in our main lecture video below: Principles of Programming for Data Analysis in the Programming for Data Analysis section. The second thing to make clear is that R and R-Studio are two different things: R is our programming language, which we need to understand in terms of general principles, syntax and structure. R-Studio is our Integrated Development Environment, which we need to understand in terms of functionality and workflow. Integrated Development Environment An Integrated Development Environment (IDE) is simply a complicated way of saying “a place where I write and build scripts and execute my code”. Nowadays, we have some really fancy IDEs that, when they know what language you are coding in, will highlight different types of code according to what they represent (e.g. a variable, a function) as well as try to proof-read/de-bug your code “on-the-fly” before you’ve even run it. R-Studio is definitely a very fancy IDE - as it offers a lot of functionality beyond just writing scripts and execute code as we’ll see over the coming weeks. As you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the R-Studio IDE (or really software now!) to execute this statisitcal programming. Together, R and R-Studio has grown into an incredibly success partnership of analytical programming language and analysis software - and is widely used for academic research as well as in the commercial sector. One of R’s great strength is that it is open-source, can be used on all major computer operating systems and is free for anyone to use. It, as a result, has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis. Believe it or not, the entire workbook you are reading right now has been created in R-Studio, utilising a mixture of programming languages, including R, HTML, CSS and Markdown. R-Studio has the flexibility to understand programming languages other than R (including Python!), whilst R can be deployed outside of the R-Studio environment in standalone scripts and other IDEs. However, for us, the partnership between R and R-Studio works pretty well for what we want to achieve - so this is what we’ll be using for the remainder of the Geocomputation module. How do I use R-Studio? Unlike traditional statistical analysis programmes you may have used such as Microsoft Excel or even SPSS, within the R-Studio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script (or similar document, more on this in Week 10), that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks. We can also use the R Console to execute simple instructions that do not need repeating - such as installing libraries or quickly viewing data (we’ll get to this in a second). In addition, R, its various graphic-oriented “packages” and R-Studio are capable of making graphs, charts and maps through just a few lines of code (you might notice a Plots window to your right in your R-Studio window) - which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted. Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and R-Studio. These include: It is broadly intuitive with a strong focus on publishable-quality graphics. It is ‘intelligent’ and offers in-built good practice – it tends to stick to statistical conventions and present data in sensible ways. It is free, cross-platform, customisable and extendable with a whole swathe of packages/libraries (‘add ons’) including those for discrete choice, multilevel and longitudinal regression, and mapping, spatial statistics, spatial regression, and geostatistics. It is well respected and used at the world’s largest technology companies (including Google, Microsoft and Facebook, and at hundreds of other companies). It offers a transferable skill that shows to potential employers experience both of statistics and of computing. The intention of the practical elements of this week is to provide a thorough introduction to R-Studio to get you started: 1. The basic programming principles behind R. 2. Loading in data from csv files, filtering and subsetting it into smaller chunks and joining them together. 3. Calculating a number of statistics for data exploration and checking. 4. Creating basic and more complex plots in order to visualise the distributions values within a dataset. What you should remember is that R/R-Studio has a steep learning curve, but the benefits of using it are well worth the effort. I highly recommend you take your time and think through every piece of code you type in - and also remember to comment your code (we’ll get to this in a bit!) . The best way to learn R is to take the basic code provided in tutorials and experiment with changing parameters - such as the colour of points in a graph - to really get ‘under the hood’ of the software. Take lots of notes as you go along and if you are getting really frustrated take a break! This week, we focus solely on using R and R-Studio (from now on, this may be simply denoted as R) for basic statistical data analysis. Next week, we will introduce using R for spatial (data) analysis - but there’s lots to get on with today to understand the fundamental principles of using R (and programming in general). Accessing R-Studio for Geocomputation You have two options for using R-Studio in this module. Using R-Studio Desktop: You should have installed this in Week 1 as per the software installation instructions. Using R-Studio Server: First sign in to the UCL VPN or UCL China Connect. To use R-Studio Server, open a web browser and navigate to: https://rstudio.data-science.rc.ucl.ac.uk/ Log in with your usual UCL username and password. You should see the RStudio interface appear. If it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the start a new session button. You can use either approach - but do recognise their may be some differences oh how our code appears. The code below has been created on an R-Studio Desktop Version 1.2.5033 and tested on the R-Studio Server. Note RStudio server will only work with an active VPN connection that links your personal computer into UCL’s network. Students in mainland China may want to use UCL China Connect. Students that use a Mac computer that is running on the latest version of MacOS (MacOS Big Sur), are advised to use Desktop@UCL as the Cisco AnyConnect VPN application may not work. If you are completely unable to access the server (e.g. your browser displays a This site can’t be reached message), it means that your VPN connection is not working correctly. Please ensure that your VPN is working correctly or use Desktop@UCL Anywhere instead. An Introduction to R-Studio and its interface Let’s go ahead and open R-Studio (Desktop or Server) and we’ll first take a quick tour of the various components of the R-Studio environment interface and how and when to use them: Introducing the R-Studio Interface Video on Stream As you’ve heard, R-Studio has various windows that you use for different purposes - and you can customise its layout dependent on your preference. When you first open R-Studio, it should look a little something like this: The main windows (panel/pane) to keep focused on for now are: Console: where we write “one-off” code, such as installing libraries/packages, as well as running quick views or plots of our data. Files: where our files are stored on our computer system - can help with checking file paths as well as file names, and general file management. Environment: where our variables are recorded - we can find out a lot about our variables by looking at the Environment window, including data structure, data type(s) and the fields and ‘attributes’ of our variables. Plots: the outputs of our graphs, charts and maps are shown here. Help: where you an search for help, e.g. by typing in a function to find out its parameters. You may also have your Script Window open, which is where we build up and write code, to a) keep a record of our work, b) enable us to repeat and re-run code again, often with different parameters. We will not use this window until we get to the final practical instructions. We’ll see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as ‘attributes’ (do not get confused here with the Attribute Table for Q-GIS) and data structures. Programming for Data Analysis Before we get started with using R-Studio, we first need to take a few steps back and address the bigger learning curve in the room, that is: How do I program?. As stated earlier, R/R-Studio is just a tool - but to use it, you need to understand how to write code in R effectively and, of course, accurately to get your analysis to work. This means we need to learn about and understand: Basic Syntax Data Structures and Types Functions and Libraries/Packages Object-Oriented Programming Here, we provide a short introduction to the basic principles of programming, with a focus on Object Oriented Programming. This is a video you might want to re-watch after completing today’s practical. Principles of Programming for Data Analysis Slides | Video on Stream In the above lecture, you heard about the different including: Syntax using variables and functions Importance of data types and data structures The role of packages/libraries in expanding R’s functionality And a brief introduction to Object-Oriented Programin (OOP) We can put some of these principles into action by testing some of R-Studio’s capability with some short pieces of coding now. Using the Console in R-Studio We’ll first start off with using R-Studio’s console to test out some of R’s in-built functionality by creating a few variables as well as a dummy dataset that we’ll be able to analyse - and to get familiar with writing code. Note, you might need to click on the console window to get it to expand - you can then drag it to take up a larger space in your R-Studio window. The video below provides an overview of the short tutorial with additional explanations, so if you’re already a bit stuck, you can watch this as you start to complete the following instructions. Using the Console in R-Studio for programming Video on Stream In your R-Studio console, you should see a prompt sign - &gt; to the left - this means we’re ready to start writing code (a bit like earlier in the shell). Error Warnings in the Console Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you see a &gt; on the left it means you can type in your next line, a + means that you haven’t finished the previous line of code. As will become clear, + signs often appear if you don’t close brackets or you did not properly finish your command in a way that R expected. In your console, let’s go ahead and conduct some quick maths - at their most basic, all programming langauges can work like calculators! Command Input Type in 10 * 12 into the console. # Conduct some maths 10 * 12 ## [1] 120 Once you press return, you should see the answer of 120 returned below. Great, you’ve now learnt how to enter code into the R-Studio console! Pretty similar to your computer’s CLI right?! Storing Variables But rather than use ‘raw’ or ‘standalone’ numbers and values, we primarily want to use variables that stores these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called creating an object and this object becomes stored as a variable. We do this by using the &lt;- symbol is used to assign the value to the variable name you have given. Let’s go ahead and try this. Let’s create two variables for experimenting with: 2.Type in ten &lt;- 10 into the console and execute. # Store our ten variable ten &lt;- 10 You have just created your first variable. You will see nothing is returned in the console - but if you check your Environment window, it has now appeared as a new variable that contains the associated value. Type in twelve &lt;- 12 into the console and execute. # Store our ten variable twelve &lt;- 12 Once again, you’ll see nothing returned to the console but do check your Environment window for your variable. We’ve now stored two numbers into our environment - and given them pretty good variable names for easy reference. R stores these objects as variables in your computer’s RAM so they can be processed quickly. Without saving your environment (we will come onto this below), these variables would be lost if you close R (or it crashes). Now we have our variables, let’s go ahead and do the same simple multiplication maths: Type in ten * twelve into the console and execute. # Conduct some maths again using our variables ten * twelve ## [1] 120 You should see the output in the console of 120 (of course..!). Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these variables can be treated in the same way as the values they contain. Next, type in ten * twelve * 8 into the console and execute. # Conduct some more maths with variables and raw values ten * twelve * 8 ## [1] 960 You should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable. Type output &lt;- ten * twelve * 8 into the console and execute. # Conduct some maths and store it as output output &lt;- ten * twelve * 8 As we’re storing the output of our maths to a new variable, the answer won’t be returned to the screen. Accessing and returning variables We can ask our computer to return this output by simply typing it into the console. Ask the computer to return the variable output. You should see we get the same value as the earlier equation. # Return the variable, output output ## [1] 960 Variables of different data types We can also store variables of different data types, not just numbers but text as well. Type in str_variable &lt;- \"This is our first string variable\" into the console and execute. # Store a variable str_variable &lt;- &quot;This is our 1st string variable&quot; We have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores “words” (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of the \" \". Let’s access our variable to see what is now stored by our computer. Type in str_variable into the console and execute. # Return our str_variable str_variable ## [1] &quot;This is our 1st string variable&quot; You should see our entire sentence returned - and enclosed in \"\". Again, by simply entering our variable into the console, we have asked R to return our variable to us. Calling functions on our variables We can also call a function on our variable. This use of call is a very specific programming term and generally what you use to say \"use\" a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to print our variable, which will give us the same output as accessing it directly via the console: Type in print(str_variable) into the console and execute. # Print str_variable to the screen print(str_variable) ## [1] &quot;This is our 1st string variable&quot; We have just used our first function: print(). This function actively finds the variable and then returns this to our screen. You can type ?print into the console to find out more about the print() function. # Gain access to the documentation for our print function ?print This can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output. In many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one argument. Returning functions When a function provides an output, such as this, it is known as returning. Not all functions will return an output to your screen - they’ll simply just do what you access them to do, so often we’ll use a print() statement or another type of returning function to check whether the function was successful or not - more on this later in the workshop. Examining our variables using functions Within the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is: Type in typeof(str_variable) into the console and execute. # Call the typeof() function on str_variable to return the data type of our variable. typeof(str_variable) ## [1] &quot;character&quot; You should see the answer: \"character\". As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables. Type in typeof(ten) into the console and execute. # Call the typeof() function on ten variable to return the data type of our variable. typeof(ten) ## [1] &quot;double&quot; You should see the answer: \"double\". As evident, our ten is a double data type. For high-level objects that involve (more complicated) data structures, such as when we load a csv into R as a data frame, we are also able to check what class our object is, as follows: Type in class(str_variable) into the console and execute. # Call the class() function on str_variable to return the object of our variable. class(str_variable) ## [1] &quot;character&quot; In this case, you’ll get the same answer - “character” - because, in R, both its class and type are the same: a character. In other programming languages, you might have had \"string\" returned instead, but this effectively means the same thing. Let’s try testing our ten variable: Type in class(ten) into the console and execute. # Call the class() function on ten to return the object of our variable. class(ten) ## [1] &quot;numeric&quot; In this case, you’ll get a different answer - \"numeric\" - because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our ten variable is an integer or not. Type in is.integer(ten) into the console and execute. # Test our ten variable by asking if it is an integer is.integer(ten) ## [1] FALSE You should see we get the answer FALSE - as we know from our earlier typeof() function, our variable ten is stored as a double and therefore cannot be an integer. Whilst knowing this might not seem important now, but when it comes to our data analysis, the difference of a decimal number vs. a whole number can quite easily add bugs into our code! We can incorporate these tests into our code when we need to evaluate an output of a process and do some quality assurance testing of our data analysis. We can also ask how long our variable is - in this case, we’ll find out how many different sets of characters (strings) are stored in our variable, str_variable. Type in length(str_variable) into the console and execute. # Call the length() function on str_variable to return the length of our variable. length(str_variable) ## [1] 1 You should get the answer 1 - as we only have one set of characters. We can also ask how long each set of characters is within our variable, i.e. ask how long the string contained by our variable is. Type in nchar(str_variable) into the console and execute. # Call the nchar() function on str_variable to return the length of each of our elements within our variable. nchar(str_variable) ## [1] 31 You should get an answer of 31. Creating a two-element object Let’s go ahead and test these two functions a little further by creating a new variable to store two string sets within our object, i.e. our variable will hold two elements. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute. # Store a new variable with two items using the c() function two_str_variable &lt;- c(&quot;This is our second string variable&quot;, &quot;It has two parts to it&quot;) In this piece of code, we’ve created a new variable using the c function in R, that stands for \"combine values into a vector or list. We’ve provided that function with two sets of strings, using a comma to separate our two strings - all contained within the function’s (). You should now see a new variable in your Environment window which tells us it’s a) chr: characters, b) contains 2 items, and c) lists those items. Let’s now try both our length() and nchar() on our new variable and see what the results are. # Call the length() function and nchar() function on our new variable length(two_str_variable) ## [1] 2 nchar(two_str_variable) ## [1] 34 22 Did you see a difference? You should have seen that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22. There is one final function that we often want to use with our variables when we are first exploring them, which is attributes() - as our variables are very simple, they currently do not have any attributes (you are welcome to type in the code and try) but it is a really useful function, which we’ll come across later on. # Call the attributes() function on our new variable attributes(two_str_variable) ## NULL We’ve had fun experimenting with simple variables in our console - and learnt about many new functions we can use with our code. In fact, we’ve learnt 7 functions - can you name/remember them all without scrolling up? If you can’t, I highly recommend taking notes on each of the functions - even if it is just a short list of the functions and what they do. We’re now going to move on to creating and analysing our dummy dataset - so fingers crossed you’ll remember these as we move forward. Using comments in our code In addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code I have written, I have provided an additional comment to explain what the code does. Comments are denoted using the hash symbol #. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code - as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments as you code and not leave it to do retrospectively - because I can tell you from experience - you most certainly will not. Whilst we are using the console, using comments is not necessary - but as we start to build up a script in our full practical, you’ll find them essential to help understand your workflow in the future! Analysing dummy data in R-Studio using the Console The objects we created and played with above are very simple: we have stored either simple strings or numeric values - but the real power of R comes when we can begin to execute functions on more complex objects. As we heard in our lecture, R accepts four main types of data structures: vectors, matrices, data frames, and lists. So far, we have dabbled with a single item or a dual item vector - for the latter, we used the c() function to allow us to combine our two strings together within a single vector. We can use this same function to create and build more complex objects - which we can then use with some common statistical functions. We’re going to try this out by using a simple set of dummy data: we’re going to use the total number of pages and publication dates of the various editions of Geographic Information Systems and Science (GISS) for our brief dummy analysis: Book Edition Year Total Number of Pages 1st 2001 454 2nd 2005 517 3rd 2011 560 4th 2015 477 As we can see, we will ultimately want to store the data in a table as above (and we could easily copy this to a csv to load into R if we wanted). But we want to learn a little more about data structures in R, therefore, we’re going to go ahead and build this table “manually”. Let’s get going. Clearing our Environemnt workspace First, let’s clear up our workspace and remove our current variables: Type rm(ten, twelve, output, str_variable, two_str_variable) into the console and execute. # Clear our workspace rm(ten, twelve, output, str_variable, two_str_variable) Note, of course you can either copy and paste this code - or try out using the tab function to autocomplete your variable names in the console as you start typing them in, just as we did when using the Command Line. You should now see we no longer have any variables in our window - we just used the rm() function to remove these variables from our environment. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later - but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment. Creating an atomic vector of multiple elements The first complex data object we will create is a vector. A vector is the most common and basic data structure in R and is pretty much the workhorse of R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types: Atomic vectors (all elements are of the same data type) Lists (elements can be of different data types) Although in practice the term “vector” most commonly refers to the atomic types and not to lists. The variables we created above are actually vectors - however they are made of only one or two elements. We want to make complex vectors with more elements to them. Let’s create our first official “complex” vector, detailing the different total page numbers for GISS: Type giss_page_no &lt;- c(454, 517, 560, 477) into the console and execute. # store our total number of pages, in chronological order, as a variable giss_page_no &lt;- c(454, 517, 560, 477) Let’s check the results. Type print(giss_page_no) into the console and execute. # print our giss... variable print(giss_page_no) ## [1] 454 517 560 477 We can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object: Type our various statistical functions (detailed below) into the console and execute. # calculate the arithmetic mean on our variable mean(giss_page_no) ## [1] 502 # calculate the median on our variable median(giss_page_no) ## [1] 497 # calculate the range numbers of our variable range(giss_page_no) ## [1] 454 560 We have now completed our first set of descriptive statistics in R! We now know that the average number of pages the GISS book has contain is 497 pages - this is of course truly thrilling stuff, but hopefully an easy example to get onboard with. But let’s see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note, I entered the total number of pages in a specific order to correspond to these publishing dates (i.e. chronological), as outlined by the table above. As a result, I’ll enter the publication year in the same order. Type giss_year &lt;- c(2001, 2005, 2011, 2015) into the console and execute. # store our publication years, in chronological order, as a variable giss_year &lt;- c(2001, 2005, 2011, 2015) Let’s check the results. Type print(giss_year) into the console and execute. #print our giss_year variable print(giss_year) ## [1] 2001 2005 2011 2015 Again, truly exciting stuff. Of course, on their own, the two vectors do not mean much - but we can use the same c() function to combine the two together to create a matrix. Creating a matrix from two vectors In R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both rows and columns. As with atomic vectors, the elements of a matrix must be of the same data type. As both our page numbers and our years are numeric (we can check this using which function?), we can add them together to create a matrix using the matrix() function: Type giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) into the console and execute. # create a new matrix from our two vectors with two columns giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) # note the inclusion of a new argument to our matrix: ncol=2 #this stands for &quot;number of columns&quot; and we want two. Again, let’s check the results. Type print(giss_year_nos) into the console and execute. print(giss_year_nos) ## [,1] [,2] ## [1,] 2001 454 ## [2,] 2005 517 ## [3,] 2011 560 ## [4,] 2015 477 The thing about matrices - as you might see above - is that, for us, they don’t have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we wouldn’t really know what to do with it. As a result, we tend to primarily use Data Frames in R as they offer the opportunity to add field names to our columns to help with their intepretation. Arguments/Parameters in Functions The function we just used above, ‘matrix()’, was the first function that we used that took more than one argument. In this case, the arguments the matrix needed to run were: What data or dataset should be stored in the matrix. How many columns (ncol=) do we need to store our data in. The function can actually accept several more arguments - but these were not of use for us in this scenario, so we did not include them. For almost any R package, the documentation will contain a list of the arguments that the function will takes, as well as in which format the functions expects these arguments and a set of usage examples. Understanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function - and whether we will need to transform our variable into a different data structure to be used for that specific function. For any function, there will be mandatory arguments (i.e. it will not run without these) or optional arguments (i.e. it will run without these, as the default to this argument has been set usually to FALSE, 0 or NULL). Creating a Data Frame from our matrix A data frame is an extremely important data type in R. It is pretty much the de-facto data structure for most tabular data and what we use for statistics. It also is the underlying structure to the table data (what we would call the attribute table in Q-GIS) that we associate with spatial data - more on this next week. A data frame is a special type of list where every element of the list will have the same length (i.e. data frame is a “rectangular” list), Essentially, a data frame is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply NULL. Data frames can have additional attributes such as rownames(), which can be useful for annotating data, like subject_id or sample_id or even UID. In statistics, they are often not used - but in spatial analysis, these IDs can be very useful. Some additional information on data frames: They are usually created by read.csv() and read.table(), i.e. when importing the data into R. Assuming all columns in a data frame are of same type, a data frame can be converted to a matrix with data.matrix() (preferred) oras.matrix(). You can also create a new data frame with data.frame() function, e.g. a matrix can be converted to a data frame, as we’ll see below. You can find out the number of rows and columns with nrow() and ncol(), respectively. Rownames are often automatically generated and look like 1, 2, …, n. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset. Let’s go ahead and create a new data frame from our matrix: Type giss_df &lt;- data.frame(giss_year_nos) into the console and execute. # Create a new dataframe from our matrix giss_df &lt;- data.frame(giss_year_nos) We now have a data frame, we can finally use the View() function in R. Still in your console, type: View(giss_df) # View our data frame View(giss_df) You should now see a table pop-up as a new tab on your script window. It’s now starting to look like our original table - but we’re not exactly going to be happy with X1 and X2 as our field names - they’re not very informative. Renaming our column field names Instead, what we can do is rename our data frame column field names by using the names() function. Before we do this, have a read of what the names() function does. Still in your console, type: ?names # Get the help documentation for the names function ?names As you can see, the function will get or set the names of an object, with renaming occuring by using the following syntax: names(x) &lt;- value The value itself needs to be a character vector of up to the same length as x, or NULL. This is one of the cool aspects of OOP, in that we can access specific parts of our object and change it without changing the object as a whole or having to create a new object/variable to enact our changes. We have two columns in our data frame, so we need to parse our names() function with a character vector with two elements. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, new_names &lt;- c(\"year\", \"page_nos\"), and then the names() function containing this vector for renaming, names(giss_df) &lt;- new_names: # Create a vector with our new column names new_names &lt;- c(&quot;year&quot;, &quot;page_nos&quot;) #Rename our columns with our next names names(giss_df) &lt;- new_names You can go and check your data frame again and see the new names using either View() function or by clicking on the tab at the top. Adding a column to our data frame We are still missing one final column from our data frame - that is our edition of the textbook. As this is a character data type, we would not have been able to add this directly to our matrix - and instead have waited until we have our data frame to do so. This is because data frames can take different data types, unlike matrices - so let’s go ahead and add the edition as a new column. To do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our data frame by storing this vector as a new column in our data frame. We use the $ sign with our code that gives us “access” to the data frame’s column - we then specify the column edition, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column. This $ is another feature of OOP. Let’s take a look. Create a edition vector variable containing our textbook edition numbers - type and execute edition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\"). We then store this as a new column in our data frame under the column name edition by typing and executing giss_df$edition &lt;- edition: # Create a vector with our editions edition &lt;- c(&quot;1st&quot;, &quot;2nd&quot;, &quot;3rd&quot;, &quot;4th&quot;) # Add this vector as a new column to our data frame giss_df$edition &lt;- edition Again, you can go and check your data frame and see the new column using either View() function or by clicking on the tab at the top. You should now have a data frame that looks like: Now we have our data frame, let’s find out a little about it. We can first return the dimensions (the size) of our data frame by using the dim() function (dim simply stands for dimensions in this case.). In your console, type dim(giss_df) and execute: # Check our data frame dimensions dim(giss_df) ## [1] 4 3 We can see we have four rows and three columns. And we can finally use our attributes() function to get the attributes of our data frame. In your console, type attributes(giss_df) and execute: # Check our data frame attributes attributes(giss_df) ## $names ## [1] &quot;year&quot; &quot;page_nos&quot; &quot;edition&quot; ## ## $row.names ## [1] 1 2 3 4 ## ## $class ## [1] &quot;data.frame&quot; You should see that we now get a list of the column and row names, plus the class of the data frame. There is a lot more we could now do with our data frame but we simply do not have time - and we’d much rather implement some of these functions or data management techniques with a much more exciting dataset than the details of the GISS textbook. Hopefully though, this has served as a good introduction to the different data structures you’ll be coming across over the next 6 weeks as we use R - and provided you with some simple code you can return to time and time again for reminders, such as how to create a new column in your data frame. Before we leave the console (and to be honest, we won’t exactly leave it behind), we’ll enter one last line of code for now: Type in install.packages(\"tidyverse\") into the console and execute. # Install the tidyverse library install.packages(&quot;tidyverse&quot;) Leave this code to run - it might take some time but you won’t need to worry about this until you’ve moved onto the practical section. Now we’re ready to move onto our next section, but first - after reading the Tips and recap below - I recommend you take a long break from this workbook! Tips &amp; Tricks R is case-sensitive so you need to make sure that you capitalise everything correctly if required. The spaces between the words don’t matter but the positions of the commas and brackets do. Remember, if you find the prompt, &gt;, is replaced with a + it is because the command is incomplete. If necessary, hit the escape (esc) key and try again. It is important to come up with good names for your objects. In the case of the majority of our variables, we used a underscore _ to separate the words. It is good practice to keep the object names as short as posssible but they still need to be easy to read and clear what they are referring to. Be aware: you cannot start an object name with a number! If you press the up arrow in the command line you will be able to edit the previous lines of code you have inputted. Coding Breakthroughs In this section you have: Entered your first commands into the R command line interface. Created objects in R. Created a vector of values. Executed some simple R functions. Created a data frame. Now, please, make sure you go ahead and take a break! The Tidyverse Philosophy and Principles Over the past weeks a lot of information has come your way, diving deep into the world of GIScience…and now programing. However, whilst you are slowly becoming proficient in using spatial data and hopefully enjoying learning about how to code, we also need to learn about how our data is structured aand organised. I told you there were quite a few learning curves in this module! This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be what’s becoming colloquially called: dirty data. What we mean by dirty data is data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Let’s think back to the Ward and Borough Population data that you downloaded from the ONS - we could not use the Excel Spreadsheet straight away as it was within a large workbook with a) many tabs and b) lots of additional formatting (e.g. empty rows, “whitespace”). Instead, we extracted the data we wanted and formatted it into a very simple table, consisting of only fields that contained individual records for each of our Wards or Boroughs. This table would fit what is understood as the tidy data approach, which is a general perspective on how to structure your data in R to ease its analysis. Tidy data was formalised by R Wizard Hadley Wickham in his contribution to the Journal of Statistical Software as a way of consistently structuring your data. In the words, of the Wizard: Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This tidy data approach is very much at the core of the tidyverse R package that we just installed - and for us as soon-to-be connoisseurs of secondary data, is also of significant importance when organising your data for future projects. So what do tidy data look like? Tidy Data In Practice Believe it or not, you can often represent the same underlying data in multiple ways. The example below, taken from the the tidyverse package and described in the R for Data Science book, shows that the same data can organised in four different ways. The data shows the population and cases (of something, e.g. malaria) for each country, for 1999 and 2000: None of these representations are wrong per se, however, not are equally easy to use. Only Table 1 can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In the case of Table 4 - we even have two tables! These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a table/data frame/tibble. Put each variable in a column. Figure 4.1: A visual representation of tidy data by Hadley Wickham. Why ensure that your data is tidy? Well, there are two main advantages (according to Hadley Wickham): There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in mutate and summary functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. We’ll see all of this in action over the coming weeks - but if you’d like, you can hear this explanation directly from one of R-Studio’s very own resident Data Scientists below. You don’t need to listen to the whole video, but from the beginning until approximately 7:40mins. Tidy Data in R* {-} The R-Studio Youtube channel is also generally a great resource for you to be aware of as you pursue your learning in R. So what is the Tidyverse? The tidyverse is a collection of packages that are specifically designed for these data sciencey tasks of data wrangling, management, cleaning, analysis and visualisation within R-Studio. Our earlier lecture introduced you to the concept of a package - but they are explained in more detail below. Whilst in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures as we’ll see over the coming weeks. The tidyverse itself is treated and loaded as a single package, but this means if you load the tidyverse package within your script (through library(tidyverse)), you will directly have access to all the functions that are part of each of the packages that are within the overall tidyverse. This means you do not have to load each package seperately - saving us lines of code, sweet! We’ve already gone ahead and executed the code to install the tidyverse within our various versions of R - but because the tidyverse consists of multiple packages, it may take a little while before everything is installed so be patient! For more information on tidyverse, have a look at https://www.tidyverse.org/. How does the tidyverse help with tidying data? There are some specific functions in tidyverse suite of packages that will help us cleaning and preparing our datasets to create a tidy dataset. The most important and useful functions, from the tidyr and dplyr packages, are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgropus for further processing tidyr pivot_longer convert data from wide format to long format tidyr pivot_wider convert long format dataset to wide format These functions all complete very fundamental tasks that we need to manipualte and wrangle our data. We will get to use these over the coming weeks, so do not panic about trying to remember them all right now. Installing and using Libraries/Packages for Data Analysis As you will have heard in our earlier lecture, our common Data Analysis languages, including Python and R, have developed large community bases and as a result there are significant amount of help and support resources for those working in data science. Beyond help and support, these large community bases have been essential for expanding the utility of a programming language for specific types of data analysis. This is because of how programming languages work – they have a core library of functions to do certain things, such as calculate the mean of a dataset as we did earlier. But to do more specific or specialized analysis, such as create a buffer around a point, a function needs to be written to enable this. You either need to write the function yourself – or hope that someone else has written it – plus you need to know that there is the supporting functions around it. E.g. your code can “read” your spatial data and know a) its spatial and b) the projection system its in to calculate a distance. Without this, you won’t be able to run your function or do your analysis. These community bases have identified these gaps, such as for spatial data reading and analysis, and spent considerable amount of time writing these functions and supporting functions to add to the core library. These functions often get packaged into an additional library (or can be called a package) that you add to your own core library by installing this library to your computer AND then importing it to your work through your script. The tidyr and dplyr packages with the tidyverse are just two examples of these additional libraries created by the wider R community. The code you just ran asked R-Studio to fetch and install the tidyverse into your R-Studio - so this means we’ll be able to use these libraries in our practical below simply by using the library(tidyverse) code at the top of our script. One thing we need to be aware of when it comes to using functions in these additional libraries, is that sometimes these functions are called the same thing as the base R package, or even, in some cases, another additional library. We therefore often need to specify which library we want to use this function from, and this can be done with a simple command (library::function) in our code - as we’ll see in practice over the next few weeks, so just make a mental note of this for now. Whilst we’ve gone ahead and installed the tidyverse, each time we start a new script, we’ll need to load the tidyverse. We are going to show all of this in our next Prcatical, which gets you to analyse crime in London whilst putting into place everything we’ve been dicussing today. Practical 3: Analysing Crime in 2020 in London Wow, we’ve got through a lot today - and barely even started our practical! But, what I can say, is that there is not a substantial more to learn in terms of principles and practicalities of programming beyond building up your “dictionary/vocabulary” of programming libraries and respective commands. There are some more complicated coding things we can do, such as for and while loops and if statements, but, for now, consider yourself a solid beginner programmer. As a result, we’re ready to put this all into practice in our practical today, which will be relatively short in comparison to everything you’ve been through above. What we’ll be doing today is running an exploratory data analysis, using basic statistics, of crime in London over a monthly basis. Let’s get started. Setting Up R-Studio for GEOG0030 In the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this isn’t routinely the case. In RStudio, we can use scripts to build up our code that we can run repeatedly - and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by creating a new project. To put it succintly, projects in R-Studio keep all the files associated with a project together — input data, R scripts, analytical results, figures. This means we can easily keep track of - and access - inputs and outputs from different weeks across our module, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with directories and paths a whole lot easier - particularly if you have followed the folder structure I advised at the start of the module. Let’s go ahead and make a New Project directly within our GEOG0030 folder. Click on File -&gt; New Project –&gt; Existing Directory and browse to your GEOG0030 folder. Click on Create Project. You should now see your main window switch to this new project - and if you check your Files window, you should now see a new R Project called GEOG0030: We are now “in” the GEOG0030 project - and any folders within the GEOG0030 project can be easily accessed by our code. Furthermore, any scripts we create will be saved in this project. Note, there is not a “script” folder per se, but rather your scripts will simply exist in this project. You can test this change of directly by selecting the Terminal window (next to your Console window) to access the Terminal in R-Studio and type our pwd command. You should see that our current directory is your GEOG0030 folder. R for Data Science by Hadley Wickham and Garret Grolemund Your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. I’d stick with these sections for now, but have a quick glance at what’s available in the book. Setting up our script In our shorter practical sessions above, we’ve had a bit of fun playing with the R code within the R console and seeing how we can store variables and access information about them. Furthermore, we’ve had a look at the different data structures we may use moving forward. But ultimately this really doesn’t offer the functionality that we want for our work - or even the reality of what we need to do with spatial analysis. What we really want to do is to start building scripts and add start analysing some data! Therefore, for the majority of our analysis work, we will type our code within a script and not the console. Let’s create our first script: Click on File -&gt; New File –&gt; R Script. You can also use the plus symbol over a white square as a shortcut or even Ctrl/CMD + Shift + N. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date. Let’s go ahead and save this script straight away. Save your script as: wk4-csv-processing.r. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with csv processing. This will help us a lot in the future when we come to find code that we need for other projects. I personally tend to use one script per type of processing or analysis that I’m completing. For example, if you are doing a lot of data cleaning to create a final dataset that you’ll then analyse, its best practice to separate this from your analysis script so you do not continually clean your raw datasets when you run your script. Giving our script some metadata The first bit of code you will want to add to any script is to add a TITLE. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (#) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run. Let’s go ahead and give our script a TITLE - and maybe some additional information: Add the following to your script (substitute accordingly): # Combining Police Data csvs from 2020 into a single csv # Followed by analysis of data on monthly basis # Script started January 2021 # NAME Save your script. Load Our Libraries Now we have our title, the second bit of code we want to include in our script is to load our libraries (i.e. the installed packages we’ll need in our script): Type the following into the script: # Libraries used in this script: # Load the tidyverse library library(tidyverse) By loading simply the tidyverse, we have a pretty good estimate that we’ll be able to access all the functions that we’re going to need today. However, often when developing a script, you’ll realise that you’ll need to add libraries as you go along in order to use a specific function etc. When you do this, always add your library to the top of your script - if you ever share your script, it helps the person you are sharing with recognise quickly if they need to install any additional packages prior to trying to run the script. It also means your libraries do not get lost in the multiple lines of code you are writing. Setting Up Our Directory Understanding and accessing our directory path used to be the worst part of programming. And if you do not use the Project approach advocated above, it certainly will continue to be. If, for example, we did not use a project approach, we would need to set our working directory directly within our script using the command: setwd(\"file/path/to/GEOG0030) We’d therefore need to know what our path is and hope we do not make any mistakes. There are some automated shortcuts to doing this in R using the Files window, but ultimately, having to set a working directory is becoming a thing of the past. Because we are using a project approach - we do not need to set a working directory - because we’re already in it! Therefore, when looking for data in our folders, we know pretty much the path we’ll need to take. However, we still might need to access data from another folder outside of our GEOG0030 folder - so we need to know how to do this. To help with this, we’re going ot add one more library to our library list, called the here library. We won’t go into too much detail what this library does per se, but essentially it alows you to direct R to a specific area on your computer and a specific file with relative ease. We actually won’t use it in this practical, but I wanted to get you into the habit of adding it to your scripts by default. First, you’ll need to install this library to your computer. In your Console window, type and execute: # Install the here library via your console install.packages(&quot;here&quot;) Once installed, we can go ahead and load this after our tidyverse library - your script should look like so: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) One thing to note, not only does installing and loading libraries need to occur in two different parts of R-Studio, but when installing, your library needs to be in \"\" but when loading, it does not. File and folder names best practice Please ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; &amp; $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ if you like. Remember to save your script. We’re now ready to run these first two lines of code. Running a script in R-Studio There are two main ways to run a script in R-Studio - all at once or by line/chunk by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code iteratively. To run line-by-line: By clicking: Select the line or chunk of code you want to run, then click on Code and choose Run selected lines. By key commands: Select the line or chunk of code you want to run and then hold Ctl or Cmd and press Return. To run the whole script By clicking: Click on Run on the top-right of the scripting window and choose Run All. By key commands: Hold Option plus Ctl or Cmd and R. Stopping a script from running If you are running a script that seems to be stuck (for whatever reason) or you notice some of your code is wrong, you will need to interrupt R. To do so, click on Session -&gt; Interrupt R. If this does not work, you may end up needing to Terminate R but this may lose any unsaved progress. Run your code line-by-line In this practical, I recommend running each line (or set of lines) of code you enter as you go - rather than wait til the end and execute the whole script. This way you will be able to find any bugs as you go along. Don’t forget to scroll to the top of your script and execute your library loads! Data Import and Processing We’re now ready to get started with using the crime data csvs currently sat in our all_crime folder. To do so, we need to first figure out how to import the csv and understand the data structure it will be in after importing. Importing and loading Data To read in a csv into R requires the use of a very simple function: read_csv(). We can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single csv, we’ll go ahead and just use the function with a simple parameter. # Read in a single csv from our crime data crime_csv &lt;- read_csv(&quot;data/raw/crime/all_crime/2020-11-metropolitan-street.csv&quot;) We can explore the csv we have just loaded as our new crime_csv variable and understand the class, attributes and dimensions of our variable. # Check class and dimensions of our data frame # you can also check the attributes if you would like - this will load up a huge list of every row though! class(crime_csv) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; dim(crime_csv) ## [1] 96914 12 We’ve found out our variable is a data frame, containing 96914 rows and 12 columns. We can also tell it’s a big file - so best not load it up right now. We however do not want just the single csv and instead what to combine all our csvs in our all_crime folder into a single dataframe - so how do we do this? Joining all of our csvs files together into a single data frame This will be the most complicated section of code you’ll come across today, and we’ll use some functions that you’ve not seen before - we also need to install and load an additional library to use something known as a pipe function which I’ll explain in more detail next week. In your console, install the magrittr package: # Install the magrittr library via your console install.packages(&quot;magrittr&quot;) And in your # Load libraries section of your script, add the magrittr library. Your library section should look like this: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) library(magrittr) Remember to execute the loading of the magrittr library by selecting the line and running the code. Now we’re ready to add and run the following code: # Read in all csvs and append all rows to a single data frame all_crime_df &lt;- list.files(path=&quot;data/raw/crime/all_crime&quot;, full.names=TRUE) %&gt;% lapply(read_csv) %&gt;% bind_rows This might take a little time to process (or might not), as we have a lot of data to get through. You should see a new datafarme appear in your global environment called all_crime, for which we now have 1,099,507 observations! Explaining the above code It is a little difficult to explain the code above without going into a detail explanation of what a pipe is (next week) but essentially what these three lines of code does is: List of of the files found in the data path: \"data/raw/crime/all_crime Read each of these as a csv (this is the lapply() function) in as a dataframe And then bind the rows of these dataframes to a single dataframe called all_crime_df We’ll go into more detail about pipes next week. We can now have a look at our large dataframe in more detail. Let’s have a look # Understand our all_crime_df cols, rows and print the first five rows ncol(all_crime_df) ## [1] 12 nrow(all_crime_df) ## [1] 1099507 head(all_crime_df) ## # A tibble: 6 x 12 ## `Crime ID` Month `Reported by` `Falls within` Longitude Latitude Location ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 37c663d86… 2020… City of Lond… City of Londo… -0.106 51.5 On or n… ## 2 5b89923fa… 2020… City of Lond… City of Londo… -0.118 51.5 On or n… ## 3 fb3350ce8… 2020… City of Lond… City of Londo… -0.113 51.5 On or n… ## 4 07172682a… 2020… City of Lond… City of Londo… -0.112 51.5 On or n… ## 5 14e02a604… 2020… City of Lond… City of Londo… -0.111 51.5 On or n… ## 6 &lt;NA&gt; 2020… City of Lond… City of Londo… -0.0980 51.5 On or n… ## # … with 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime ## # type` &lt;chr&gt;, `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt; You should now see with have the same number of columns as our previous single csv, but 1,099,507 rows! You can also see that the head() function provides us with the first five rows of our dataframe. You can conversely use tail() to provide the last five rows. Filtering our data to a new variable For now in our analysis, we only want to extract the theft crime in our dataframe - so we will want to filter our data based on the Crime type column. However, as we can see, we have a space in our field name for Crime type and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding (or else our code will break!), we need to rename our fields. To do so, we’ll first get all of the names of our fields so we can copy and paste these over into our code: # Get the field names of our all_crime_df names(all_crime_df) ## [1] &quot;Crime ID&quot; &quot;Month&quot; &quot;Reported by&quot; ## [4] &quot;Falls within&quot; &quot;Longitude&quot; &quot;Latitude&quot; ## [7] &quot;Location&quot; &quot;LSOA code&quot; &quot;LSOA name&quot; ## [10] &quot;Crime type&quot; &quot;Last outcome category&quot; &quot;Context&quot; We can now copy over these values into our code to create a new vector variable that contains these field names, updated without spaces. We can then rename the field names in our dataset - just as we did with our GISS table earlier: # # Create a new vector containing updated no space / no capital field names no_space_names &lt;- c(&quot;crime_id&quot;, &quot;month&quot;, &quot;reported_by&quot;, &quot;falls_within&quot;, &quot;longitude&quot;,&quot;latitude&quot;, &quot;location&quot;, &quot;lsoa_code&quot;, &quot;lsoa_name&quot;, &quot;crime_type&quot;, &quot;last_outcome_category&quot;, &quot;context&quot;) # Rename our df field names using these new names names(all_crime_df) &lt;- no_space_names Note, we could have cleaned our data further and so would only needed to rename a few columns using slicing - but we’ll save data frame slicing for next week!. We now have our dataframe ready for filtering - and to do so, we’ll use the filter() function for the dplyr library. This function is really easy to use - but there is also a filter() function in the R base library - that does something different to the function in dplyr. As a result, we need to use a specific type of syntax - library::function - to tell R to look for and use the the filter function from the dplyr library rather than the default base library. We then also need to populate our filter() function wsith the necessary paramteres to extract only the “Theft from the person” crime type. This includes providing the function with our main dataframe plus the filter query, as outliend below: # Filter all_crime_df to contain only theft, store as a new variable: all_theft_df all_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == &#39;Theft from the person&#39;) You should now see the new variable appear in your Environment with 28,476 observations. Great, you’ve completed your first ever filter using programming. We now want to follow the tidy data philosophy and create one final dataframe to allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London - and luckily for us dplyr has another function that will do this for us, known simply as count(). You perhaps can see already that dplyr is likely to become well-used library by us in Geocomputation…! Go ahead and search the help to understand the count() function - you’ll also see that there is only one function called count() so far, i.e. the one in the dplyr library, so we do not need to use the additional syntax we used above. Let’s go ahead and count the number of thefts in London by month. The code for this is quite simple: # Count in the all_theft_df the number of crimes by month and store as a new dataframe theft_month_df &lt;- count(all_theft_df, month) We’ve stored the output of our count() function to a new dataframe: theft_month_df. Go ahead and look at the dataframe to see the output - it’s a very simple table containing simply the month and n, i.e. the number of crimes occuring per month. We can and should go ahead and rename this column to help with our interpretation of the dataframe. We’ll use a quick approach to do this, that uses selection of the precise column to rename only the second column: # Rename the second column of our new data frame to crime_totals names(theft_month_df)[2] &lt;- &quot;crime_totals&quot; This selection is made through the [2] element of code added after the names() function we have used earlier. We’ll look more at selection, slicing and indexing in next week’s practical. Data Analysis: Distribution of Crime in London by month in 2020 We now have our final dataset ready for our simple analysis for our Assignment. Assignment 1: Analysis of crime per month in London in 2020 For your assignment this week, I would like you to complete two tasks. 1. Basic statistics First what I would like you to do, using the code you’ve written previously is to find out: What is the mean average crime per month? What is the median average crime per month? You may also automate the collection of the max, min and range of our crime per month dataframe. 2. Monthly graph The second thing I would like you to do is present our data on a simple bar chart. The basic code to generate a bar chart is provide here: # Read in a single csv from our crime data barplot(theft_month_df$crime_totals, main=&quot;Crime distribution in London by month in 2020&quot;, names.arg = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;)) As you’ll see, we have added a Title to our graph (main= argument), whilst used the names.arg parameter to add the months of the year in along the x-axis. Using the barplot() documentation, I would like you to figure out how to change the bar chart fill and borders another colour from grey and black respectively. You may also look to customise your chart further, but primarily I’d like you to work out how to change these colours to something more aesthetically appealing! You do not need to submit your bar chart this week, but have it ready for your work in Week 5. Beyond the assignment, just take a look at your bar chart and how the distribution of crime changed last year… Well that’s pretty cool, huh?! I wonder what happened in March to make theft from a person decrease so substantially? We’ve managed to take a dataset of over 1 million records and clean and filter it to provide a chart that actually shows the potential impact of the COVID-19 lockdown on theft crime in London. Of course, there is a lot more research and exploratory data analysis we’d need to complete before we could really substantiate our findings, but this first chart is certainly a step in the right direction! Next week, we’ll be doing a lot more with our dataset - including a lot more data wrangling and of course spatial anlaysis, but hopefully this week has shown you want you can achieve with just a few lines of code. Now, make sure to save your script, so we can return to it next week. You do not need to save your workspace - but can do so if you’d like. Saving the workspace will keep any variables generated during your current session saved and available in a future session. Extension Activity: Mapping Other Crime Type Distributions Across London If you’ve whizzed through this workshop and would like an additional challenge, you are more than welcome to deploy the code you’ve used above on one or more other crime types in London. If you remember, each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. You can conduct the same analysis on one or more of these categories in addition to theft, to see if you can find a similar pattern in their prevalance/distribution over the same months. R for Data Science by Hadley Wickham and Garret Grolemund As highlighted earlier, your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. In addition, you can look at Sections: 1, 2, 4, 11 and 12. I’d stick with these sections for now, but have a quick glance at what else is available in the book. We’ll be looking at Data Transformation (5) and Visualisation (3) next week, plus more on Exploratory Data Analysis (7) in Week 6. Don’t worry about completing the exercises - unless you would like to! Recap In this section you have learnt how to: Create an R script. Load a csv into R, perform some analysis, and write out a new csv file to your working directory. Subset R data frames by name and also column and/or row number. Created a simple graph to plot crime in London by month. Learning Objectives You should now hopefully be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R Acknowledgements Part of this page is adapted from POLS0008: Understanding Data and GEOG0114: Exploratory spatial data analysis by Justin Van Dijk at UCL as well as Software Carpentry’s Introduction to R Data Types and Data Structures Copyright © Software Carpentry. The examples and datasets used in the workbook are original to GEOG0030. "],["programming-for-giscience-and-spatial-analysis.html", "5 Programming for GIScience and Spatial Analysis", " 5 Programming for GIScience and Spatial Analysis Welcome to Week 5 in Geocomputation! This week is, again, heavily practical oriented - with our practical taking up the majority of our time this week. You’ll find in this practical, many additional explanations of key programming concepts - such as selection, slicing and pipes - integrated within it. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. Week 5 in Geocomp To be added on Thursday - always nice to say hi to you all! Video on Stream This week’s content introduces you to the foundational concepts associated with Programming for Spatial Data Analysis, where we have three new areas of work to focus on: Data wrangling in programming (using indexing, selection and slicing) Using spatial libraries in R to store and manage spatial data Using visualisation libraries in R to map spatial data This week’s content is split into 4 parts: Spatial Analysis for Data Science Research (20 minutes) Spatial Analysis Software and Programming (20 minutes) Spatial Analysis in R-Studio (40 minutes) Practical 4: Analysing Crime in 2020 in London from a spatial perspective (90 minutes) This week, we have 2 lectures (15 mins and 40 mins), and an additional instructional video to help you with the completion of this week’s practical. A single Key Reading is found towards the end of the workshop. After promising to set a Mini-Project during Reading Week, I appreciate the delivery of this material is late, so I will not be setting the Project as promised. Instead, I would like you to spend time going through the practical and experimenting with the visualisation code at the end. There is also an extension that I would like you to complete, if possible over Reading Week. Part 4 is, as usual, the main part of analysis for our Practical for this week - all programming this week is within Part 4, which is a little longer than usual to account for this. If you have been unable to download R-Studio Desktop or cannot access it via Desktop@UCL Anywhere, you will have access to our R-Studio Server website instead. Instructions on how to access this are provided in the previous week’s workshop. Learning Objectives By the end of this week, you should be able to: Understand how spatial analysis is being used within data science applications Recognise the differences and uses of GUI GIS software versus CLI GIS software Understand which libraries are required for spatial analysis in R/R-Studio Conduct basic data wrangling in the form of selection and slicing Create a map using the tmap visualisation library We will continue to build on the data analysis we completed last week and look to further understand crime in London by looking at its prevalence on a month-by-month basis but this time, from a spatial perspective. Spatial Analysis for Data Science Research Over a decade ago, when I first became involved in the GIScience world, the term “data science” barely existed - fast-forward to today, and it doesn’t go a day without hearing the phrase and the hubris surrounding its potential to help solve the many grand challenges the modern world faces. Whilst there is much hubris (and not a huge amount of evidence) of data science’s ability to “save the world”, on a more fundamental level, data science, and the community of practice associated with it, is having a transformational impact on how we think about and “do” data-focused (and primarily quantitative) research. For us geographers and geographically-minded analysts, our traditional use of GIScience and spatial analysis is most certainly not immune to this transformation - many of the datasets assicated with data science do have a locational component and thus we have seen an increasing interest in and entry into the spatial analyis field from more “generalised” data analysts or data scientists. Furthermore, the increasing popularity of data science amongst ourselves as geographers is also having a signficant impact on how we “do” spatial anaysis. We have, as a result, seen a greater focus on the use of programming as a primary tool within spatial analysis, concomitant to a new prioritisation of openness and reproducibility in our research and documentation of our results. Hence why, a decade later, an Undergraduate module on GIScience now focues on “Geocomputation”, a precursor to spatial data science, rather than a more generalised understanding of the GIS industry and the traditional applications of GIS and spatial analysis, such as: Transportation Logistics Supply Chain Management Generalised Urban Plannign Insurance Environmental modelling Whilst these traditional applications and industries still utilise GIS software (and there is substantial potential to build careers in these areas, particularly through the various Graduate Schemes offered by related companies such as Arup, Mott MacDonald, Esri, to name a few), with data science emerging as a dominant area of growth in spatial analysis, it is important to prioritise the skills you will need to complete in the relevant sectors that are hiring “spatial data scientists”, i.e. learning to code effectively and efficiently. Once you have acquired these skills, the outstanding question becomes: how will I apply them in my future career? Whilst the majority of spatial analysis using programming is not exactly too different from spatial analysis using GIS software, the addition of programming skills have opened up spatial analysis to many different applications and, of course, novel datasets. Within academia and research itself, we see the use of spatial analysis within data science research for: 1. Analysis of distributions, patterns, trends and relationships within novel datasets The most basic application of spatial analysis - but one that now utilises large-scale novel datasets, such as mobile phone data, social media posts and other human ‘sensor data’. To get a better understanding of the various applications, a key recommendation is to look at Carto’s introduction video to their Spatial Data Science conference held this year, where they highlighted how spatial data science has been used for various applications within COVID-19. As a commerical firm, they seem to have a bit of cash to make great videos, but I’d also recommend looking at the various talks held at the conference this year that show the diversity of applications using spatial data science from the various participants. Carto’s take on the use of spatial data science for COVID-19 2. Supplementing the analysis of traditional datasets for augmented information Adding a ‘spatial data science’ edge to traditional analysis, supplementing “small” datasets with big data (or vice versa) to provide new insights into both datasets. An example of this is the recent combination of geodemographic classification (Week 9) with big data information on mobility (e.g. mobile phone data, travel card data) to understand different types of commuter flows and thinking through how this can inform better urban planning policy. A recent paper that did just such is from Liu and Cheng (2020), with the following abstract: Plentiful studies have discussed the potential applications of contactless smart card from understanding interchange patterns to transit network analysis and user classifications. However, the incomplete and anonymous nature of the smart card data inherently limit the interpretations and understanding of the findings, which further limit planning implementations. Geodemographics, as ‘an analysis of people by where they live’, can be utilised as a promising supplement to provide contextual information to transport planning. This paper develops a methodological framework that conjointly integrates personalised smart card data with open geodemographics so as to pursue a better understanding of the traveller’s behaviours. It adopts a text mining technology, latent Dirichlet allocation modelling, to extract the transit patterns from the personalised smart card data and then use the open geodemographics derived from census data to enhance the interpretation of the patterns. Moreover, it presents night tube as an example to illustrate its potential usefulness in public transport planning. (Yunzhe Liu &amp; Tao Cheng (2020) Understanding public transit patterns with open geodemographics to facilitate public transport planning, Transportmetrica A: Transport Science, 16:1, 76-103, DOI: 10.1080/23249935.2018.1493549) We’ll be looking at this in a little more detail in Week 9. 3. Creation of new datasets from both traditional and novel datasets Opening up spatial analysis to novel datasets has enabled many researchers to identify opportunities in the creation of new datasets that can ‘proxy’ certain human behaviours and characteristics that we currently do not either have data for, or the data is old/insufficient/not at the right scale. A good example of this is my previous research group at the University of Southampton: Worldpop. Worldpop create population and socio-economic datasets for every country across the world utilising (primarily) bayesian modelling approaches alongside both census data and more innovative datasets, such as mobile phone data or tweets. You can watch this incredibly cheesey but informative video made by Microsoft about the group below: What does Worldpop do? There are plenty of examples in recent GIS and spatial analysis research where new datasets are/have been created for use in similar applications. Another example is Facebook, who is using a lot of their socila network data to create mobility and social connectivity datasets with their ‘Data For Good’ platform (see more here). 4. Creation of new methods and datasets Finally, the intersection of data science and spatial analysis has also seen geographers adapt data science techniques to create new methods and analytical algorithims to puruse the creation of more new datasets and/or new insight. An example of this is the increased use and adaptation of the DB-Scan algorithm (Week 7) within urban analytics, seen within the various papers: Xinyi Liu, Qunying Huang &amp; Song Gao (2019) Exploring the uncertainty of activity zone detection using digital footprints with multi-scaled DBSCAN, International Journal of Geographical Information Science, 33:6, 1196-1223, DOI: 10.1080/13658816.2018.1563301 Arribas-Bel, D., Garcia-López, M. À., &amp; Viladecans-Marsal, E. (2019). Building (s and) cities: Delineating urban areas with a machine learning algorithm. Journal of Urban Economics, 103217. Jochem, W. C., Leasure, D. R., Pannell, O., Chamberlain, H. R., Jones, P., &amp; Tatem, A. J. (2020). Classifying settlement types from multi-scale spatial patterns of building footprints. Environment and Planning B: Urban Analytics and City Science. https://doi.org/10.1177/2399808320921208 Beyond these research-oriented applications, we can also think of many ‘data sciencey’ applications that we use in our day to day lives that use spatial analysis as a key component. From the network analysis behind route-planning within mapping applications to searching travel apps for a new cafe or restaurant to try, not only does spatial analysis underline much of the distance and location-based metrics these applications rely on, it also helps to integrate many of the novel datasets - such as traffic estimations or social media posts - that augment these distance metrics and become invaluable to our own decision-making. Applications of Spatial Analysis with ‘Data Science’ Applications A short blog piece by Esri on the insight that can be derived from spatial analysis can be found here. Spatial Analysis Software and Programming This week - and the previous - is your first introduction in our module to using R-Studio for the management and and analysis of spatial data. Prior to this, we’ve been using traditional GIS software in the form of QGIS. As we’ve suggested above, the increasing popularity of data science is having a signficant impact on how we “do” spatial anaysis, with a shift in focus to using programming as our primary tool rather than traditional GIS-GUI software. GUI-GIS software still has its place and purpose, particularly in the wider GIScience and GIS industry - but when we come to think of data science, the command line has become the default. Behind this shift in focus, alongside the need to have a tool that is capable of handling large datasets, has been a focus on improving openness and reproducibility within spatial analysis research. As Brunsdon and Comber (2020) propose: Notions of scientific openness (open data, open code and open disclosure of methodology), collective working (sharing, collaboration, peer review) and reproducibility (methodological and inferential transparency) have been identified as important considerations for critical data science and for critical spatial data science within the GIScience domains. (Brunsdon, C., Comber, A. Opening practice: supporting reproducibility and critical spatial data science. J Geogr Syst (2020). https://doi.org/10.1007/s10109-020-00334-2) As part of this move towards openness and reproducibility within spatial data science, we can look to the emerging key principles of data science research to explain why programming is becoming the primary tool for spatial analysis research. Key principles of data science research When thinking about spatial analysis, we can identify the key principles of data science as: 1. Repeatability: the idea that a given process will produce the same (or nearly the same) output given similar inputs. Instruments and procedures need to be consistent. 2. Reproducibility: There are three types of reproducibility when we think of data science research. Statistical reproducibility: an analysis is statistically reproducible when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. Empirical reproducibility: an analysis is empirically reproducible when detailed information is provided about non-computational empirical scientific experiments and observations. In practice, this is enabled by making data freely available, as well as details of how the data was collected. Computational reproducibility: an analysis is computationally reproducible if there is a specific set of computational functions/analyses (in data science, almost always specified in terms of source code) that exactly reproduce all of the results in an analysis. 3. Collaboration: an analysis workflow that is easy to share work with others and collaborate, preferably in real-time, alongside easy integration with version control. 4. Scalability: at its most basic, an analysis that can re-run the same processing easily, with simple adjustment of variables and parameters to include additional data; at an intermediate level, the analysis and workflow can be easily expanded to include larger datasets (which require more processing requirements); at the most advanced, the workflow is suitable for distributed/multiple core computing. We can use these principles to review the different tools/software available to us for spatial analysis, in order to be confident moving forward, that we use the appropriate tools for the tasks we have at hand. A Review of Spatial Analysis Software Slides | Video on Stream Spatial Analysis in R-Studio We have now seen that for us, to work towards completing spatial analysis research that adheres to these data science pricinples, we need to focus on using programming tools, such as R and R-Studio, rather than the traditional GIS GUI software. But the question is, how do we use R and R-Studio as a piece of GIS software? As you’ll already have seen, there are quite a few aesthetic differences between R-Studio and Q-GIS - for one, there is no \"map canvas area where we’ll see our data as we load it. There are also quite a few other differences in terms of how we: Load Manage Process Analyse Visualise Disseminate spatial data and our spatial analysis outputs. To help you understand these differences, the following longer lecture (approximately 40 minutes) provides you with a thorough introduction into how we use R-Studio as a GIS software: Using spatial data in R/R-Studio Slides | Video on Stream Practical 4: Analysing Crime in 2020 in London from a spatial perspective Now we’ve had our introduction to using R-Sutdio as a GIS software, it’s time to get started using it ourselves for spatial analysis. As outlined earlier, we’ll be completing an analysis of our crime dataset in London, but rather than solely looking at crime change of time - we’re going to add in a spatial component to our analysis, and understanding how crime has changed across our wards over the year. To do this, what we’ll first do is head back to our script from last week, run our script - and then write our all_theft_df to a csv file. If you had saved your environment from last week, keeping your variables in the memory, theoretically you won’t need to export the dataframe as you should have access to this variable within your new script - but it would be good practice to write out the data - and then load it back in. We’re going to be adding in and using a few additional libraries into our script today - but we’ll explain them as and when we use them; for now, just add them into our library section of our script when instructed to below. Overall, our workflow will be: Take our all_theft_df and wrangle it to produce a dataframe with a ward per row with a crime count for each month in our fields. Join this dataframe to our ward_population_2019 shapefile (in your working folder) and then produce a crime rate for each month, for each ward. Create a map for January 2020 using the tmap library. Extension: Create a new dataframe that represents crime from a quarterly perspective and create four maps ready for export. Let’s get started! Write out / export our dataframe from last week Open up R-Studio (Server or Desktop), and make sure you open up your GEOG0030 project. Next open your script from Week 4 - it should be saved as: wk4-csv-processing.r and should be visible in your files from your GEOG0030 project. First check your Environment box - if you have a variable in your Global Environment with the name all_theft_df then you do not need to run your script. If you do not have a variable saved, go ahead and run your script to and including the code that filters our large all_crime_df to only the all_theft_df: # Filter all_crime_df to contain only theft, store as a new variable: all_theft_df all_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == &#39;Theft from the person&#39;) We should all now have an all_theft_df variable in our environment that we’re ready to export to a csv. Scroll to the bottom of your Week 4 script and enter the following code and execute: # Write out the theft_crime_df to a csv within our raw crime data folder write.csv(all_theft_df,&quot;data/raw/crime/all_crime_2020.csv&quot;, row.names = FALSE) Remember, if using a Windows machine, you’ll need to submit your forward-slashes (/) with backslashes, and in this case, within R, it will need to be two backslashes (\\\\). You should now see a new csv within your raw crime data folder (data -&gt; raw -&gt; crime). Save your wk4-csv-processing.r script and then close the script. Setting up your script Open a new script within your GEOG0030 project (Shift + Ctl/Cmd + N) and save this script as wk5-crime-spatial-processing.r. At the top of your script, add the following metdata (substitute accordinlgy): # Analysing crime in 2020 by month and ward # Script started February 2021 # NAME Now let’s add all of the libraries we’ll be using today: # Libraries used in this script: library(tidyverse) library(here) library(magrittr) library(sf) library(tmap) As you’ll have heard in our lecture, we’ll be using sf to read and load our spatial data, use the tidyverse libraries to complete our data wrangling and then use the tmap library to visualise our final maps. The here library enables easy reference to our , whilst magrittr allows us to use the pipe function (%&gt;%) which we’ll explain in a bit more detail below. Loading our datasets We’re going to load both of the datasets we need today straight away: 1) the all_theft_2020.csv we have just exported and 2) the ward_population_2019.shp we created in Week 3. First, let’s load our all_theft_2020.csv into a dataframe called all_theft_df. You should see we use the same read_csv code as last week. For those of use with the variable still stored in your Environment, you can still add this code to your script - it will simply overwrite your current variable (which essentially stores the same data that is contained in the csv). # Read in our all_theft_2020 csv from our raw crime data folder all_theft_df &lt;- read_csv(&quot;data/raw/crime/all_crime_2020.csv&quot;) We can double-check what our csv looks like by either viewing our data or simply calling the head() function on our dataframe. + Call the `View()` function in the console or the `head()` function in the script. Call the head() function on our data to check the first five rows: # Check the first five rows of our all_theft_2020 dataframe head(all_theft_df) ## # A tibble: 6 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37c663d… 2020… City of Lo… City of Lon… -0.106 51.5 On or n… E01000916 ## 2 dcfa16f… 2020… City of Lo… City of Lon… -0.0941 51.5 On or n… E01000002 ## 3 be9310e… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 4 0cbb0c5… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 5 85df9c1… 2020… City of Lo… City of Lon… -0.0761 51.5 On or n… E01000005 ## 6 8249cc1… 2020… City of Lo… City of Lon… -0.0750 51.5 On or n… E01000005 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You should see these rows display in your console. Great, the dataset looks as we remember, with the different fields, including, importantly for this week, the LSOA_code which we’ll use to process and join our data together (you’ll see this in a second!). Next, let’s load our first ever spatial dataset into R-Studio - our ward_population_2019.shp. We’ll store this as a variable called ward_population and use the sf library to load the data: # Read in our ward_population_2019 shp from our working data folder # Note the st_read function here - keep a record of this function as it is your main function to read in shape data # Do not worry about the stringsAsFactors paratmer this week ward_population &lt;- st_read(&quot;data/working/ward_population_2019.shp&quot;, stringsAsFactors = FALSE) ## Reading layer `ward_population_2019&#39; from data source `/Users/Jo/Code/GEOG0030/data/working/ward_population_2019.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 657 features and 7 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## CRS: 27700 You should now see the ward_population variable appear in your Environment window. As this is the first time we’ve loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial data frame. Interacting with spatial data The first thing we want to do when we load spatial data is, of course, map it to see its ‘spatiality’ (I’m going to keep going with that word..) or rather how the data looks from a spatial perspective. To do this, we can use a really simple command from R’s base library: plot(). As we won’t necessarily want to plot this data everytime we run this script in the future, we’ll type this command into the console as a “one-off”. In your console, plot our new spatial data: # Plot our ward_population data plot(ward_population) You should see your ward_population plot appear in your Plots window - as you’ll see, your ward dataset is plotted ‘thematically’ by each of the fields within the dataset, including our POP2019 field we created last week. Note, this plot() function is not to be used to make maps - but simply as a quick way of viewing our spatial data. We can also find out more information about our ward_population data. Let’s next check out our class of our data. Again, in the console type: # Find out the class of our ward_population data class(ward_population) ## [1] &quot;sf&quot; &quot;data.frame&quot; We should see our data is an sf dataframe, which is great as it means we can utilise our tidyverse libraries with our ward_population. We can also use the attributes() function we looked at last week to find out a little more about our “spatial” data frame. Again, in the console type: # Find out the attributes of our ward_population data attributes(ward_population) ## $names ## [1] &quot;NAME&quot; &quot;GSS_CODE&quot; &quot;DISTRICT&quot; &quot;LAGSSCODE&quot; &quot;HECTARES&quot; ## [6] &quot;NONLD_AREA&quot; &quot;POP2019&quot; &quot;geometry&quot; ## ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 ## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 ## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 ## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 ## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 ## [253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 ## [271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 ## [289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 ## [307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 ## [325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 ## [343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 ## [361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 ## [379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 ## [397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 ## [415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 ## [433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 ## [451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 ## [469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 ## [487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 ## [505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 ## [523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 ## [541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 ## [559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 ## [577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 ## [595] 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 ## [613] 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 ## [631] 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 ## [649] 649 650 651 652 653 654 655 656 657 ## ## $class ## [1] &quot;sf&quot; &quot;data.frame&quot; ## ## $sf_column ## [1] &quot;geometry&quot; ## ## $agr ## NAME GSS_CODE DISTRICT LAGSSCODE HECTARES NONLD_AREA POP2019 ## &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## Levels: constant aggregate identity We can see how many rows we have, the names of our rows and a few more pieces of information about our ward_population data - for example, we can see that the specific $sf_column i.e. our spatial information) in our dataset is called geometry. We can investigate this column a little more by selecting this column within our console to return. In the console type: # Get info about the geometry of our ward_population data ward_population$geometry ## Geometry set for 657 features ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## CRS: 27700 ## First 5 geometries: You should see new information about our geometry column display in your console. From this selection we can find out the dataset’s: geometry type dimension bbox (bounding box) CRS (coordinate reference system) And also the first five geometries of our dataset. This is really useful as one of the first things we want to know about our spatial data is what coordinate system it is projected with. As we should know, our ward_population data was created and exported within British National Grid, therefore seeing the EPSG code of British National Grid - 27700 - as our CRS confirms to us that R has read in our dataset correctly! We could also actually find out this information using the st_crs() function from the sf library. # Find out the CRS of our ward_population data st_crs(ward_population) ## Coordinate Reference System: ## User input: 27700 ## wkt: ## PROJCS[&quot;OSGB 1936 / British National Grid&quot;, ## GEOGCS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB_1936&quot;, ## SPHEROID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## AUTHORITY[&quot;EPSG&quot;,&quot;7001&quot;]], ## TOWGS84[446.448,-125.157,542.06,0.15,0.247,0.842,-20.489], ## AUTHORITY[&quot;EPSG&quot;,&quot;6277&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4277&quot;]], ## PROJECTION[&quot;Transverse_Mercator&quot;], ## PARAMETER[&quot;latitude_of_origin&quot;,49], ## PARAMETER[&quot;central_meridian&quot;,-2], ## PARAMETER[&quot;scale_factor&quot;,0.9996012717], ## PARAMETER[&quot;false_easting&quot;,400000], ## PARAMETER[&quot;false_northing&quot;,-100000], ## UNIT[&quot;metre&quot;,1, ## AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], ## AXIS[&quot;Easting&quot;,EAST], ## AXIS[&quot;Northing&quot;,NORTH], ## AUTHORITY[&quot;EPSG&quot;,&quot;27700&quot;]] You’ll see we actually get a lot more information about our CRS beyond simply the code using this function. This function is really important to us as users of spatial data as it allows us to retrieve and set the CRS of our spatial data (the latter is used in the case the data does not come with a .proj file but we do know what projection system should be used). To reproject data, we actually use the st_transform() function - but we’ll take a look at this in more detail in Week 7. The final thing we might want to do before we get started with our data analysis is to simply look at the data table part of our dataset, i.e. what we’d call the Attribute Table in Q-GIS, but here it’s simply the table part of our data frame. To do so, you can either use the View() function in the console or click on the ward_population variable within our enviroment. Processing our crime data to create our required output data frame Now we have our data loaded, our next step is to process our data to create what we need as our final output for analysis: a spatial dataframe that contains a theft crime rate for each ward for each month (of available data) in 2020. But wait - if we look at our all_theft_df, we do not have a field that contains the wards our crimes have occured in. We only have two types of spatial or spatially-relevant data in our all_theft_df: The approximate (“snap point”) latitude and longitude of our crime in WGS84. The Lower Super Output Area (LSOA) in which it occured. From Week 3’s practical, we know we can map our points using the coordinates and then provide a count by using a point-in-polygon (PIP) operation. However to do this for each month, we would need to filter our dataset for each month and repeat the PIP operation - when we know a little more advanced code, this might end up being quite simple, but for now, when all we’re trying to do is some basic table manipulation, surely there must be a quicker way? Adding Ward Information to our all_theft_df dataframe Yes, there is! All we need to do is figure our which Ward our LSOAs fall within and then we can add this as an additional attribute or rather column to our all_theft_df - so how do we do this? From a GIScience perspective, there are many ways to do this - but the most straight forward is to use something called a look-up table. Look-up tables are an extremely common tool in database management and programming, providing a very simple approach to storing additional information about a feature (such as a row within a dataframe) in a separate table that can quite literally be “looked up” when needed for a specific application. In our case, we will actually join our look-up table to our current all_theft_df to get this information “hard-coded” to our dataframe for ease of use. To be able to do this, we therefore need to find a look-up table that contains a list of LSOAs in London and the Wards in which they are contained. Lucky for us, after a quick search of the internet, we can find out that the Office for National Statisitcs provides this for us in their Open Geography Portal. They have a table that contains exactly what we’re looking for: Lower Layer Super Output Area (2011) to Ward (2018) Lookup in England and Wales v3. As the description on the website tells us, \"this file is a best-fit lookup between 2011 lower layer super output areas, electoral wards/divisions and local authority districts in England and Wales as at 31 December 2018. As we know we are usig - but the LSOAs are still from 2011 within the police data, we know this is the file we’ll need to complete our look-up. In addition, the description tells us what field names are included in our table, which we can have a good guess at which we’ll need before we even open the data: LSOA11CD, LSOA11NM, WD18CD, WD18NM, WD18NMW, LAD18CD, LAD18NM. (Hint, it’s the ones beginning with LSOA and WD!) We therefore have one more dataset to download and then load into R. Download the look-up table at the ONS: https://geoportal.statistics.gov.uk/datasets/8c05b84af48f4d25a2be35f1d984b883_0 Move this file in your data -&gt; raw -&gt; boundaries folder and rename to “data/raw/boundaries/lsoa_ward_lookup.csv”. Load the dataset using the read_csv() function: Do not worry if you have a few parsing failures, the table should still work fine. # Read in our lsoa_ward_lookup csv from our raw boundaries data folder lsoa_ward_lookup &lt;- read_csv(&quot;data/raw/boundaries/lsoa_ward_lookup.csv&quot;) ## Warning: 1909 parsing failures. ## row col expected actual file ## 32801 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Broncoed &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32802 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Dwyrain &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32803 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - De &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32804 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - De &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## 32805 WD18NMW 1/0/T/F/TRUE/FALSE Yr Wyddgrug - Gorllewin &#39;data/raw/boundaries/lsoa_ward_lookup.csv&#39; ## ..... ....... .................. ....................... .......................................... ## See problems(...) for more details. Now we have our lookup table, all we are going to do is extract the relevant ward name and code for each of the LSOAs in our all_theft_df. To do so, we’re going to use one of the join functions from the dplyr library. Joining data by fields in programming We’ve already learnt how to complete Attribute Joins in Q-GIS via the Joins tab in the Propeties window - so it should come of no surprise that we can do exactly the same process within R. To conduct a Join between two dataframes (spatial or non-spatial, it does not matter), we use the same principles of selecting a unique but matching field within our dataframes to join them together. As we have seen from the list of fields above - and with our knowledge of our all_theft_df dataframe - we know that we have at least two fields that should match across the datasets: our lsoa codes and lsoa names. We of course need to identify the precise fields that contain these values in each of our data frames, i.e. LSOA11CD and LSOA11NM in our lsoa_ward_lookup dataframe and lsoa_code and lsoa_name in our all_theft_df dataframe, but once we know what fields we can use, we can go ahead and join our two data frames together. But how do we go about join them in R? Within R, you have two options to complete a data frame join: The first is to use the Base R library and its merge() function: By default the data frames are merged on the columns with names they both have, but you can also provide the columns to match separate by using the parameters: by.x and by.y. E.g. your code would look like: merge(x, y, by.x = \"xColName\", by.y = \"yColName\"), with x and y each representing a dataframe. The rows in the two data frames that match on the specified columns are extracted, and joined together. If there is more than one match, all possible matches contribute one row each, but you can also tell merge whether you want all rows, including ones without a match, or just rows that match, with the arguments all.x and all. The second option is to use the Dplyr library and one of their mutate()-based join() functions: dplyr uses SQL database syntax for its join functions. There are four types of joins possible (using this SQL syntax) with the dplyr library. inner_join(): includes all rows in x and y. left_join(): includes all rows in x. right_join(): includes all rows in y. full_join(): includes all rows in x or y. Figuring out which one you need will be on a case by case basis. Again, if the join columns have the same name, all you need is left_join(x, y). If they don’t have the same name, you need a by argument, such as left_join(x, y, by = c(“xName” = “yName”)) . Note the syntax for the by parameter in the dplyr: you submit only the column name you’re interested in (e.g. LSOACD) but within qutation marks (i.e. “LSOACD”). Left of the equals is the column for the first data frame, right of the equals is rthe name of the column for the second data frame. So which approach should I choose? In all cases moving forward, we will use the one of the dplyr join approaches. There are three reasons for using the dplyr approach: The base merge() function does not always work well with data frames and can create errors in your joining. With the dplyr code built on SQL, joins run substantially faster and very well on dataframes. All tidyverse functions use NAs as a part of data, because it should explain some aspects of information that can’t be explained by “identified” data and will not drop NAs during processing, which, if this happens without your realisation, can affect your data and its reliability quite substantially. When using the tidyverse, we often need to use a specific function to drop NA values, e.g. na.omit() or find ways of replacing NAs, as we’ll see later. One thing to note is that there is a new package entering the “game” of data wrangling in R, called data.table. We won’t look into this package now, because its best suited for really large datasets but one to quickly make a note about if you end up dealing with datasets for your dissertations that have millions of entries. Joining our two tables using the left_join() function from dplyr Now we know what set of functions we can use to join our data, let’s go ahead and join our lsoa_ward_lookup dataframe to our all_theft_df dataframe so we can get our ward information. We’re going to need to make multiple joins between our tables as we have multiple entries of crime for the same LSOA - as a result, we’re going to need to use a function that allows us to keep all rows in our all_theft_df dataframe, but we do not need to keep all rows in our lsoa_ward_lookup if those wards are not within our dataset. Let’s have a look in detail at how the four different types of joins from dplyr work: It looks like we’re going to need to use our left_join() function as we want to join matching rows from our lsoa_ward_lookup dataframe to our all_theft_df dataframe but make sure to keep all rows in the latter. Within your script, create a join between our two dataframes and store as a new variable: # Join lsoa_ward_lookup rows to the all_theft_df on our two lsoa code fields # Note again how we state to the two fields we&#39;ll use in the join in the by parameter all_theft_ward_df &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;)) Run the code. Let’s go ahead and check our join - we want to check that our LSOA codes and names match across our new dataframe. In your console, check the first five rows of our new data frame: # Check our join via the first five rows head(all_theft_ward_df) ## # A tibble: 6 x 19 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 37c663d… 2020… City of Lo… City of Lon… -0.106 51.5 On or n… E01000916 ## 2 dcfa16f… 2020… City of Lo… City of Lon… -0.0941 51.5 On or n… E01000002 ## 3 be9310e… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 4 0cbb0c5… 2020… City of Lo… City of Lon… -0.0945 51.5 On or n… E01000003 ## 5 85df9c1… 2020… City of Lo… City of Lon… -0.0761 51.5 On or n… E01000005 ## 6 8249cc1… 2020… City of Lo… City of Lon… -0.0750 51.5 On or n… E01000005 ## # … with 11 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt;, LSOA11NM &lt;chr&gt;, WD18CD &lt;chr&gt;, ## # WD18NM &lt;chr&gt;, WD18NMW &lt;lgl&gt;, LAD18CD &lt;chr&gt;, LAD18NM &lt;chr&gt;, FID &lt;dbl&gt; You should now see that you have with 19 variables: 12 from all_theft_df, plus 7 from lsoa_ward_lookup. Note, the join does not keep the ‘join key’ fields from both dataframes by default. It keeps only the field from the “left” dataframe - hence we are now missing LSOA11CD. To keep both fields in future, we would need to add the keep parameter into our code, and set this to TRUE as so: Do not add this to your script, it is just provided as an example! # Join lsoa_ward_lookup rows to the all_theft_df on our two lsoa code fields # Set keep to TRUE to keep both join key fields all_theft_ward_df &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;), keep = TRUE ) Now we have our joined dataset, we can move forward with some more data wrangling. The thing is, our data frame is getting quite busy - we have duplicate fields and some fields we just won’t need. It would be good if we could trim down our dataframe to only the relevant data that we need moving forward, particularly, for example, if we wanted to go ahead and write out a hard copy of our theft data that now contains the associated ward. To be able to “trim” our data frame, we have two choices in terms of the code we might want to run. First, we could look to drop certain columns from our data frame. Alternatively, we could create a subset of the columns we want to keep from our data frame and store this as a new variable or simply overwrite the currently stored variable. To do either of these types of data transformation, we need to know more about how we can interact with a data frame in terms of indexing, selection and slicing. Data Wrangling: Introducing Indexing, Selection and Slicing Everything we will be doing today as we progress with our data frame cleaning and processing relies on us understanding how to interact with and transform our data frame - this interaction itself relies on knowing about how indexing works in R as well as how to select and slice your data frame to extract the relevant cells, rows or columns and then manipulate them - as we’ll be doing in this practical. Whilst there are traditional programming approaches to this using the base R library, dplyr is making this type of data wrangling easier by the year! If you’ve not used R before - or have but not familiar with how to index, select and slice, I would highly recommend watching this following video that explains the process from both a base R perspective and using the dplyr library - it also includes a good explanation about what our pipe function , %&gt;% , does. I’d love to have time to make this video for you all myself, but this is currently not possible - and this video provides a very accessible explanation. I’ll add some detailed notes as and when we use these functions in the next section of the practical, but for an audio/visual explanation, I’d highly recommend watching this video. Selection and slicing in R As you can see from the video, there are two common approaches to selection and slicing in R, which rely on indexing and/or field names in different ways. The following summarises the above video, for ease of reference during the practical: Base R approach to selection and slicing (common programming approach) The most basic approach to selecting and slicing within programming relies on the principle of using indexes within our data structures. Indexes actually apply to any type of data structure, from single atomic vectors to complicated data frames as we use here. Indexing is the numbering associated with each element of a data structure. For example, if we create a simple vector that stores three strings: # Store a simple vector of three strings simple_vector &lt;- c(&quot;Aa&quot;, &quot;Bb&quot;, &quot;Cc&quot;, &quot;Dd&quot;, &quot;Ee&quot;, &quot;Ff&quot;, &quot;Gg&quot;) R will assign each element (i.e. string) within this simple vector with a number: Aa = 1, Bb = 2, Cc = 3 and so on. Now we can go ahead and select each element by using the base selection syntax which is using square brackets after your element’s variable name, as so: # Select the first element of our variable ss simple_vector[1] ## [1] &quot;Aa&quot; Which should return the first element, our first string containing “Aa”. You could change the number in the square brackets to any number up to 7 and you would return each specific element in our vector. However, say you don’t want the first element of our vector but the second to fifth elements. To achieve this, we conduct what is known in programming as a slicing operation, where, using the [] syntax, we add a : (colon) to tell R where to start and where to end in creating a selection, known as a slice: # Select the second to fifth element of our vector, creating a &#39;slice&#39; of our vector simple_vector[2:5] ## [1] &quot;Bb&quot; &quot;Cc&quot; &quot;Dd&quot; &quot;Ee&quot; You should now see our 2nd to 5th elements returned. You’ve created a slice! Now what is super cool about selection and slicing is that we can add in a simple - (minus) sign to essentially reverse our selection. So for example, we want to return everything but the 3rd element: # Select everything but the third element of our vector simple_vector[-3] ## [1] &quot;Aa&quot; &quot;Bb&quot; &quot;Dd&quot; &quot;Ee&quot; &quot;Ff&quot; &quot;Gg&quot; And with a slice, we can use the minus to slice out parts of our vector, for example, remove the 2nd to the 5th elements (note the use of a minus sign for both): # Select the second to fifth element of our vector, creating a &#39;slice&#39; of our vector simple_vector[-2:-5] ## [1] &quot;Aa&quot; &quot;Ff&quot; &quot;Gg&quot; This use of square brackets for selection syntax is common across many programming languages, including Python, but there are often some differences you’ll need to be aware of if you pursue other languages. For example: Python always starts its index from 0! Whereas we can see here with R, our index starts at 1 R is unable to index the characters within strings - this is something you can do in Python, but in R, we’ll need to use a function such as substring() - more on this another week. But ultimately, this is all there is to selection and slicing - and it can be applied to more complex data structures, such as data frames. Let’s take a look. Selection and slicing in data frames We apply these selection techniques to data frames, but we will have a little more functionality as our data frames are made from both rows and columns. This means when it comes to selections, we can utilise an amended selection syntax that follows a specific format to select individual rows, columns, slices of each, or just a single cell: [ rows, columns] There are many ways we can use this syntax, which we’ll example below using our lsoa_ward_lookup data frame. First, before looking through and executing these examples (in your console) familiarise yourself with the lsoa_ward_lookup data frame: # View lsoa_ward_lookup dataframe, execute this code in your console View(lsoa_ward_lookup) To select a single column from your data frame, you can use one of two approaches. First we can follow the syntax above carefully and simply set our column parameter in our syntax above to the number 2: # Select the 2nd column from the data frame, Opt 1 lsoa_ward_lookup[,2] You should see your second column display in your console. Second, we can actually select our column by only typing in the number (no need for the comma). By default, when there is only one argument present in the selection brackets, R will select the column from the data frame, not the row: # Select the 2nd column from the data frame, Opt 2 lsoa_ward_lookup[2] Note, this is different to when we “accessed” the properties of the column last week using the $ syntax - we’ll look at how we use this in later practicals. To select a specific row, we need to add in a comma after our number - this will tell R to select the relevant row instead: # Select the 2nd row from the data frame lsoa_ward_lookup[2,] You should see your second row appear. Now, to select a specific cell in our data frame, we simply provide both arguments in our selection parameters: # Select the value at the 2nd row and 2nd column in the data frame lsoa_ward_lookup[2,2] What is also helpful in R is that we can select our columns by their field names by passing these field names to our selection brackets as a string. For a single column: # Select the LSOA11NM column (2nd column) by name lsoa_ward_lookup[&quot;LSOA11NM&quot;] Or for many columns, we can supply a combined vector: # Select the LSOA11CD (1st column) and LSOA11NM column (2nd column) by name lsoa_ward_lookup[c(&quot;LSOA11CD&quot;, &quot;LSOA11NM&quot;)] This approach to selecting multiple columns is also possible using the indexing, but in this case we use the slicing approach we saw earlier (note, you cannot slice using field names but need to provide each individual field name within a vector as above). To retrieve our 2nd - 4th columns in our data frame, we can use either approach: # Select the 2nd to 4th columns from our data frame lsoa_ward_lookup[2:4] # Does the same thing: # lsoa_ward_lookup[,2:4] We can also apply the negative # Select everything but the 2nd to 4th columns from our data frame lsoa_ward_lookup[-2:-4] If you do not want a slide, we can also provide a combined list of the columns we want to extract: # Select the 2nd, 3rd, 4th and 7th columns from our data frame lsoa_ward_lookup[c(2, 3, 4, 7)] We can apply this slicing approach to our rows: # Select the 2nd to 4th rows from our data frame lsoa_ward_lookup[2:4,] As well as a negative selection: # Select everything but the 2nd to 4th rows from our data frame lsoa_ward_lookup[-2:-4,] (Note we have fewer rows than we should in the original data frame!) And if it’s not a slice you want to achieve, you can also provide a list of the rows (akin to our approach with the columns above)! And of course, for all of these, we can store the output of our selection as a new variable or pipe it to another function. That’s obviously what makes selection and slicing so useful - however it can be at times a little confusing. Dplyr approach to selection and slicing (making our lives easy!) We’re quite lucky, therefore, as potential data wranglers that the dplyr library has really tried to make this more user-friendly. Instead of using the square brackets [] syntax, we now have functions that we can use to select or slice our data frames accordingly: For columns, we use the select() function that enables us to select a (or more) column(s) using their column names or a range of “helpers” such as ends_with() to select specific columns from our data frame. For rows, we use the slice() function that enables us to select a (or more) row(s) using their position (i.e. similar to the proess above) For both functions, we can also use the negative / - approach we saw in the base R approach to “reverse a selection”, e.g.: # A few Dplyr examples in one! # Select column 2 select(lsoa_ward_lookup, 2) # Select everything but column 2 select(lsoa_ward_lookup, -2) # Select LSOA11CD column, note no &quot;&quot; select(lsoa_ward_lookup, LSOA11CD) # Select everything but column 2 select(lsoa_ward_lookup, -LSOA11CD) # Select LSOA... columns select(lsoa_ward_lookup, starts_with(&quot;LSOA&quot;)) # Select everything but column 2 select(lsoa_ward_lookup, -LSOA11CD) We’ll be using these functions throughout our module, so we’ll leave our examples there for now! In addition to these index-based functions, within dplyr, we also have: filter() that enables us to easily filter rows within our data frame based on specific conditions (such as being a City of London ward). This function requires a little bit of SQL knowledge, which we’ll pick up on throughout the module - but look further at in Week 6. Image: Allison Horst In addition, dplyr provides lots of functions that we can use directly with these selections to apply certain data wrangling processes to only specific parts of our data frame, such as mutate() or count(). We’ll be using quite a few of these functions in the remaining data wrangling section below - plus throughout our module, so I highly recommend downloaded (and even printing off!) the dplyr cheat sheet to keep track of what functions we’re using and why! One thing to note is that in either the base R or dplyr approach, we can use the magrittr pipe - %&gt;% - to ‘pipe’ the outputs of our selection into another function. This is explained in more detail in another section. As we have seen above, whilst there are two approaches to selection using either base R library or the dplyr library, we will continue to focus on using functions directly from the dplyr library to ensure efficiently and compatibility within our code. Within dplyr, as you also saw, whether we want to keep or drop columns, we always use the same function: select. To use this function, we provide our function with a single or “list” (not a programmatical list, just a list) of the columns we want to keep - or if we want to drop them, we use the same approach, but add a - before our selection. (We’ll use this drop approach in a litte bit). Let’s see how we can extract just the relevant columns we will need for our future analysis - note, in this case we’ll overwrite our all_theft_ward_df variable. In your script, add the following code to extract only the relevant columns we need for our future analysis: # Reduce our data frame using the select function all_theft_ward_df &lt;- select(all_theft_ward_df, crime_id, month, longitude, latitude, lsoa_name, lsoa_code, crime_type, WD18CD, WD18NM) You should now see that your all_theft_ward_df data frame should only contain 9 variables - you can go and view this data frame or call the head() function on the data in the console if you’d like to check out this new formatting. Improving efficiency in our code Our current workflow looks good - we now have our data frame ready for use in wrangling… but wait, could we not have made this whole process a little more efficient? Well, yes! There is a quicker way - particularly if I’m not writing out explanations to you to read through - but generally, yes, we coud make our code way more “speedy” by using the pipe function, %&gt;%, introduced above, which for those of you that remember, we used in our work last week. As explained above, a pipe is used to pipe the results of one function/process into another - when “piped”, we do not need to include the first “data frame” (or which data structure you are using) in the next function. The pipe “automates” this and pipes the results of the previous function straigt into this function. It might sound a little confusing at first, but once you start using it, it really can make your code quicker and easier to write and run - and it stops us having to create lots of additional variables to store outputs along the way. It also enabled the code we used last week to load/read all the csvs at once - without the pipe, that code breaks! Let’s have a think through what we’ve just achieved through our code, and how we might want to re-write our code. In our workflow, we have: Joined our two data frames together Remove the columns not needed for our future analysis Let’s see how we can combine this process into a single line of code: Option 1: Original code, added pipe # Join dataframes, then select only relevant columns all_theft_ward_df_speedy_1 &lt;- left_join(all_theft_df, lsoa_ward_lookup, by = c(&quot;lsoa_code&quot; = &quot;LSOA11CD&quot;)) %&gt;% select(crime_id, month, longitude, latitude, lsoa_name, lsoa_code, crime_type, WD18CD, WD18NM) You should see that we now end up with a data frame akin to our final output above - the same number of observations and variables, all from one line of course. We could also take another approach in writing code, by completing our selection prior to our join, which would mean having to write out fewer field names when piping this output into our join. Let’s have a look: Option 2: New code - remove columns first # Select the relevant fields from the lookup table, then join to dataframe all_theft_ward_df_speedy_2 &lt;- select(lsoa_ward_lookup, LSOA11CD, WD18CD, WD18NM) %&gt;% right_join(all_theft_df, by = c( &quot;LSOA11CD&quot; = &quot;lsoa_code&quot;)) You’ll see in this approach, we now have 14 variables instead of the 9 as we haven’t really “cleaned” up the fields from the original all_theft_df - we could drop these fields by piping our output into another select() function, but we may end up creating even more coding work for ourselves this way. What these two options do show is that there are multiple ways to achieve the same output, using similar code - we just need to always think through what outputs we want to use. Pipes help us improve the efficiency of our code - the one thing however to note in our current case is that by adding the pipe, we would not be able to check our join prior to the selection - so sometimes, it’s better to add in this efficiency, once you’re certain that your code has run correctly. For now, **we’ll keep our original all_theft_ward_df data frame that you would have created prior to this info box - but from now on, we’ll use pipes in our code when applicable. Go ahead and remove the speedy variables from your environment: rm(all_theft_ward_df_speedy_1, all_theft_ward_df_speedy_2). We now FINALLY have our dataset for starting our last bit of data wrangling: aggregating our crime by ward for each month in 2020. Aggregate crime by ward and by month To aggregate our crime by ward for each month in 2020, we need to use a combination of dplyr functions. First, we need to group our crime by each ward and then count - by month - the number of thefts occuring in each ward. To do so, we’ll use the group_by() function and the count() function. The group_by() function creates a “grouped” copy of the table (in memory) - then any dplyr function used on this grouped table will manipulate each group separately (i.e. our count by month manipulation) and then combine the results to a single output. If we solely run the group_by() function, we won’t really see this effect on its own - instead we need to add our summarising function -in our case the count() function, which \"counts the number of rows in each group defined by the variables provided within the function, in our case, month. Let’s see this in action: Pipe our grouped table into the count function to return a count of theft per month for each Ward in our all_theft_ward_df data frame: # Group our crimes by ward, then count the number of thefts occuring in each month theft_count_month_ward &lt;- group_by(all_theft_ward_df, WD18CD) %&gt;% count(month) To understand our output, go ahead and View() the variable. We have 3 fields - with 4490 rows. You should see that we’ve ended up with a new table that lists each ward (by the WD18CD column) eleven times, to detail the number of thefts for each month - with the months represented as a single field. But does this table adhere to the Tidyverse principles we read about this and last week? Not really - whilst it is just about usable for a statistical analysis - if we think about joining this data to our ward_population dataset, we are really going to struggle to add each monthly count of crime in this format. What we would really prefer is to have our **crime count detailed as one field for each individual month, i.e. 2020-01 as a single field, then 2020-02, etc. To do this, we need to figure out how to transform our data to present our months as fields - and yes, before you even have a chance to guess it, the Tidyverse DOES have a function for that! Do you see why using the Tidyverse is an excellent choice to our R-Studio learning… ;) This time, we look to the tidyr library which has been written to quite literally: “help to create tidy data, where each column is a variable, each row is an observation, and each cell contains a single value. ‘tidyr’ contains tools for changing the shape (pivoting) and hierarchy (nesting and ‘unnesting’) of a dataset, turning deeply nested lists into rectangular data frames (‘rectangling’), and extracting values out of string columns. It also includes tools for working with missing values (both implicit and explicit).” tidyr documentation And even in our explanation of the tidyr library, we may have found our solution in tools for changing the shape (pivoting). To change the shape of our data, we’re going to need to use tidyr’s pivot functions. Note, do not get confused here with the traditional sense of pivot in data processing in terms of pivot tables. If you’ve never use a pivot table before in a spreadsheet document (or R-Studio for that matter), they are primarily used to summarizes the data of a more extensive table. This summary might include sums, averages, or other statistics, which the pivot table groups together in a meaningful way. In our case, the application of the word pivot is not quite the same - here, our pivot() functions will change just the shape of our data (and not the values). In the tidyr library, we have the choice of two pivot() functions: pivot_longer() or pivot_wider(). pivot_wider() “widens” data, increasing the number of columns and decreasing the number of rows. pivot_longer() “lengthens” data, increasing the number of rows and decreasing the number of columns. Well, our data is already pretty long - and we know we want to create new fields representing our months, so I think we can make a pretty comfortable guess that pivot_wider() is the right choice for us. We just need to first read through the documentation to figure out what parameters we need to use and how. Type ?pivot_wider into the console. You should now see the documentation for the function. We have a long list of parameters we may need to use with the function - but we need to figure out what we need to use to end up with the data frame we’re looking for from our data: If we read through the documentation, we can figure our that our two parameters of interest are the names_from and values_from fields. We use the names_from parameter to set our month column as the column from which to derive ouput fields from, and the values_from field as our n field (count field) to set our values. As we do not have a field that uniquely identifies each of our rows, we can not use the id_cols parameter. We will therefore need to state the parameters in our code to make sure the function reads in our fields for the right parameter. Pivot our data “wider” to create a theft by ward by month data frame: # Read in our lsoa_ward_lookup csv from our raw boundaries data folder theft_by_ward_month_df &lt;- pivot_wider(theft_count_month_ward, names_from = month, values_from = n) Have a look at the resulting data frame - does it look like you expect? Trial and error your code When you come across a new function you’re not quite sure how to use, I can highly recommend just trialling different inputs for your parameters until you get the output right. To do this, just make sure you don’t overwrite any variables until you’re confident the code work. In addition, always make sure to check your output against what you’re expecting. In our case here, we can check our original theft_count_month_ward data frame values against the resulting theft_by_ward_month_df dataframe - for example, do we see 30 thefts in January for ward E05000026? As long as you do, we’re ready to move forward with our processing. One final thing we want to do is clean up the names of our fields to mean a little more to us. Let’s transform our numeric dates to text dates (and change our WD18CD in the process). Rename our field names for our theft_by_ward_month_df data frame: # Read in our lsoa_ward_lookup csv from our raw boundaries data folder names(theft_by_ward_month_df) &lt;- c(&#39;ward_code&#39;, &#39;jan_2020&#39;, &#39;feb_2020&#39;, &#39;mar_2020&#39;, &#39;apr_2020&#39;, &#39;may_2020&#39;, &#39;jun_2020&#39;, &#39;jul_2020&#39;, &#39;aug_2020&#39;, &#39;sept_2020&#39;, &#39;oct_2020&#39;, &#39;nov_2020&#39;) And we’re now done! We have our final data frame to join to our ward_population spatial data frame. Excellent! Let’s just do one final bit of data management and write out this completed theft by ward by month table to a new csv for easy reference/use in the future. Write out completed theft table to a new csv file for future reference: # Write out the theft_crime_df to a csv within our working data folder write.csv(theft_by_ward_month_df, &quot;data/working/theft_by_ward_per_month_2020.csv&quot;, row.names = FALSE) Join our theft data frame to our ward population data frame We’re now getting to the final stages of our data processing - we just need to join our completed theft table, theft_by_ward_month_df to our ward_population spatial data frame and then compute a theft crime rate. This will then allow us to map our theft rates per month by ward - exactly what we wanted to achieve within this practical. Luckily for us, the join approach we used earlier between our all_theft_df and our lsoa_ward_lookup is the exact same approach we need for this, even when dealing with spatial data. Let’s go ahead and use the same left_join function to join our two data frames together - in this case, we want to keep all rows in our ward_population spatial data frame, so this will be our x data frame, whilst the theft_by_ward_month_df will be our y. Join our two data frames together, using our respective ward code fields to join the data: # Join theft by month to the correct wards in our ward_population data frame all_theft_ward_sdf &lt;- left_join(ward_population, theft_by_ward_month_df, by = c(&quot;GSS_CODE&quot; = &quot;ward_code&quot;)) To double-check our join, we want to do one extra step of “quality assurance” - we’re going to check that each of our wards has at least one occurence of crime over the eleven months. We do this by computing a new column that totals the number of thefts over the 11 month period. By identifying any wards that have zero entries (i.e. NAs for each month), we can double-check with our original theft_by_ward_month_df to see if this is the correct “data” for that ward or if there has been an errors in our join. We should actually remember from last week, that only those wards in the City of London (that are to be omitted from the analysis) should have a total of zero. We can compute a new column by using the mutate() function from the dplyr library. We use the rowsums() function from the base library to compute the sum of rows, which we use the across() function from the dplyr library to parse. This code is actually a little complicated - and not wholly straight-foward to identify from reading through dplyr documentation alone. And believe it or not, I do not know every single function available within our various R libraries - so how did I figure this out? Well, just through simple searching - it might take a few attempts to find the right solution, but the great thing about programming is that you can try things out easily and take steps back. You can find the original post where I found this code on Stack Overflow and what you’ll notice is that there is a variety of answers to try - and believe me, I certainly did! Luckily the final answer provided a good solution to what we needed. Summarise all thefts for each ward by computing a new totals column using the mutate() and rowsums() functions: # Total number of thefts for each ward, create new column all_theft_ward_sdf &lt;- all_theft_ward_sdf %&gt;% mutate(theft_total = rowSums(across(8:18), na.rm = T)) You can now View() our updated all_theft_ward_sdf spatial data frame - and sort out columns to see those with a theft_total of 0. What you should see is that we have approximately 20 City of London wards without data, whilst we do indeed have 10 additional wards without data. The question is why? Do we have errors in our processing that we need to investigate? Or do these areas simply have no theft? If we had not complete this analysis in Q-GIS prior to this week’s practical, we would need to conduct a mini-investigation into the original theft dataset and search for these individual wards within the dataset to confirm to ourselves that they are not present within this original dataset. Luckily, having done the practical two weeks before, I can very much confirm that these wards do not have any records of thefts within them. We can therefore move forward with our dataset as it is, but what we’ll need to do is adjust the values present within these wards prior to our visualisation analysis - these should not have “NA” as their value but rather 0. In comparison our City of London wards should only contain “NAs”. To make sure our data is as correct as possible prior to visualisation, we will remove our City of London wards that do not have any data (crime or population), and then convert the NAs in our theft counts to 0. Filter out the City of London wards with a theft count of 0 and then replace the NAs in our theft columns with 0. # Filter out City of London wards with a crime count of 0 or a population of 0 # Note the logic is a little complicated here to achieve the above filter all_theft_ward_sdf &lt;- filter(all_theft_ward_sdf, theft_total &gt; 0 | DISTRICT != &quot;City and County of the City of London&quot;) # We&#39;re also going to remove the ward of Vintry, which whilst it has a positive crime count, it does not contain a population # I only realise this at the end of the practical, therefore it&#39;s added as a single line of code here! all_theft_ward_sdf &lt;- filter(all_theft_ward_sdf, NAME != &quot;Vintry&quot;) # Replace all NAs in our data frame with 0 all_theft_ward_sdf[is.na(all_theft_ward_sdf)] = 0 The final thing we need to do before we can map our theft data is, of course, compute a crime rate per month for our all_theft_ward_sdf data frame. We have our POP2019 column within our all_theft_ward_sdf data frame - we just need to figure out the code that allows us to apply our calculation that we’ve used in our previous practicals (i.e. using the Attribute/Field Calculator in Q-GIS: value/POP2019 * 10000) to each of our datasets. Once again, after a bit of searching, we can find out that the mutate() function comes in handy - and we can follow a specific approach in our code that allows us to apply the above equation to all of our columns within our data frame. Now this is certainly a big jump in terms of complexity of our code - below, we are going to store within our crime_rate variable our own function that calculates crime rate on a given value, currently called x, but will be (through our second line our code) each individual cell within our all_theft_ward_sdf spatial data frame contained within our month columns (using the mutate_at() function). How this code works - for now - is not something you need to worry about too much, but it shows you that a simple task that we completed easily in Q-GIS can, actually, be quite complicated when it comes to writing code. What is great is that you now have this code that you’ll be able to refer to in the future if and when you need it - and you can of course trial and error different calculations to include with the function. For now, let’s get on with calculating our theft crime rate. We’re going to create a new dataframe to store our crime rate as when we apply our calculation to our current data frame, we are actually transforming the original values for each month and not creating a new column per se for each month. Create a new data frame to store the crime rate for each ward for each month. # Create a new function called crime rate, which takes an argument, x, and the following calculation # The calculation to pass x through is equal to ( x / POP2019) * 10 000) crime_rate = function(x, na.rm = FALSE) ((x/all_theft_ward_sdf$POP2019)*10000) # Apply this calculation to all columns between 8 and 18 within the all_theft_ward_sdf and transform the values theft_crime_rate_sdf &lt;- mutate_at(all_theft_ward_sdf, vars(8:18), crime_rate) Have a look at your new theft_crime_rate_sdf spatial data frame - does it look as you would expect? Complexities of coding These last few chunks of code are the most complicated pieces of code we have come across so far in this module - and not something I would expect you to be able to write on your own. And to be honest, neither have I. Much of programming is figuring out what you need to do - trying out different approaches and if you get stuck, searching online for solutions - and then copy and pasting! You then use trial and error to see if these solutions work - and if not, find a new option. What is important is to recognise what inputs and outputs you need for the functions you are using - and starting from there. This is only knowledge you’ll gain from programming more, so do not worry at this stage if this feels a little overwhelming, because it will. Just keep going and you’ll find in six weeks time, you’ll be able to re-read the code above and make a lot more sense out of it! Now we have our final data frame, we can go ahead and make our maps. Making Maps in R-Studio: Grammar of Graphics Phew - we are so nearly there - as we now have our dataset ready, we can start mapping. This week, we’re going to focus on using only one of the two visualisation libraries mentioned in the lecture - and we’ll start with the easiest: tmap. tmap is a library written around thematic map visualisation. The package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. It is also based on the grammar of graphics, and resembles the syntax of ggplot2 and so provides a reasonable introduction into understanding how to make maps in R. What is really great about tmap is that it comes with one quick plotting method for a map called: qtm - it quite literally stands for quick thematic map. We can use this function to plot the theft crime rate for one of our months really really quickly. Let’s create a crime rate map for January 2020. Within your script, use the qtm function to create a map of theft crime rate in London in January 2020. # Use qtm, pass our theft_crime_rate_sdf as our data frame, and then the jan-2020 column as our fill argument qtm(theft_crime_rate_sdf, fill=&quot;jan_2020&quot;) In this case, the fill argument is how we tell tmap to create a choropleth map based on the values in the column we provide it with - if we simply set it to NULL, we would only draw the borders of our polygon (you can try this out in your console if you’d like). Within our qtm function, we can pass quite a few different parameters that would enable us to change specific aesthetics of our map - if you go ahead and search for the function in the Help window, you’ll see a list of these parameters. We can, for example, set the lines of our ward polygons to white by adding the borders parameter. Update our map to contain white borders for our ward polygons: # Use qtm, pass our theft_crime_rate_sdf as our data frame, and then the jan-2020 column as our fill argument # Add the borders parameter and set to white # Note colour based parameters can take words or HEX codes qtm(theft_crime_rate_sdf, fill=&quot;jan_2020&quot;, borders = &quot;white&quot;) Yikes - that doesn’t look great! But at least we tried to change our map a little bit. Setting colours in R Note, when it comes to setting colours within a map or any graphic (using ANY visualisation library), we can either pass through a colour word, a HEX code or a pre-defined palette when it comes to graphs and maps. You can find out more here, which is a great quick reference to just some of the possible colours and palettes you’ll be able to use in R but we’ll look at this in more detail in the second half our module. For now, you can use the options I’ve chosen within my maps - or if you’d like, experiment a little bit and see what works! We can continue to add and change parameters in our qtm function to create a map we are satisfied (we just need to read the documentation to figure out what parameters do what). The issue with the qtm function is that it is extremely limited in its functionality to: Change the classification breaks used within the Fill parameter Add additional data layers, such as an underlying ward polygon layer to show our City of London wards that are missing. Instead, when we want to develop more complex maps using the tmap library, we want to use their main plotting method which uses a function called tm_shape(), which we build on using the ‘layered grammar of graphics’ approach. Using the `tm_shape() function and the “grammar of graphics” The main approach to creating maps in tmap is to use the “grammar of graphics” to build up a map based on what is called the tm_shape() function. Essentially this function, when populated with a spatial data frame, takes the spatial information of our data (including the projection and geometry/“shapes” of our data) and creates a spatial “object”. This object contains some information about our original spatial data frame that we can override (such as the projection) within this function’s parameters, but ultimately, by using this function, you are instructing R that this is the object from which to “draw my shape”. But to actually draw the shape, we next need to add a layer to specify the type of shape we want R to draw from this information - in our case, our polygon data. We need to add a function therefore that tells R to “draw my spatial object as X” and within this “layer”, you can also specific additional information to tell R how to draw your layer. You can then add in additional layers, including other spatial objects (and their related shapes) that you want drawn on your map, plus a specify your layout options through a layout layer. Hence the “layered” approach of making maps mentioned in the lecture. This all sounds a little confusing - and certainly not as straight-forward as using the Print Layout on Q-GIS. However, as with Everything In Programming, the more times you do something, the clearer and more intuitive it becomes. For now, let’s see how we can build up our first map in tmap. Building a map: theft in January 2020 To get started with making a map, we first need to specify the spatial object we want to map. In our case, this is our theft_crime_rate_sdf spatial data frame, so we set this to our tm_shape() function. However, on it’s own, if you try, you’ll see that we have “no layer elements defined after tm_shape”. For the following lines of code, I want you to build on the first line by adding the extra pieces of code I’ve added at each step. DO NOT duplicate the entire code at each step (i.e. copy and paste below one another!). In the end you only want ONE CHUNK of code that plots our map. Set our tm_shape() equal to our theft_crime_rate_sdf spatial data frame. Execute the line of code and see what happens: # Set our tm_shape equal to our spatial data frame tm_shape(theft_crime_rate_sdf) We therefore need to tell R that we want to map this object as polygons. To do so we use the tm_polygons() function and add this function as a layer to our spatial object by using a + sign: Add our + tm_polygons() layer and execute: # Draw our spatial object as polygons tm_shape(theft_crime_rate_sdf) + tm_polygons() As you should now see, we have now mapped the spatial polygons of our theft_crime_rate_sdf spatial data frame - great! A step in the right direction. However, this is not the map we want - we want to have our polygons represented by a choropleth map where the colours reflect the theft crime rate in January, rather than the default grey polygons we see before us. To do so, we use the col= parameter that is within our tm_polygons() shape. The col= parameter is used to “fill” our polygons with a specific fill type, of either: a single color value (e.g. “red”) the name of a data variable that is contained in shp. Either the data variable contains color values, or values (numeric or categorical) that will be depicted by a specific color palette. In the latter case, a choropleth is drawn. “MAP_COLORS”. In this case polygons will be colored such that adjacent polygons do not get the same color. (This information is all extracted from the `tm_polygons() documentation - see how important the Help window is in R!) Let’s go ahead and pass our jan_2020 column within the col= parameter and see what we get. Add col = \"jan_2020\" to our tm_polygons() function as a parameter: # Map our theft_crime_rate_sdf using the Jan 2020 column tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;) Awesome! It looks like we have a choropleth map. We are slowly getting there. But there are two things we can notice straight away that do not look right about our data. The first is that our classification breaks do not really reflect the variation in our dataset - this is because tmap has defaulted to its favourite break type: pretty breaks, whereas, as we know, using an approach such as natural breaks, aka jenks, may reveal better variation in our data. So how do we state our classification breaks in tmap? To figure this out, once again, we need to visit the documentation for tm_polygons() and read through the various parameters to find out what we might need… ..Hmm, if we scroll through the parameters the three that stick out are: n, style, and breaks. It seems like these will help us create the right classfication for our map: n: state the number of classification breaks you want style: state the style of breaks you want, e.g. “cat”, “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, and “log10_pretty”. breaks: state the numeric breaks you want to use when using the fixed style approach. There are some additional parameters in there that we might also want to consider, but for now we’ll focus on these three and specifically the first two today. Let’s say we want to change our choropleth map to have 5 classes, determined via the jenks method - we simply need to add the n and style parameters into our tm_polygons() layer. Add the n and style parameters into our tm_polygons() layer. Note we pass the jenks style as a string. # Map our theft_crime_rate_sdf using Jan 2020 column, with 5 breaks and using the jenks style tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;jenks&quot;) We now have a choropleth that reflects better the distribution of our data - but I’m not that happy with the classification breaks used by the “jenks” approach - they’re not exactly as readable as our pretty breaks. Therefore, I’m going to use these breaks, but round them down a little to get a compromise between my two classification schemes. To do so, I need to change the style of my map to fixed and then supply a new argument from breaks that contains these rounded classification breaks. Change the style parameter and add the breaks parameter: # Map our theft_crime_rate_sdf using the Jan 2020 column, but fixed breaks as defined tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) That looks a little better from the classification side of things. We still have one final “data-related” challenge to solve, before we start to style our map - and that is showing the polygons for City of London wards, even though we have no data for them. We always want to create a map that contains as much information as possible and leave no room in interpretation error - by leaving our CoL wards as a white space within our current map, we are not telling our map readers anything other than there is a mistake in our map that they’ll question! We therefore want to include our CoL wards in our map, but we’ll symbolise them differently so we will be able to explain why we do not have data for the CoL wards when, for example, we’d write up our analysis or present the map on a poster. This explanation is something you’d add into a figure caption or footnotes, for example, depending on how long it needs to be! For now, we want to add these polygons to our map - and the easiest way to do so is to simply add another spatial object to our map that symbolises our polygons as grey (/“gray”, alas US-centric programming here ;) wards. Let’s go ahead and try this out using the “layered” approach of graphic making, where we simply rinse and repeat and add our layer (and spatial object) as another addition to our map. This time, we’ll use our original spatial data frame that we loaded into our script, ward_population, so we do not get confused between our layers. As per out tm_polygons() layer, we simply add this using the + sign. Create a new tm_shape(), equal to our ward_population spatial data frame and draw as grey polygons. # Map our theft_crime_rate_sdf as above and our CoL wards (from ward_population) tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) + tm_shape(ward_population) + tm_polygons(&quot;gray&quot;) Hmmm! Interesting! We have our grey polygons of our ward - and what appears to be the legend for our choropleth - but no choropleth map? What could have gone wrong here? Maybe this is to do with the LAYERED approach - ah! As we have added our new shape-layer, we have simply added this on top of our original map. So it seems how we order our tmap code is really important! As we build our maps, we need to be conscious of the order in which we layer our objects and polygons. Whatever comes first in our code is drawn first, and then the next layer is drawn on top of that and so on! This should be simple fix - and requires just a little rearranging of our code. Re-arrange our code to have our grey CoL wards first, and then our Jan 2020 theft crime rate choropleth map: # Re-order map to display grey polygons under our choropleth map tm_shape(ward_population) + tm_polygons(&quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434)) Ahah, that’s much better! Now we have our data displayed, we want to go ahead and start styling our map. Styling maps using tmap And now things start to get even more complicated… As you’ve seen, getting to the point where we have a choropleth map in R takes a lot of knowledge about how to use the tmap library successfully. Whilst ultimately it is only four functions so far, it is still A LOT to learn and understand to make a good map, compared to, for example, the Q-GIS Print Layout. To style our map takes even more understanding and familiarity with our tmap library - and it is only something you’ll only really learn by having to make your own maps. As a result, I won’t go into explaining exactly every aspect of map styling - but I will provide you with some example code that you can use as well as experiment with to try to see how you can adjust aspects of the map to your preferences. Fundamentally, the key functions to be aware of: tm_layout(): Contains parameters to style titles, fonts, the legend etc tm_compass(): Contains parameters to create and style a North arrow or compass tm_scale_bar(): Contains parameters to create and style a scale bar To be able to start styling our map, we need to interrogate each of these functions and their parameters to trial and error options to ultimately create a map we’re happy with. Here, for example, is a first pass at styling our above map to contain a title, change the colour palette of our map, plus change the position of the legend, add a north arrow and a scale bar - whilst also formatting the font: Example code: feel free to implement/adjust: # Creating a map template for us to use tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = FALSE, legend.position = c(&quot;left&quot;,&quot;top&quot;), legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Well this is starting to look a lot better, I’m still not happy with certain aspects. For example, I think moving the legend outside of the map might look better - plus I’d prefer that the legend also has a different title that is more informatives. Let’s see what small adjustments we can make. Here I’ve added: a title = argument into the tm_polygons() layer for the theft_crime_rate_sdf whilst adding legend.outside = TRUE, legend.outside.position = \"right\" to the tm_layout() layer. Let’s see: More example code - feel free to add and implement: # Creating a map template for us to use, legend outside tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), title=&quot;Theft Crime Rate per 10,000 people&quot;, style=&quot;jenks&quot;, palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) Well I’m pretty happy with that! There’s only a few more things I’d want to do - and that would be to add an additional legend property to state why the City of London wards are grey as well as our data source information. Remember - all our maps contain data from the Ordnance Survey and Office for National Statistics and this needs to be credited as such (I’ve put this for now in our Acknowledgements section of the workshop). This could all be added in an additional text box within our map using the tm_credits() function - but I’m not happy with the display that R creates (feel free to experiment with this if you’d like!). I haven’t quite figured out how to get the tm_credits() box to appear outside the main plotting area! For now, I would add this in post-production or take the my next step in my own R map-making learning curve is to figure out how to make an additional box outside the map area. Let’s see what we get up to in the second half of term! Exporting our final map to a PNG Once we’re finished making our map, we can go ahead and export it to our maps folder. To do so, we need to save our map-making code to a function and then use the tmap_save() function to save the output of this code to a picture within our maps folder. Once you’re happy with your map, export it using the code below. # Store map as a variable jan2020_map &lt;- tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = &quot;jan_2020&quot;, n = 5, style = &quot;fixed&quot;, breaks = c(0, 5, 16, 40, 118, 434), title=&quot;Crime Rate per 10,000 people&quot;, style=&quot;jenks&quot;, palette=&quot;Blues&quot;, border.col=&quot;white&quot;) + tm_layout(main.title=&#39;January 2020&#39;, main.title.fontface = 2, fontfamily = &quot;Helvetica&quot;, legend.outside = TRUE, legend.outside.position = &quot;right&quot;, legend.title.size = 1, legend.title.fontface = 2) + tm_compass(type=&quot;arrow&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_scale_bar(breaks = c(0, 5, 10, 15, 20), position = c(&quot;left&quot;, &quot;bottom&quot;)) ## save an image (&quot;plot&quot; mode) tmap_save(jan2020_map, filename = &quot;maps/jan2020_theft_crime_map.png&quot;) We also want to export the rest of our hard work in terms of data wrangling that we’ve completed for this practical - so let’s go ahead and export our data frames so we can use them in future projects, where during GEOG0030 or beyond. What we’ll do is export both the all_theft_ward_sdf spatial data frame and theft_crime_rate_sdf as shapefiles. This means we’ll have both datasets to use in the future - you can, if you want, also export the all_theft_ward_sdf spatial data frame as a csv if you like. Export our final spatial data frames as shapefiles: # Write out the theft_crime_df to a csv within our raw crime data folder st_write(theft_crime_rate_sdf,&quot;data/working/theft_rate_by_ward_per_month_2020.shp&quot;, row.names = FALSE) # Write out the all_theft_ward_sdf to a shapefile within our working data folder st_write(all_theft_ward_sdf,&quot;data/working/theft_count_by_ward_per_month_2020.shp&quot;, row.names = FALSE) # Write out the all_theft_ward_sdf to a csv within our raw crime data folder write.csv(all_theft_ward_sdf,&quot;data/working/theft_count_by_ward_per_month_2020.csv&quot;, row.names = FALSE) And that’s it - we’ve made it through our entire practical - awesome work and well persevered! You will have learnt a lot going through this practical that we’ll keep putting into action as we move forward in Geocomputation. Therefore, as I always say, do not worry if you didn’t understand everything we’ve covered as we’ll revisit this over the next five weeks - and you’ll of course always have this page to look back on. To consolidate our learnings, I have a small task for you to complete - as I’ve said earlier, I won’t set the mini-project I had planned, but what I would like you to do is complete a small assignment in time for our seminar in Week 6. Assignment: Making maps for another month! For your assignment for this week, what I’d like you to do is to simply make a map for a different month of 2020 and export this to submit within your respective seminar folder. If you navigate to your folder from here, you’ll see I’ve added folders within each seminar for the different maps we’re making within our practicals. What I’d like you to do is check this folder to see what months are already covered within the folder - and then make a map for the month that isn’t yet made! To help, when you export your map, make sure to use the name of the month at the start of your title (i.e. as prescribed above!). You’ll of course see that January 2020 is already taken - by me! But it’d be great to get maps for every single month of the year (until November, or December if the data is now available) within each seminar folder. But what if all the months are now done? Please go ahead and make a duplicate map (not of January, of course!) - the more the merrier, and if you can look into different colour palettes and styling effects, even better! Remember, you’ll need to really think about your classification breaks when you change to map a different map from January as my breaks are based on January’s distribution! We won’t worry about standardising our breaks across our maps for now - just make sure you represent the distribution of your data well! If you have any issues with this, please get in touch! Recap Wow - that’s been a lot to get through, but over the last two weeks, you really have had a crash-course in how to use programming for statistical and spatial analysis. In this week’s workshop, you’ve learnt about why we use programming for spatial anlaysis, including how the four key principles of data science have affected how we “do” spatial analysis. You’ve then had a thorough introduction into how we use R and R-Studio as a GIS - and as we can see through our practical, in comparison to Q-GIS, there is a lot more to learn, as we need to know a lot about programming, particuarly to “wrangle” our data - before we even get to map-making. Furthermore, when it comes to map-making in R, this isn’t even as straight-forward! We need to know all about this “grammar of graphics” and how to layer our data and what parameters do what, which, compared to drawing a few boxes etc. in Q-GIS, is a whole lot more complicated! You can therefore see that Geocomputation really requires a combination of foundational concepts in GIScience, Cartography and Programming in order to understand precisely what you’re doing - and even when you’ve had this foundational introduction, it can still feel overwhelming and a lot to learn - and that’s because it is! I do not expect you to “get this” all at once, but this workbook is here for you to refer to as and when you need to get your “Aha” moments, that you’ll get a) on this course and b) as you, for example, complete your own independent research projects, such as your dissertations. Take this all in good time, and we’ll get there in the end - and I will revisit lots of the concepts we’ve looked at over the last two weeks time and time again! What you should realise however is that once you have this code written - you can just come back to it and copy and paste from your scripts, to use in other scripts, for example, changing variables, data files and, of course, parameter settings. And that’s how you end up building up a) your scripts in the first place but b) your understanding of what this code does! If you’re concerned that need to know and understand every function – I can whole-heartedly say - no, you don’t. It takes time, experimenting and research to learn R. For example, last week I had you clean the names of our crime dataset clean names manually - I found out this week there is a great package called janitor that has a function called clean_names() would do that all for us. We’ll use this in Week 6 for some data cleaning, so we won’t deviate now. Ultimately programming - and increasing your “vocabulary” of packages and functions - is an iterative learning process and only one you’ll build upon by writing more and more scripts! To help with all of this new learning, I recommend only one key reading for now: Key Reading Geocomputation with R (2020) by Robin Lovelace, Jakub Nowosad and Jannes Muenchow, which is found online here. I’d recommend reading through Chapters 1, 2, 3 and 8. We’ll continue to build on everything we’ve learnt over the last five weeks as we move into the second half of the module, where we focus more on spatial analysis techniques. You’ll be probably happy to know we will focus less on programming concepts and more on spatial analysis concepts - and use what we know so far with programming to conduct the spatial analysis. This should mean that our practicals will be a little shorter in terms of reading - and even more active in terms of doing! Extension: Facet Mapping So you’ve got this far and really want more work? Really? Are you serious? Ok, well here we go! (And for those of you that don’t, do not worry, as we’ll be looking at this in more detail at Week 10!) So how cool would it be if we could make a map for all 11 (12) months of data in an instant using code…? Well that’s exactly what faceting is for! According to Lovelace et al (2020): Faceted maps, also referred to as ‘small multiples’, are composed of many maps arranged side-by-side, and sometimes stacked vertically (Meulemans et al. 2017). Facets enable the visualization of how spatial relationships change with respect to another variable, such as time. The changing populations of settlements, for example, can be represented in a faceted map with each panel representing the population at a particular moment in time. The time dimension could be represented via another aesthetic such as color. However, this risks cluttering the map because it will involve multiple overlapping points (cities do not tend to move over time!). Typically all individual facets in a faceted map contain the same geometry data repeated multiple times, once for each column in the attribute data. However, facets can also represent shifting geometries such as the evolution of a point pattern over time. In our case, we want to create facet maps that show our theft rate over the 11 months and to do so, we need to add two bits of code to our original tmap approach. First, in our tm_polygons() shape, we add all our months as a combined vector. + *We make this easy for ourselves by creating a month variable that stores these values from a selection of the names() function on our spatial data frame. Second, we add a tm_facets() function that tells tmap to facet our maps, with a specific number of columns. The code below shows how to create a basic facet map using this code. What I’d like you to do is figure out how to make this facet map more aesthetically pleasing - including changing the location of the legend (or removing it?) as well as altering the colours etc. If you manage to create a facet map you are happy with, please export this and upload it to your relevant seminar folder! # Store our months as a variable month &lt;- names(theft_crime_rate_sdf)[8:18] # Map all our months at once tm_shape(ward_population) + tm_polygons(&quot;gray&quot;, border.col = &quot;gray&quot;) + tm_shape(theft_crime_rate_sdf) + tm_polygons(col = month) + tm_facets( ncol=3) Learning Objectives You should now hopefully be able to: Understand how spatial analysis is being used within data science applications Recognise the differences and uses of GUI GIS software versus CLI GIS software Understand which libraries are required for spatial analysis in R/R-Studio Conduct basic data wrangling in the form of selection and slicing Create a map using the tmap visualisation library Acknowledgements This page is entirely original to GEOG0030. The datasets used in this workshop (and resulting maps): Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). "],["analysing-spatial-patterns-i-spatial-auto-correlation-regression.html", "6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression", " 6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Content for this week will be released at 10am on the 23rd February 2021. "],["analysing-spatial-patterns-ii-clusters.html", "7 Analysing Spatial Patterns II: Clusters", " 7 Analysing Spatial Patterns II: Clusters Content for this week will be released at 10am on the 2nd March 2021. "],["rasters-zonal-statistics-and-interpolation.html", "8 Rasters, Zonal Statistics and Interpolation", " 8 Rasters, Zonal Statistics and Interpolation Content for this week will be released at 10am on the 9th March 2021. "],["geodemographics.html", "9 Geodemographics", " 9 Geodemographics Content for this week will be released at 10am on the 16th March 2021. "],["optional-road-network-analysis.html", "OPTIONAL: Road Network Analysis", " OPTIONAL: Road Network Analysis Content for this week will be released at 10am on the 16th March 2021. "],["cartography-and-visualisation-ii.html", "10 Cartography and Visualisation II", " 10 Cartography and Visualisation II Content for this week will be released at 10am on the 23rd March 2021. "],["assessment-information.html", "Assessment Information Useful additional resources", " Assessment Information Geocomputation is assessed through two separate Assessments: Social Atlas: The first assessment will involve the completion of a spatial analysis project, based on the theory, concepts and application learnt during the module. For this coursework you are required to create a small “social atlas” on a topic or area that interests you. Exam: The second assessment will take the form of an Exam, the exact format to be confirmed. More information on your Assessments will be provided at the end of Week 5 (i.e. 12th February 2021). Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out that may be useful for your first Assessment: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 "],["extra-resources-for-help-with-quantitative-dissertations.html", "Extra resources for help with Quantitative Dissertations", " Extra resources for help with Quantitative Dissertations This page is provided for those on the Geography UG degree programme, who are looking to follow a quantitative approach to their dissertation. The page provides general guidance on how to think through preparing your Dissertation outline as well as a list of links to either data portals or potential datasets that are openly available and therefore may be of interest for your dissertations. The list is not exhaustive nor do you need to use data from this list - it is simply provided as a resource. This is applicable to the below videos - these are not part of your Practice Of Geography module, but simply additional content produced to help you get into the right mindset when it comes to developing your dissertation outline and proposal. Video Guidance on Quantitative Dissertations The following videos are provided simply as general guidance to help with completing your Dissertation Outline / Proposal. They are not mandatory for you to watch, nor are they exhaustive of everything you should be considering in your outline/proposal write-up. Thinking Through Your Dissertation Outline This video provides general guidance to help think through your dissertation outline. I also recommend watching it prior to scheduling a meeting with a member of staff as these are the questions they are likely to ask you, so please come prepared. Common Quantitative Approaches in Geographical Research This video outlines the three approaches you can take in quantitative research currently. However for an Undergraduate Dissertation I only recommend two out of the three approaches: Common Mistakes in Quantitative Dissertations This video lists common mistakes made when you start working on your dissertation. To be updated Fri 5th Feb Dataset Guidance Openly Available Datasets From the CASA0005 repository. This is by no means an extensive data list, but summarises data used within some of the practicals alongside a few additions that you might want to explore when sourcing data for your dissertation. You are not limited to these datasets for your dissertation. Google dataset search Tesco store data (London) NHS data (ready for R) US City Open Data Census nomis ONS geoportal UK data service ONS Edina (e.g. OS mastermap) Open Topography USGS Earth Explorer Geofabrik (OSM data) Global weather data (points) London data store Air b n b data NASA SocioEconomic Data and Applications Center (SEDAC) UN environmental data explorer World pop World pop github DIVA-GIS DEFRA US Cesus data TFL open data TFL cycling data EU tourism data NASA EARTHDATA Camden air action Kings data on air pollution Uber travel time data Eurostat London Tube PM2.5 levels Bike docking data in an R package UK COVID data R package for COVID data Tidy Tuesday data (although look for spatial data) Correct statistical tests Data from the CDRC UG students can apply to CDRC for some of their Safeguarded data. There is a process to access these datasets, detailed on CDRC website here. To access any CDRC safeguarded data, you will need to follow this process. It normally takes 4-5 weeks for your application to be granted. As part of the process, you will need to say in your application why you want that specific dataset and what you are going to do with it. You will also need to have at least thought about the ethical implications of using that data and provide this with your data application (alongside your standard ethics application). In terms of specific datasets avaiable, you can apply for: Bicycle Sharing System Docking Station Observations CDRC Modelled Ethnicity Proportions - LSOA Geography NHS Hospital Admission Rates by Ethnic Group and other Characteristics Local Data Company - SmartStreetSensor Footfall Data – Research Aggregated data Speedchecker Broadband Internet Speed Tests FCA Financial Lives Survey - currently the 2017 survey, the 2020 survey may be available around May. There is also a substantial amount of open data available via the CDRC. In this case, you can just register on the site and download. This includes the CDRC Residential Mobility Index, a population ‘churn’ dataset, which has recently been reclassified from Safeguarded to Open. Other Data Lists Awesome public datasets have a wide range all data (some geographic, some not). Robin Wilson has authored one of the most extensive data lists that I’ve come across. "],["week-2-practical-alternate-using-agol-for-population-mapping.html", "Week 2 Practical Alternate: Using AGOL for Population Mapping", " Week 2 Practical Alternate: Using AGOL for Population Mapping For this week’s Practical Alternate, we’ll be using ArcGIS Online.The instructions below outline how to complete the same processing as the Q-GIS practical conducts. It is also includes the all extra information included in the Q-GIS tutorial about Attribute Joins and Classification Schemes. One thing I would recommend is to watch the two videos within the practical: a short introduction to Q-GIS and an introduction to Attribute Tables and Properties. These are not included within this practical. A short introduction to ArcGIS Online Feel free to skip this part and head straight to the Sign Up to ArcGIS Online section. What is ArcGIS Online? ArcGIS Online (AGO) is Esri’s “Software-as-a-service” GIS offering, that enables you to conduct some basic (as well as some quite advanced!) spatial analysis, as well as create interactive maps for sharing with others. It has some very similar features to Esri’s GIS Desktop software (ArcMap and ArcPro) discussed in last week’s lecture, but it does not have all of their capabilities, for example, it is not a tool I would use to create paper maps/ones for use in publication. In contrast, it does offer a lot of web interactivity, as we’ll see when we share our maps with one another at the end of the practical. It also has some really useful analysis tools that are quick and easy to use, in compared to their counterparts in the Desktop software, such as creating something called “drive-time” or “network” buffers – we’ll have a look at these next week when looking at spatial properties. The Esri Ecosystem AGO is just one of the may additional tools Esri offers. Their entire ecosystem of products is huge - you can see their list of products here. Whilst many of the products and/or extensions are created for specific industries and purposes, there are other web-based tools that I can recommend you looking into during your time on this course, to at least be aware of the capabilities moving forward. The first would be ArcGIS StoryMaps, where you can create a webpage a bit like the ones you are using for these workshops, but also integrate any maps you make within the page as well! In addition to StoryMaps, Esri has its own survey collector application – ArcGIS Survey123. Within this application, you can create online forms to collect spatial and non-spatial data – which can then be directly used as inputs within AGO or StoryMap applications. You might see why I call this an “ecosystem” – Esri have constructed their software, tools and applications to work well together and sync across their respective platforms (e.g. web, desktop and mobile)! You just need to be able to afford the license to use them in the first place – we have an educational license which enables ArcMap usage, whilst Esri (as you’ll see) offers AGO for free for non-commercial purposes. Using ArcGIS Online – limitations to be aware of! AGO is a very useful solution to conducting GIS and spatial analysis within the Esri ecosystem when you, as an analyst, are in a scenario where computing resources may be restrictive (and therefore downloading Q-GIS, or Esri’s ArcMap or ArcPro is not a good idea) but internet access is ok – or if, for example, you own a Mac and do not want to split your hard drive to install a Windows operating system, or, finally, when Virtual Machine alternatives may not meet your needs. One thing to flag before we get started with AGO though, is that the platform does simplify some aspects of the traditional GIS workflow – for example, defining your Coordinate Reference Systems and Projection Systems (CRS/PS). This will be an issue in next week’s practical - but I will address this in more detail then. Another aspect of using AGO instead of Desktop software is that your data is ultimately hosted on the AGO server, rather than on your hard drive. One critical aspect of GIS is to practice good file management, including establishing a good use of folders and data storage protocols, so you know where to access your data and where your outputs from any analysis are stored. Normally in Desktop GIS, or even in R-Studio, you would establish a project folder, and within this folder create folders for your data, scripts and outputs (e.g. maps, figures). With ArcGIS specifically, you can use geodatabases to store any spatial data you use or create, whilst R-Studio can create a project in which your work will be saved. QGIS in comparison will rely primarily on your use of folders. For AGO, your data and layers will be managed in their server, under your content page - so in a way you still need to organise your files somewhat. Sign Up to ArcGIS Online With all of this in mind, let’s get ourselves set up to continue with the practical! First head to: https://learn.arcgis.com/en/become-a-member/ and fill in your details as below: By signing up here, you will become part of the Learn ArcGIS organisation, which Esri has created to help support teaching of GIS online for non-commercial purposes, i.e. what we’re doing here! Once you’ve clicked on Join, you’ll need to go authorise your account from your UCL email. The sign-up box may not disappear (it did not for me), but check your emails first before clicking on Join again! Once you’ve authorised your account, you’ll be taken to the ArcGIS online home screen – feel free to navigate around the website yourself before starting the practical. Practical Instructions Open your ArcGIS Online (AGO) home webpage and click on the Map tab. This is the main interface we will use to import and analyse our data and is a light version of a traditional Desktop GUI-GIS. Save your map as Population Change in London. You can add as many tags as you like – I used: population | London | analysis. Let’s go ahead and start adding data to our map. Click on the Add button and select Add Layer from File: 3. You should then see the following pop-up: As you can see from the instructions, AGO requires the shapefile to be provided in the zipfile format, rather than the individual files. As a result, what we need to do is navigate to our raw data folder and compress our wardLondon_ward` shapefile to create a zipped version. For now, close down the pop-up. Navigate to your boundaries folder in your file management system and then to 2011 folder. Select all files related to the London_ward shapefile and right-click and select compress or archive (depending on your Operating System): 6. Back in AGO, click back on the Add button and select Add Layer from File. Navigate to your London_ward zipfile and select this as your file to import. + Click the option to ‘Keep original features’ and then import the layer. You should see the data appear on your map as such: The data is current styled according to the different names in our Name field. Before we go ahead and import our population data, let’s first change the symbolisation of our dataset to only a Single Symbol. In the Change Style option appearing on the left of the screen, select the option to show the Location (Single symbol). All of your wards should now be displayed in a single colour – but we would prefer to see them as simple grey polygons with a black outline. Click on the Options button hovering over the Single Location box that should have appeared. Click on the blue Symbols button – for FILL, select a light grey colour, for OUTLINE, choose a colour of your choice and make sure to reduce the transparency of your lines. Once you are happy with your symbolisation, click through (i.e. click the OKs and DONEs) until you are presented with the main screen. You should now see your Ward data in the main map canvas of AGO - you should also see what looks like a table of contents on the left-hand side which now contains the layers for the London_Ward data and the base map. If you hover over the layer, you’ll see the various options we have through AGO to interact with our dataset. These options include: Displaying the legend (i.e. how the data is symbolised) Show table (i.e. displaying the Attribute Table) Change style (i.e. return to the Symbology options you were just using) Perform analysis (i.e. what we’ll use to perform different types of analysis on our layers, including our attribute join) More options (i.e. other tools you might want to use, such as zooming to a layer or saving your content) We’ll utilise a few of these options over the practical. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using AGO right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 ward shapefile to create our Ward Population dataset. To do this, we need to add the 2011 population data to our map. In AGO, import the 2011 population csv from your working folder by using the Add data button as before. Note for csvs, the population data can be imported as the original file and there is no need to zip it. For the csvs, add the layer just as a table. Now we have it loaded on our map, we can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. Use the Show Table option to open the Attribute Tables for both the Ward and Population data layers. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. To create an Attribute Join in AGO, you need to click on the Perform Analysis button when hovering over the London_Ward dataset and then open the Summarise Data drop-down to find the Join Features tool. Click on the Join Features tool and add the appropriate inputs for each step (again make sure you get your target and join layer and their respective fields correct and also select to keep all target features): 6. Click Run Analysis! AGO will then return to the original layer screen and create your new layer! It might take a little time for AGO to create this join - just be patient. But, if, after ten minutes, your join has still not worked, you may download the complete 2011 ward population dataset here. It is provided as a zipfile, which you’ll then need to add/upload to your AGO map. Once AGO has finished processing, the next thing we would like to do with this dataset is to style it by our newly added Population field to show population distribution around London. Hover over your new layer, and then click the Symbology button. Next in Choose an attribute to show choose our POP2011 (population) field. AGO will automatically style your data for you as Counts and Amounts (Size), which is a useful way to view our dataset. This approach is also known as Proportional Symbols. We can see from just this initial styling that there are some differences in population size across our wards. You can click on the Options button to find more ways of altering how your data is currently styled. This also provides you with a histogram of the data (even though it is on its side!) to see how our data is distributed. As we can see, our dataset shows a normal Gaussian distribution. Understanding our data’s distribution is really important when it comes to thinking about how to style and visualise our data as well as understanding what sort of analysis techniques we can apply to our data – more on this next week. Alternatively to the Size option, you can also create a choropleth map from our dataset. Navigate back to the Change Style menu of the Symbology tab (this may involve clicking done to exit the previous menus). Click on Counts and Amounts (Color) – you’ll see the map change automatically to a choropleth map. We can change the colour scheme of our map, as well as the way in which AGO displays the data either via the Theme dropdown or by clicking the Classify Data box. The latter provides you with more control over the data’s classification scheme and details the different types of classification schemes you can use with your data: We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on Natural Breaks and change it to 7 classes. You may also want to reduce the transparency. Then click OK and then DONE. :::note A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using AGO, which makes it a good playground to see how certain parameters or settings can change your output. ::: You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Empty wards in the City of London In our Q-GIS tutorial, we would now go through the steps to exporting the data. When using AGO, we do not need to worry about this at the moment - make sure you save your map, and if you would like you can save your final Layer to your AGO content. To do this: Click on the More Options button when hovering your item and select Save Layer. Name your layer London_Ward_Population_2011 and add a few tags. Click create item. This layer should then appear in your AGO content. When looking at your layer in the AGO Content page (not the Map page we have been using), if you publish your layer, you will created a hosted layer than you can then download as a Shapefile for use within Desktop software etc from the AGO website. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Zip/compress the respective Ward dataset prior to adding it to AGO Add the respective Ward dataset as the zipped file Load the respective Population csv Join the two datasets together using the Join Features tool. Style your data appropriately. Save your joined dataset as a layer within your AGO content. To then make accurate visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with AGO, so for now, if you want, you can leave your symbolisation to the default settings. Alternatively, if you would like to standardise your classification breaks, you’ll need to return to the Classify Data option within the Symbology tab and manually change your breaks here. If you have any issues with AGO and joining your datasets (i.e. the processing takes longer than 10 minutes each), you can download the remaining pre-joined files here. You will need to download, then upload these datasets to style them appropriately for the next step. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint from AGO: Click on Print –&gt; Map with Legend and either take a screenshot or use the File –&gt; Export as PDF and then trim your PDF to the map. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) ArcGIS Online, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. "],["week-3-practical-alternate-using-agol-for-crime-mapping.html", "Week 3 Practical Alternate: Using AGOL for Crime Mapping", " Week 3 Practical Alternate: Using AGOL for Crime Mapping For this week’s alternate practical, we will continue to use AGOL as our main GIS system to process and analyse our data, similar to the Q-GIS practical. This week, compared to last, we will however be dealing with two main ‘compromises’ in our use of AGOL vs. Q-GIS that you will need to be aware of: 1. Projections Within Q-GIS, as you will see if you read through the main practical (which I highly advised doing), setting the Project and Data CRSs are an essential step in successfully analysing and visualisng spatial data correctly. In our case, our practical data uses two CRS - BNG for the administrative boundaries and WGS84 for the crime data. As a result, in the main practical, we use a tool within Q-GIS to reproject our crime data into the same CRS as the administrative boundaries, i.e. BNG. AGOL, in comparision, uses WGS84/Mercator as default CRS for all its data mapping and visualisation - and can only be altered if you change the basemap to a dataset that is in your desired CRS/PS (although in their Beta version, it appears that there will be more user choice over choosing projections). AGOL will convert our data, such as our Administrative Boundaries (which are in British National Grid) “on the fly” to WGS84 - so we do not need to reproject it. However, this will mean we may forgot this step in the future - for example, when using R-Studio instead; therefore it is important to recognise that this aspect of our GIS workflow is missed in this tutorial. 2. Map-Making &amp; Visualisation AGOL also has relatively limited capacity for map-making. As a result, for this practical, I would recommend using a mixture of your output from AGOL alongside either a graphic software or even PowerPoint to make final additions that are needed to your map. You’ll see these recommendations below as my proposed workaround. Detailed cartography is one of the key advantages that Q-GIS and ArcGIS have over the use of programming tools, such as R-Studio. As you’ll see in future practicals, we can still make excellent maps in R-Studio, it just takes a little more time and experience than the “speed” of the traditional GIS software. With all that being said, we still have plenty of data analysis to learn - so let’s get started! Practical Instructions We now have our datasets downloaded and ready to process - we simply need to get them loaded onto our AGO map. Open your ArcGIS Online (AGO) home webpage and click on the Map tab. Save your map as Crime Analysis in London. You can add as many tags as you like – I used: crime | London | analysis. Let’s go ahead and start adding data to our map. Ward Population We already have our ward_population_2019.shp dataset complete from last week, so we can go ahead and add this directly to the map. Click on the Add button and select Add Layer from File: Add your `ward_population_2019.shp’ to the map using the Add -&gt; Add Layer From File tool. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Borough Population We, as yet, do not have a borough_population_2019.shp. To create our Borough population shapefile, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Add the London_Borough_Excluding_MHW.shp file from the 2011 boundary data (in your raw data folder) to your map. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Add the borough_population_2019.csv you have just created from your working folder to your map. This can just be added as a csv, but remember to add just as a table. Join the two datasets together using the Join Features tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the London_Borough dataset. Crime Data We now are ready to load and map our crime data. We will now add our all_theft_2020.csv from our raw folder - we will load this exactly like our previous population csv but this time, when presented with the option, we need to add the point coordinates to map our crime data as points. Before we can load our data, we actually need to do one final step of data cleaning (compared to the Q-GIS tutorial). Unfortunately AGOL cannot handle all of the data from 2020 - so we need to reduce the size of our dataset. For now, we will analyse theft crime for March in 2020. Open your all_theft_2020.csv from our raw folder in your number editing software, and extract all rows that the field Month is equal to 2020-03. I do not mind how you do this, but just make sure to save to a new CSV called: march_theft_2020.csv into your working folder. Once you have extracted this smaller dataset: Click on Add -&gt; Add Layer From File. AGOL should automatically detect the Longitude and Latitude columns and map your data. You may have an error message, but you can ignore this for now. Unlike Q-GIS, we do not need to reproject our data when using AGOL as the software has done this for us - we can, as a result, move on to the next step - counting the number of crimes in each of our Wards and Boroughs respectively. Counting Points-in-Polygons with AGOL The next step of our analysis is incrediby simple - as AGOL has an in-built tool for us to use. We will use the Aggregate Points tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the March_theft_2020 point dataset to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Hover over the March_theft_2020 point dataset and click the Perform Analysis button. Next, click on the Summarise Data option and then Aggregate Points. Set up your query as follows: Point Layer: march_theft_2020 Aggregation areas: borough_population Add Statistics: Field = UID | Statistic = Sum Result Layer Name: borough_march_theft No need to add anything to Option 4 (group by) Click Run Analysis Note, the processing for the borough level will take around 5 minutes to process. Once complete, re-run the same process for the Ward scale - note this will take even longer to process (approximately 10 minutes). Calculating Crime Rate in AGOL Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. We therefore now want to add a Crime Rate statistic to our dataset - we want to normalise our crime count by our population data. Note, if your processing did not work OR is still processing after 10 minutes, you can find two pre-processed Ward and Borough shapefiles with population and crime count here. Let’s go ahead and calculate our Crime Rate statistic. To do this in AGOL, we actually need to access the Symbology menu. Click on the Change Style / Symbology button whilst hovering over your borough dataset that now contains your crime count. In 1: Choose Attribute, click on the drop-down next to the currently selected attribute, scroll to the bottom of the list and click on New Expression: A new pop-up window should appear - this is where we’ll add a new expression to calculate our crime rate. Edit the Custom name to crime_rate. In the expression box, remove the current comments. Add the expression: ($feature.crimecount/$feature.POP2019)*10000 You can double-click on the fields on the right of the box to add these if you want. Click on OK. You’ll now have a new field populated with the crime rate for each borough. Whilst you’re still in the Style tab, go ahead and change the styling to show the crime rate for each borough by creating a choropleth map: Click on Counts and Amounts (Colour) to access the correct style option. You can click on the Classify check box to change the type of classification scheme and the number of classes. Once you’re happy with your styling, click through the OKs and Dones to return to the main AGO map. Now you just need to repeat the above steps for your Ward crime data and we’ll have our maps ready to export. Remember to uncheck the box next to your borough layer, so this data does not show through on your ward map (and make sure you ward map has not shown through on your borough layer for that matter!). As an FYI, we won’t export our data from AGOL as I’ll provide you with the final shapefiles in Week 5 for the practical that week. Just remember to save your map once you’ve exported your maps as instructed below. Making our Crime Rate Maps for analysis in AGOL As stated at the top of this practical, AGOL does not have a huge amount of flexibility when it comes to cartography - so we’ll need to get a bit inventive. To create maps to submit for your assignment, these are the steps I recommend: Click on Print -&gt; Map with Legend and either take a screenshot or use the File -&gt; Export as PDF. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w3. To add the various map components, open up PowerPoint - preferably find a slide size that is wider than it is taller. Insert your two maps onto your slide, placing them side by side. Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We will use PowerPoints text box and shape features to replicate this on our slide. We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on PowerPoint: Add a north arrow: choose an arrow from PPTs shapes and draw it pointing upwards (i.e. north on your map) Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Feel free to customise your font etc. to give the final maps a good aesthetic. Once you have added these properties in, you should have something that looks a little like this: You’ll notice I got a bit creative with cropping my maps in various ways to try to create a similar format to the one I made in the Q-GIS tutorial. The only thing I haven’t managed to add is a scale bar as this would require accuracy in digitising that we do not have in PPT. This is as close as we can get with creating maps using AGO - it would be great if we could edit it further but this is the constraint with using an online tool. Export map Now we have our maps put together, we are finally ready to export our map! Export your slide as a PNG. Remember to also save your original slide. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in AGOL. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. In AGOL, you may find an option that does this for us. If not, you can use the New Expression builder to calculate these values. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "]]
