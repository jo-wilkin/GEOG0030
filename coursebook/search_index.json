[["index.html", "Geocomputation 2020-2021 Work Book Module Introduction Learning Objectives", " Geocomputation 2020-2021 Work Book Module Introduction Welcome to this year’s Geocomputation module, a course that introduces you to both the principles of spatial analysis and the use of programming for data analysis. Over the next ten weeks, you’ll learn about the theory, methods and tools of spatial analysis whilst implementing small research projects, first using Q-GIS, and then using the R programming language within the R-Studio software environment. You’ll learn how to find, manage and clean spatial, demographic and socio-economic datasets, and then analyse them using core spatial and statistical analysis techniques. The course is an excellent precursor for those of you interested in a career in (spatial) data science! The course will consist of approximately 10 lectures, 10 self-led practicals, 5 seminars (held online) and 5 coding help sessions (held online), further details of which are provided on the next page, Module Information. For now, if you’ve not watched the Introduction video on Moodle, you can catch up below: Geocomputation Introductory Video Remember you must have joined our Geocomputation Team on Microsoft Teams to be able to watch our lecture videos - instructions are provided on Moodle. Learning Objectives As you’ll have read in the Module Catalogue entry, the main learning objectives for the module are as follows: Understand the ways in which digital representations of the observable world are created, and how representations of neighbourhood communities are built from publicly available Open Data. Gain practical experience of the use of analytical methods to profile small areas of London. Understand the nature of geographic data, and the concepts of spatial autocorrelation, modifiable areal units and neighbourhood classification. Understand the sources and operation of uncertainties in the creation of geographic representations, and the importance of generalisation, abstraction and metadata. Gain practical experience of software, map design and visual communication. Develop practical skills in data acquisition and analytics, which may be useful in the planning of dissertations. We hope that you’ll learn many other things during the module and it inspires you to think about how you might use spatial analysis, GIS and programming in your future career! Getting in touch during the module The module is convened and taught by Dr Joanna Wilkin - you can contact her at j.wilkin [at] ucl.ac.uk or, for online office hours, you can book a half hour slot using MS Bookings. The module is further supported by two Postgraduate Teaching Assistants: Jakub (Kuba) Wyszomierski and Nikki Tanu. They will host coding help sessions on the alternative weeks to our scheduled seminar sessions. Acknowledgements Putting together a workbook such as this is no easy feat - but it’s also something that after a little time with R-Studio, you’d be able to produce! The reason for this, as we’ll repeat throughout the course, is that there is an incredible amount of resources available online that can help you learn the skills required to produce a website like this (e.g. using Git with R, using GitHub to host websites, R-Markdown, basic CSS styling). These skills firmly fall outside of the requirements for this course, but something you can build on in your spare time and over your future career. Believe it or not, we as lecturers are also always still learning - particularly, as you’ll find if you continue in spatial data science, the tools and technology available to us is continuously changing! Content-wise, the lectures and practicals for this course are all original this year to the Geography Department at UCL. There is some overlap between this course and the Principles of Spatial Analysis module that is run at the Master’s level, e.g. the extensions offered in several of the practicals. Aesthetics-wise, the R package and analysis artwork used within this book has been produced by allison_horst, whilst much of the artwork used in information boxes has been produced by Desirée De Leon, as well as by Jo. You can find Allison’s images on the stats illustration GitHub repository and Desirée’s on the rstudio4edu GitHub repository. Yihui Xie’s Authoring Books with R Markdown and rstudio4edu’s book, A Handbook for Teaching and Learning with R and RStudio were key resources in the creation and editing of this book. In addition, the CASA0005 practical handbook (by Dr Andy MacLachan and Adam Dennett) alongside our own Principles of Spatial Analysis practical handbook (by myself and Justin van Dijk) served as inspiration for the structure and formatting of this book. For some practicals, additional acknowledgements are made at the end where code or inspiration has also been borrowed! Noticed a mistake in this resource? Please let us know through the GitHub issues tab, send us a message over MS Teams, or contact us by e-mail. "],["module-information.html", "Module Information Self-guided learning for Geocomputation Reading List Troubleshooting Module Content Feedback: Weekly and End of Module", " Module Information If you’ll read one page of this entire workbook in depth, please make sure it is this one!. The following page outlines exactly how we hope you will engage with online learning for this course. Running a practical-based course online is not easy - for both lecturers and students alike! To help, we’ve tried to break down our content into short chunks, using a mix of recorded lecture videos, recorded practical and coding videos, as well as recommended reading and even short explanations of our own. Self-guided learning for Geocomputation The majority of your learning this year for Geocomputation will be ‘self-guided’ - however this is not to say, you’ll be learning alone. We’ll be running fortnightly seminars to check-in on your progress and discuss what you’ve learnt in our small groups (attendance is recorded), whilst also encouraging you to attend (optional) Help/ Study Group sessions on the weeks the seminars are not held. In addition, you’ll have small Assignments to complete, that we’re (time-permitted) hoping to provide you with small feedback on, either during the seminars or through discussion forums (held either on Teams or Moodle, depending on requirements, instructions will be provided as and when). Geocomputation Timetable A typical fortnight for Geocomputation will look something like this: Tuesday 10am: New content is released. Friday 5pm: Post/Send Assignment submissions for online feedback (Optional, but recommended) Monday/Tuesday (allocated slots): Help/Study Group, run by Jakub and Nikki - get help on your practical work if you’ve been unable to submit your work, bring articles you’ve read for discussion, discuss your feedback, and catch-up with friends! Tuesday 10am: New content is released. Friday 5pm: Post/send assignment submissions for seminar (if required) Monday/Tuesday (allocated slots): Seminar, run by Jo - discuss content for the last two weeks, bring articles you’ve read for discussion, discuss your feedback, and ask questions! And repeat! There will of course be a break for Reading Week where we will set you a short Coding Challenge to complete ready for the seminar at the start of the second half of term. In addition to our scheduled fortnightly help sessions, there will also be the weekly Coding Therapy classes run by PhD students within the Department to help you with coding issues if you get stuck and can’t wait for our help sessions. Please do use these classes in addition to our Help Sessions - they are there to help you with any module that requires any type of programming, not just Geocomputation, as well as your Dissertation! We will also provide some additional help sessions in the first two weeks of the Easter break to help you with your first Assessment. The hours for this are TBC. What will I be learning each week? As you’ll soon find out, you will learn a lot of different things in Geocomputation - from spatial analysis techniques to understanding how to write code to process and analyse data, as well as how to organise your investigation and use statistical and spatial analysis to answer research questions. To help, we have broken the course into three main sections: Foundational Concepts Core Spatial Analysis Advanced Spatial Analysis We hope this helps with the various learning curves you’re about to embark on - as outlined and explained further in Week 1’s content. The topics covered over the next ten weeks are: Week Date Section Topic Online Session 1 11/01/2021 Foundational Concepts Geocomputation: An Introduction N/A 2 18/01/2021 Foundational Concepts GIScience and GIS software Seminar 3 25/01/2021 Foundational Concepts Cartography and Visualisation I Help/Study Groups 4 01/02/2021 Foundational Concepts Programming (for Statistical Analysis) Seminar 5 08/02/2021 Foundational Concepts Programming for Spatial Analysis &amp; ESDA Help/Study Groups READING WEEK READING WEEK READING WEEK CODING CHALLENGE - - 6 22/02/2021 Core Spatial Analysis Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Seminar 7 01/03/2021 Core Spatial Analysis Analysing Spatial Patterns II: Clusters Help/Study Groups 8 08/03/2021 Core Spatial Analysis Rasters, Zonal Statistics and Interpolation Seminar 9 15/03/2021 Advanced Spatial Analysis Geodemographics Help/Study Groups 10 22/03/2021 Cartography &amp; Visualisation Cartography and Visualisation II Seminar The lectures and practicals of this course only form a part of the learning process. You are expected to undertake wider reading (see the Reading List below), particularly to help with your second assessment. In addition to our course, there are many other online resources and tutorials that can help expand on the topics and content we cover - whilst it is not necessary for your assessments, you are encouraged to go beyond our recommendations and fully engage with applied GIS research, methods and visualisation techniques. Following certain ‘movers and shakers’ in the GIScience / Spatial Data Science world is one approach to learn more about what’s happening in the field and might prove inspirational for your dissertation ideas later this year. Reading List We link to books and resources throughout each practical. The full reading list for the course is provided on the UCL library reading list page for the course. Alternatively, you can always easily find the link to the Reading List in the top right of any Moodle page for our module, under “Library Resources”. This Reading List will be updated on a weekly basis, in preparation for the week to come, so you may see some weeks without reading for now. But please check back at the start of each week as the lecture, seminar and/or workshop material is released for that week to check for new readings. All reading for that week will be provided by the time your learning materials are released - so you will not need to check the reading list for updates as the week progresses. Troubleshooting Module Content Spatial analysis can yield fascinating insights into geographical relationships. However, at times it can be difficult to work with - particularly when we combine this with learning how to program at the same time. You will get lots of error messages and have software crash, you’ll end up with bugs in your code that are difficult to find, and you may spend a whole day trying to track down a single dataset. But the rewards of learning how to do all of this (particularly with this year’s emphasis on this online research for your dissertations) will become apparent. To bring in my first of potentially several cycling references, a well-known quote from Tour De France (1986, 1989, 1990) winner Greg Lemond: “It never gets easier, you just go faster.” Resonates quite well with spatial analysis and programming! Even after years of programming, we can still forget the syntax to a for loop or question what kernel density estimation actually shows, but you will - by the end of the next ten weeks - know how to find out the answers to these problems faster! Beyond the help sessions mentioned above, if you need specific assistance with this course please: Attend the fortnightly GIS/coding help sessions (from Week 3) to ask questions directly to Jakub or Nikki. Post in the respective tech-help or r-help channels within the Geocomputation Team. Ask a question at the end of a seminar (time-permitting) Check the Moodle assessment tab for queries relating to the assessment (more information will be provided in Week 5) Attend the Coding Therapy sessions that are run on a weekly basis Due to the size of the class we will only reply to tech and R help messages on Teams so all students can see the discussion. If you have a personal matter in relation to completing the course then please speak to or email Jo. We’d also encourage you to monitor the tech-help or r-help channels and contribute to/answer questions as/if you can! Creating a small community across our course will help all of us in the long-run. If after pursuing all these avenues you still need help, you can book into our office hours. These meetings are to discuss a geographical concept in relation to the material/assessment or for any personal matters relevant to the completion of the module. Additional Online Help &amp; Resources We are here to help you work through these practicals but even we do not know everything. Therefore, it’s a good idea to become familar with online sources of help, such as: Stack Exchange RStudio community QGIS documemtation R documentation ArcGIS help pages Ultimately, if you are struggling to use R don’t worry…here is some advice from a tweet and interview with Hadley Wickham, chief scientist at RStudio… You're doing it right if you get frustrated: if you're not frustrated, you're (probably) not stretching yourself mentally — Hadley Wickham (@hadleywickham) 11. Februar 2015 It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. We highly advocate taking a break if you get stuck - the Workbook will not disappear, and you can complete the content at your own pace! Expanding your R learning (to infinity and beyond!) In addition to our course, there are many online tutorials that can help with learning R, specifically: Free RStudio Education resources Codeacademy YaRrr! The Pirate’s Guide to R Feedback: Weekly and End of Module We can only make this course better through your feedback. We collect feedback: At the end of every practical. There is a link to an anonymous feedback form, please let us know if something is unclear and we will go over it in a future session. At the end of the module. The standard UCL feedback form will be available to fill in on Moodle. Acknowledgements Part of this page is adapted from CASA0005. "],["what-is-this-workbook.html", "What is this Workbook? Using this Workbook Workbook Functionality", " What is this Workbook? All course content, including lectures and practical material, will be contained within this Workbook (hence it’s name!). As outlined earlier, you will also need to be part of our Geocomputation Team to have access to the lectures within the workbook. Any official course requirements (e.g. submission links for assessments) will be on Moodle. Each week’s content will be uploaded to the Workbook for Tuesday 10am UK time after the third seminar or coding help sessions. Using this Workbook Alongisde the recorded lectures and practical instructions, key things to look out for in the Workbook are Assignments, which are short optional submissions, Key Reading(s) and Points of Information, including Learning Objectives, Tips and Recap. To help, we’ll try to highlight them as follows: Assignment Each week, you’ll have 1-3 short assignments where we would like you to submit either a response, map or code prior to our seminar session or have it ready to present during the session itself. Key Reading(s) The recommended readings for this week will be highlighted in a Reading Box as and when appropriate in the week’s content. You’ll be able to find direct links to them within the E-Reading list. Learning Objectives Each week, we’ll start with a highlight of the learning objectives we hope you’ll achieve through the practical and lecture content. Get Ahead Tips Tips for effective note-taking during the practicals such as recording the functions you end up using in our practicals and your understanding of the arguments that they require. Recap A recap at the end of each section or week - make sure you take a note of these and are confident that you understand the points addressed. Workbook Functionality To get the most out of this book spend a few minutes learning how to control it, in the top left of this webpage you will see this toolbar: These buttons will let you: control the side bar search the entire book for a specific word change the text size, font, colour propose an edit if you see a mistake that I can review view the webpage in the ‘raw’ RMarkdown format, we cover RMarkdown in the course information about shortcuts for this book and most others like it In addition the icon in the top right of the page takes you to the GitHub repository for this book, where the online files for the book are stored. Acknowledgements Part of this page is adapted from CASA0005. "],["software-installation.html", "Software Installation QGIS R and R-Studio ArcGIS Installation Issues", " Software Installation This course primarily uses the R data science programming language and we strongly advise you complete the assignment using it. We briefly touch upon QGIS in the first few weeks to give you a basic foundation in spatial analysis alongside the range of spatial software available. Please follow the instructions below before completing the first practical session (in Week 2) to install the software on your local computer. Alternatively, both software are available on UCL’s computers and therefore can be accessed through Desktop Anywhere - however depending on your internet connection, this may be slow to use and, as a result, a highly frustrating experience! As outlined below, we have an online version of R-Studio available for use, but as yet, we do not have one for Q-GIS. If you are unable to download Q-GIS for your own computer, please let us know through the form below. QGIS QGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. To get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html We recommend installing the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates. R and R-Studio R is both a programming language and software environment - in the form of R-Studio- originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks. R-Studio Setup 1 Search for and open RStudio. You can install R Studio on your own machine from: https://www.rstudio.com/products/rstudio/download/#download R studio requires R which you can download from: https://cran.rstudio.com/ RStudio is a free and open-source integrated development environment for R — it makes R much easier to use. If you are using a Mac and run into issues, firstly follow the instructions below then check out the [Mac R issues] section if the problem persists. R-Studio Setup 2 UCL students (and staff) can now also make use of R Studio Server. It’s RStudio on a webpage, so no installation is required. Access information will be provided on Moodle in Week 2. ArcGIS ArcGIS Pro (previously ArcMap) is the main commercial GIS software that you may have already used - or seen/heard about through other modules or even job aderts. We do not use ArcGIS Pro in our Practicals for several reasons: Computing requirements for ArcGIS Pro are substantial and it only operates on the Windows Operating System. For Mac users, using ArcGIS Pro (and ArcMap) would require using iether a Virtual Machine or “splitting your own harddrive” to install a Windows OS. It is proprietary software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license (around £100!) to continue using the software and repeat any analysis you’ve used the software for. Whilst ArcPro can use pure Python (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own ArcPy and ArcGIS API for Python packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others if they do not have their own ArcGIS license. Recent developments in the ArcPro software however does make it an attractive tool for spatial data science - it has cross-user functionality, from data analysts who like to use a tool called Notebooks for their code development, to those focused more on cartography and visualisation with in-built bridges to Adobe’s Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but for this course, we want to ensure the reproducibility of work (you’ll learn more about this in Week 1’s lectures). Therefore, the analysis for your coursework must be completed in R/R-Studio and QGIS (where permissible, see guidance in Week 5). Installation Issues If you have any issues with installing either Q-GIS or R, please let us know during Week 1 via the tech-help channel within the Geocomputation Team. PLEASE CONFIRM YOUR SOFTWARE INSTALLATION We would appreciate it if you can fill in this Installation Confirmation Form to confirm whether you have been able to install the relevant software by Friday 15th January 2021 5pm UK time. Acknowledgements Part of this page is adapted from CASA0005. "],["external-usage.html", "External Usage Issues / Contributions License Version", " External Usage All the required data to run this course or individual practicals is publicly available through the direct links provided in the practicals. For UCL students, access to the Geocomputation Moodle page will enable you to access the pre-formatted datasets, when appropriate. There are two main options to adopt the written content and practicals of this course: Adopt the course in its entirety by forking the repository on GitHub and Pulling to your local machine or simply download a .zip file containing the entire course. Adopt a single practical by downloading the .rmd file. You will still need to follow the instructions within each practical to download the data - and format it as appropriate. For external users, you are welcome to get in touch with Jo (see previous details) directly if you would like access to the formatted files or help in how to format them. Issues / Contributions To raise an issue simply log it on the GitHub issues tab for the repository. To propose an edit click on the edit symbol in the top tool bar (see [How to use this book]) and submit it for review. If you wish to contribute material or data then please contact the course convenor Jo Wilkin (details below). License If you use this material for teaching, research or anything else please let me (Andy) know via Twitter or email — j [dot] wilkin [at] ucl [dot] ac [dot] uk). This practical book is licensed under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) License. You are free to: Share — copy and redistribute the material in any medium or format Adapt — remix, transform, and build upon the material for any purpose, even commercially. However, you give appropriate credit, provide a link to the license, and indicate if changes were made. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. But, you do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. The code within this pracical book is available under the MIT license; so it is free to use (for any purpose) as long as you cite the source. Version This is version 1.0 of the Workbook. Acknowledgements Part of this page is adapted from CASA0005. "],["geocomputation-an-introduction.html", "1 Geocomputation: An Introduction Getting Ready for our Practicals", " 1 Geocomputation: An Introduction Welcome to the first week of Geocomputation! Week 1 in Geocomp This week’s content provides you with a thorough introduction into what is Geocomputation, outlining how and why it is different to a traditional ‘GIScience’ course. We set the scene for the remainder of the module and explain how the foundational concepts that you’ll learn about in the first half of term fit together to form the overall Geocomputation curriculum. We also outline how the course is a great step towards those interested in a career in (spatial) data science. For this week only, there is no practical per se, but you will need to complete a few practical tasks in preparation for our future practicals. We appreciate that to get to this point in our content, you will have read a lot on both the Moodle and Coursebook and we do have a few readings we’d like you to do in anticipation of next week’s seminar. There are however 3 assignments that we’d like you to complete, two of which involve setting up your computer ready for next week’s practical. Learning Objectives By the end of this week, you should be able to: Understand the differences between traditional GIScience and Geocomputation Explain what spatial analysis is and why it is important for Geocomputation Understand why we will use programming as our main tool for data analysis Know how you will access both of required software for this course: QGIS and R-Studio Establish good file management practices, ready for the module’s practical content, starting next week. What is Geocomputation? According to Lovelace et al (2020): Geocomputation is a young term, dating back to the first conference on the subject in 1996…[Geocomputation] is closely related to other terms including: Geographic Information Science (GIScience); Geomatics; Geoinformatics; Spatial Information Science; [Spatial Data Science]; and Geographic Data Science (GDS). Each term shares an emphasis on a ‘scientific’ (implying reproducible and falsifiable) approach influenced by GIS, although their origins and main fields of application differ. GDS, for example, emphasizes ‘data science’ skills and large datasets, while Geoinformatics tends to focus on data structures…Geocomputation is a recent term but is influenced by old ideas. It can be seen as a part of Geography, which has a 2000+ year history (Talbert 2014); and an extension of Geographic Information Science and Systems (Neteler and Mitasova 2008), which emerged in the 1960s (Coppock and Rhind 1991). Geocomputation is part of but also separate to the wider discipline of GIScience (and Systems). As geographers, particularly ones at UCL, you are likely to have come across GIScience in one of its many forms, including the use of GIScience software, known simply as GIS software, such as ArcGIS Pro or ArcMap. What differentiates Geocomputation from traditional GIScience is that it is: working with geographic data in a computational way, focusing on code, reproducibility and modularity. Lovelace et al, 2020 We would also add that its main focus is on the analysis of data, rather than wider technological and informational challenges that GIScience also addresses. Suggested Reading If you’d like to read where the above quote is from, you’re welcome to get ahead of Week 5’s reading by looking at Lovelace et al (2020) linked below. This is just suggested reading for this week - and may make a little more sense when we come to Week 5. But there’s always benefits in doing (and reading!) things twice. Book Chapter (10 mins): Lovelace et al, An Introduction to Geocomputation with R, Preface and Introduction. What is important to recognise is that Geocomputation benefits from many of the epistemological and ontological developments that were made in the 1960s onwards within GIScience to enable us now to process substantial amount of spatial data, geographic and non-geographic, at signficiant speeds and visualise our results accordingly. This includes how we capture, record and store the world around us in a digital format, how to take this data and turn it into insight and also the more technical issues of data formats, storing assigned metadata such as projections, and ensuring cross-compatibility across different GIS software and programming languages. Key Definitions Geographic refers to space on the earth’s surface and near-surface. Non-geographic can refer to other types of space, such as network and graph space. The use of spatial incorporates both geographic and non-geographic space. You’ll also see geospatial analysis mentioned which subset of spatial analysis applied specifically to the Earth’s surface and near-surface. Although there are subtle distinctions between the terms geographic(al), spatial, and geospatial, for many practical purposes they can be used interchangeably. Within this module, our focus will be on how we can analyse spatial data in a computational way to address specific research questions - we will try to focus on issues that often concern geographers, including socio-economic and environmental challenges, such as driving factors of crime and deprivation, inequalities in access to greenspace and food and health establishments, and exposure to environmental concerns, such as poor air quality. To achieve this, we need to draw on specific foundational concepts from GIScience, such as spatial data models and data interoperability (Week 2), Cartography and Visualisation, including map projections (Week 3), alongside traditional Data Analysis, including using Statistics (Week 4), and also learn how to Program effectively and efficiently, particularly when it comes to using spatial data (Week 4 and 5). The remainder of this week’s content provides you with a brief introduction into each of these foundational concepts for Geocomputation. Before you get started with the rest of this week’s content, however, we’d like you to make sure you’ve installed the software ready for next week. It should also serve as a good break between reading and watching our lecture videos. Assignment 1: Download Q-GIS and R-Studio software Your first assignment for this week is to complete the steps found in Software Installation and complete the Installation Confirmation Form once done. GIScience: A Short History Almost everything that happens, happens somewhere. Longley et al, 2015 Geographic information has an important role across a multitude of applications, from epidemiology, disaster management and demography, to resource management, urban and transport planning, infrastructure modelling and many more. With almost all human activities involving an important geographic component, understanding where something happens – and also why – can often be the most critically important piece of information when decisions need to be made that are likely to affect individuals, communities, our increasingly connected societies, as well as the environment and ecology that exist in the area of study. Current methods of analysing geographic information have its roots firmly within the discipline of Geographic Information Science (GIScience), which first came into prominence in the 60s and 70s as the first Geographic Information System (GIS) was conceptualised by the “Father of Geographic Information Science and Systems”: Roger Tomlinson. He formalised the ideas within his Doctoral Thesis here at UCL in 1974, under the title “The application of electronic computing methods and techniques to the storage, compilation, and assessment of mapped data”. Whilst the thesis is nearly fifty years old, much of its content remains extensively relevant to the problems faced by the collection and processing of geographic data and analysis of geographic information today. Furthermore, he identifies two important requirements for the success of GIScience: Within the discipline of geography, it is suggested that the mutual development of formal spatial models and geographic information systems will lead to future beneficial shifts of emphasis in both fields of endeavour. Tomlinson, 1974 For GIScience to work as a discipline, there was a need to focus on both the development of spatial modelling (i.e. how to represent and analyse real world spatial phenomena in digital systems as spatial data) and of geographic information systems (i.e. how this data is stored, managed, retrieved, queried and visualised as information). Much of Tomlinson’s work contributed to establishing both the spatial models and GISystems we use today - and UCL has remained active in the development of this knowledge, culminating in our course textbook by Professor Paul Longley et al, who you will find in our Department. The foundations of GIScience have been built upon, with fifty years of development in the digital collection, recording, management, sharing and analysis of geographic data and information. For spatial modelling, the discipline has seen researchers develop and implement new methods and techniques of spatial representation and analysis to augment and extend the capabilities of working with spatial data. For GIS, the discipline has spawned a new industry focusing on the (commercial) development of GIS tools, software and applications. These tools have enabled different types of GIS, from databases to analytical software to online data services and servers. Furthermore, for both to work in unison with one another, GIScience has seen the establishment of the Open Geospatial Consortium, which aims to provide consensus on the standards and codes used with geographic data, information, content and services. The following short lecture provides an introduction to GIScience, including the topics that we’ll cover in more detail in next week’s lecture and practical. What is GIScience: past, present and future Slides | Video on Stream In addition to the short lecture, and in preparation for next week’s seminar, please read the following two Book Chapters: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 1: Geographic information: Science, Systems, and Society. Article (15 mins): Albrecht, J. (2020). Philosophical Perspectives. The Geographic Information Science &amp; Technology Body of Knowledge. We’ll cover the history of GIS software in a little more detail next week. Spatial Analysis: An Overview “80% of all data is geographic.” A geographer Whilst no one really knows the origin of this urban legend within GIScience, it is a quote that has been heavily used across the GIScience industry to explain the importance of spatial analysis and the untapped potential of spatial data. It is incredible to believe that just over a decade ago, when I was in your exact position, there was a need to justify why studying, collecting and analysing geographic information was important. In just over a decade, we have now seen a substantial transformation where analysing geographic data is no longer a niche activity, but almost omnipresent to our personal lives and our society at large. Suggested Reading Article (3 mins): Forbes, 2020, Mapping the way forward: GIS is powering solutions to global challenges We now have an entire map in our pocket, which not only provides us with spatial analysis on the fly, but the device on which this map exists itself provides data to others to conduct spatial analysis on our own behaviours. Not only can we find out the best route for us to drive to our favourite park at the touch of a button, we can also passively inform others on how long it will take them to get there too. We all now actively create, use and analyse spatial data - whether we are aware of it or not! The question that has faced those working in GIScience - and now in fields and disciplines beyond - is how to formalise these analyses into identifiable and, importantly, rigorous methods and techniques. Over the course of this module, we’ll introduce you to some of the core and advanced analysis methods that will be essential to analysing spatial data, including geometric operations, spatial autocorrelation, spatial regression, cluster analysis, interpolation and network analysis. These methods have been developed by those working actively in GIScience and spatial analysis, such as Dr Luc Anselin and his development of spatial autocorrelation methods. What you’ll learn - and quickly find out through your own application of these analysis methods - is that understanding the theory and principles behind them is just essential as knowing how to implement them, either through GIS software or programming. The following short lecture provides an introduction to spatial analysis and the techniques you’ll come across in the following weeks. Spatial Analysis: A key component of GIScience and beyond Slides | Video on Stream Assignment 2: Spatial Analysis and the COVID-19 Pandemic For your second assignment this week, we’d like to you think about how spatial analysis has been used in the current pandemic (don’t worry, this is one of the few times we’ll reference it moving forward!). Prior to next’s week seminar (and preferably by Friday 5pm), we’d like you to submit a short description (100~ words or less!) of an application you may be using, or have seen in the news, where you think spatial analysis has been critical to its success. You don’t need to know exactly how spatial analysis is being used, but you’re welcome to make a guess - you can also submit a reference as well, if you’d like. Also - an application does not necessary mean a phone app, but can be a tool, website, or dashboard - or anything else you can think of that has a spatial component to it! Please submit your description here! Programming for Data Analysis Concurrent to the developments within GIScience and spatial analysis, particularly over the last twenty years or so, we have begun to see a growing dataficaton of our everyday lives, where we: “take all aspects of life and turn them into data.” Cukier &amp; Mayer-Schöenberg, 2013 Our personal use of digital sensors - from our mobile phone data, use of online social networks and fitness trackers, to our travel and credit card - has created a deluge of data, most commonly known as ‘big data’. What sets ‘big data’ apart from traditional data is these data are often substantial in their volume, velocity and variety - making them difficult to manage, store, process and analyse. The hope, however, has been with big data is that by harnessing and ‘wrangling’ ita, we will be able to derive new insight from this data that can help address real world challenges, from something as simple as Google’s Traffic alerts within its Maps application, to tracking food security in areas where access for surveys are unfeasible. For GIScience and spatial analysis, what is important to note is that this data deluge and resulting specialisation has created a new approach to the analysis of data that goes beyond traditional data analysis, known as data science, which has worked its way into analysis streams and lexicon of many industries, from commercial organisations to academic research institutions. What distinguishes data science from traditional data analysis is that data scientists are able extract knowledge from these substantial datasets by using an intersection of three skills: hacking skills (or computational skills), statistical analysis and domain expertise. These computational skills - in the form of 1) programming, 2) distributed computing and 3) large-scale (complex) analysis - often set these data analysts apart from their traditional counterparts. The increasing popularity of data science is having a signficant impact on how we “do” spatial anaysis as more and more data scientists become involved with spatial analysis as, for many of these datasets, location is a key component for its management, processing and analysis - after all, everything happens somewhere. Concomittantly, there has also been an growing availability and accessibility of other geographical data, such as satellite and UAV imagery, that have significant interest to those working in these computational fields, such as computer vision and machine learning, such as extracting building and/or roads from true-colour satellite imagery. As a result, the “world” of geographic information has transformed rapidly from a data-scarce to a data-rich environment (Miller and Goodchild, 2015) and has garnered significant interest from those who do not necessarily consider themselves as working within the GIScience discipline. Their involvement has increased the utility of computational tools, such as programming languages and data servers, to ensure that traditional programming, data analysis and statistical langauges, such as Python and R can incorporate and conduct spatial analysis. This has involved the creation of many GIScience and spatial analysis focused libraries or packages (to be explained further in Week 4) within these programming languages, that have enabled analysts and researchers to run specific techniques or algorithms (such as calculating a buffer around a specific point) but for substantially larger datasets than traditional software can normally handle. Whilst this adoption of spatial analysis and GIS by non-GIScience practitioners certainly has (and continues to have) its pitfalls (as you’ll see later on in the module), there is also a growing influence and appeal of data science to many working in GIScience (and its related fields) – including its focus on analysing large-scale datasets that may have the potential to study geographic phenomena at unprecedented scales and detail. Unlike GIScience of ten years ago, there is, as a result, a pertinent need to teach these computational skills - first in the form of programming - to you as future GIS analysts, researchers or even data scientists, alongside the theory and principles of spatial analysis and the wider GIScience knowledge base. As you might guess where this is going, our focus on Geocomputation is a first step in this direction - which you may build upon in your third year, following through with modules such as Mining Social and Geographic Datasets. The following short lecture outlines the key reasons why we should program for spatial analysis: Slides | Video on Stream Programmming vs. GUI-GIS If you’ve not programmed before, the learning curve to program can be daunting - and also very frustrating! To be honest, even when you know how to program, it can still be incredibly frustrating! But the benefits of being able to program your data processing and analysis using a Command Line Interface (CLI) program, compared to using a traditional Graphical User Interface (GUI) sotware, are substantial. This does not mean using a GUI GIS does not have purpose or its own advantages. In my opinion, GUI GIS are incredibly useful tools to understand the “spatialness” of your spatial data and your spatial analysis, particularly when looking at spatial operations and spatial neighbours. The scripting aspect of R/Python often shield or hide you from this spatiality, which when you’re starting out with GIS and spatial analysis, is also an important learning curve! Furthermore, with GUI GIS, if you are interested in making paper-based maps and establishing your own “James Cheshire and Oliver Uberti” coffee table map books, learning map-making in a GUI GIS can be incredibly helpful in terms of understanding the flexibility of styling, label placement etc. ArcGIS, for example, has bridges with Adobe and its Creative Suite catalogue of software, enabling you to easily format maps you’ve made in ArcGIS Pro within Illustrator and/or InDesign. As a result, for the first two weeks of practicals - and part of the final practical - we will use Q-GIS so you have a basic understanding of how to use the software, and can then develop your use of the software outside of our course if and when you need. To understand more about what spatial (geographic) data science is (and why we program!), please read our other two key readings for this week: Key Reading(s) Article (25 mins): Brunsdon and Comber, 2020, Opening Practice: Supporting Reproducibility and Critical Spatial Data Scinece Article (10 mins): Singleton and Arribas-Bel, 2019, Geographic Data Science, Geographic Analysis In addition, you can watch this short video from Carto, a major commercial organisation working in several aspects of spatial data science. It outlines these key skills you’ll need to learn to become a competent spatial data scientist, including an understanding of spatial data, which many data scientists often lack prior to engaging with spatial data. Carto’s ‘What is spatial data science’ video What’s next for us in Geocomputation We believe strongly that effective users of GI systems require some awareness of all aspects of geographic information, from the basic principles and techniques to concepts of management and familiarity with applications. Longley et al, 2015 pg.32 For the next few weeks, we’ll be taking a deeper look at many of these foundational concepts that will ultimately enable you to be able to confidently and competently analyse spatial data using both programming and GIS software. As you might guess, you’ll therefore be going on many learning curves over the coming weeks - some that may feel familiar (e.g. applying descriptive statistics) and others that are more challenging (e.g. learning how to write code and debug it as you find errors). To help with this, I highly recommend that you try to stay organised with your work, including taking notes and making yourself a coding handbook. I’d also list the different datasets you come across - and importantly, the scales and different projections you use them at - more on this in the next two weeks. Finally, you should also make notes about the different spatial analysis techniques you’ll come across, including the different properties they assess and parameters they require to run. Furthermore, over the next nine weeks, you’ll learn how to plan, structure and conduct your own spatial analysis using programming – whilst making decisions on how to best present your work, which is a crucial aspect of any type of investigation but of particular relevance to your dissertation. Establishing an organised file system, for both your data and your documents, is essential to working effectively and efficiently as a researcher, whether in Gecomputation, Spatial Data Science or any other application you might think of! To this end, we move to the final part of our content for this week: creating our folders to establish good File Management procedures. Getting Ready for our Practicals To get ready for our practicals, which start next week, we would like you to set-up a file management system as follows (either on your local computer or DesktopAnywhere VM) - this will also help ensure any code you use from Week 4 onwards works without issue (*theoretically!): Create a GEOG0030 folder in your Documents folder on your computer (most likely inside a UCL or Undergrad or Geography folder, and then again within a Year 2 folder - although we are not mindreaders here ;) ). Next, within your GEOG0030 folder, create the following subfolders: data lecture_slides maps qgis notes any other folder types you may think you need for this course (although you can of course add these as the module continues) Note the _ separating the two words in lecture_slides folder. PLEASE DO NOT LEAVE ANY GAPS INBETWEEN YOUR FOLDER NAMES (OR FILE NAMES). We will explain why in our seminar next week. Also note we do not use any capitals in our folder names. Within your data folder, create the following subfolders: raw working final If you’ve downloaded the lecture slides, move these into your lecture_slides folder. We’ll explain more about establishing good file management procedures in the seminar at the beginning of next week. Assignment 3 Follow the above guidelines to create your folders in your local system ready for our practicals to begin next week. And that’s it. You’re now ready to start our practicals next week. We look forward to meeting you all in our first seminars next week and address any questions you might have from this week’s content! Week 1 Recap This week, we’ve provided you with an introduction to the Foundational Concepts you’ll be coming across in our course as we train you to become competent spatial data analysist. You should now: Understand the differences between traditional GIScience and Geocomputation. Be able to explain what spatial analysis is and why it is important for Geocomputation. Understand why we will use programming as our main tool for data analysis. Know how you will access both of required software for this course: QGIS and R-Studio - or flagged this as an issue to us via the form! Have created your file system for GEOG0030 ready to practice good file management for the module’s practical content, starting next week. "],["giscience-and-gis-software.html", "2 GIScience and GIS software", " 2 GIScience and GIS software Welcome to Week 2 in Geocomputation! I hope you have your favourite caffeinated (or not!) beverage at hand and some good concentration music because this will be a longer than usual week of work to get through - but if you concentrate, take your notes, and complete our practicals, it will hold you in good stead as you progress along our course. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 3 will be shorter in content, therefore you will have time to catch up before the seminars at the start of Week 4. Week 2 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with GIScience and GIS software. Out of all our foundational concepts you’ll come across in the next four weeks, this is probably the most substantial to get to grips with - and has both significant theoretical and practical aspects to its learning. This week’s content is split into 6 parts: What is Representation? (5 minutes) Geographic Representation (25 minutes) Spatial Structure, Sampling and Scale (25 minutes) Spatial Data Models (45 minutes) Spatial Data File Formats (20 minutes) Practical 1: Exploring Population Changes Across London (1 hour) Videos can be found in Parts 2-5, alongisde Key and Suggested Reading and the first two of 3 assignments. Video content this week is a mixture of short lectures from myself, and two videos from YouTube. The two explanations from YouTube summarise the content presented in this workbook succinctly and with some really interesting examples. Using these videos have allowed me to spend more time on your practical - including ensuring there is a practical for those of you who cannot download Q-GIS. Part 6 is our Practical for this week, where you will be introduced to Q-GIS and apply the knowledge gained in the previous parts from Parts 1-5 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical. Learning Objectives By the end of this week, you should be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and their respective file types Know how to manage and import different spatial file types into a GIS software Learn how to use attributes to join table data to vector data Introduce you to the concept of an Administrative Geography (more next week!) As stated above, there is a lot to go through in this week’s content - but everything you will learn this week will provide you with a comprehensive background for the following weeks in the module. What is Representation? To be able to conduct any spatial analysis using our GIS tools or software, we first need to establish how we capture the geographical features, processes and/or phenomena that we wish to study as digital data that is readable by ourselves and by our computers. Coming to GIScience, at this point of time in its development, we often take this above statement for granted – we are, as mentioned last week, surrounded by geographical (and social) data, where much of this conceptual work has been done and, as a result, is often hidden from us as users of this data and/or technology. We can, for example, access data on every single train station in England - we can download this data directly from OpenStreetMap. But you’ll have a choice - depending on how you wish to represent your train stations, which is usually determined by your work purpose. For example, are you looking to show the distribution of stations over England? Do you therefore download the train stations as individual points that you could label with their name? Train stations in England represented as points on a map. Data from © OpenStreetMap and its contributors However, is this truly enough to represent a “train station” - surely, you might want to have the building instead, because this is the actual “station” itself? But then again, is this still enough? Do you need to have the components that constitute a train station - the railway tracks, the ticket office (or ticket stations more common now!), and even the waiting room - to truly represent every train station in England? In our case example, when looking only at the distribution of train stations, a point representation is likely to be sufficient enough - but this representation does not tell us much about the size and service area of each of the stations - or much else about them! We often do not think nor question the representations used to present data to us when we use or interact with spatial data - until, for example, we see something wrong that does not fit with our expectations or does not contain the information we want or expect for our purpose. Often, at times, representations can also be misleading, if the right information is not conveyed - or conveyed in the wrong way. However, as you’ll find out below, we often need to weigh up including too much detail in our representations, particularly if this detail is redundant to the information we wish to convey. We therefore use representations to convey information about something in the real world - but these representations almost always simplify the truth. We simply can’t fit every piece of information about the world around us within a representation - we have to select what bits are most important and relevant to what we are trying to convey. What this also means is that for these representations to mean something to us (i.e. can be interpreted), they need to fit this information into a standard form or model that we have come to expect in their representation. In our case above, we are able to associate a point on a map as a point of interest - and understand the distribution of the train stations thusly - because this has become the most dominant way to represent the location of an entity in a simple format on a map. As a result: the creation of these representations have required significant epistemological and ontological developments in order to turn the complexities of the world around us into information that we can understand. This includes: How to “view” the world around us in ways that lend themselves to be modelled by digital data. How to “sample” the world around us to be able to model these “views” as digital data. How to structure these models as digital data to facilitate their processing and analysis. How to create standardised formats to store and share these digital data across programs, software and computers. As a result, there are established rules, and classification schema (“models”) to how we represent geographic pheonomena and processes, which you will learn about today. Definitions Epistemological: ‘what we know and how we can know it’ - the theory of how a piece of knowledge has come into being, including the methods behind generating its “truth” and the validity of these methods, the belief in this truth, and the justification of holding these beliefs. Ontological: ‘studying what there is’ – questioning how we see our reality and categorise it in order to determine how things come into being. Geographic Representation To be able to convert the world around us into digital geographic data, we first need to understand how we can represent the features, processes and phenomena we may want to study. As Longley et al (2015) explain: “Representations help us assemble far more knowledge about the Earth than is possible on our own…They are reinforced by the rules and laws that we humans have learned to apply to the unobserved world around us.” As outlined above, increasingly due to our use of digital technology, this representation itself is rarely seen or really understood by the users of the data – only those creating the data are likely to ever see its individual elements and/or components. But behind the data that you’ll become familiar with over the course of this module, there are significant and specific decisions that have been made, which you should be aware of in order to understand these data (and their limitations) fully. One of the major developments in GIScience was the creation of representations that can capture the different types of geographic phenomena and processes around us - which could then ultimately be modelled and turned into digital data. These representations view the world in two fundamental ways: as discrete objects and as continuous fields. In summary, the discrete object view represents the geographic world as objects with well-defined boundaries, within larger objects of well-defined boundaries, in otherwise empty space, i.e. similar to our reference mapping schematisation. In comparision, the continuous field view represents the real world as a finite number of variables, that can each be defined at every possible position to create a continuous surface of the respective variable. The following short video outlines these in more detail, with examples: Understanding how to represent the world around us However, one thing to note is that many geographic phenomena have both object and field characteristics. When representing and modelling many features, the boundaries are not often clearly continuous or discrete. A continuum is created in representing geographic features, with the extremes being pure discrete and pure continuous features. Most features fall somewhere between the extremes. An example could be looking at edges of forest and trying to define their boundaries – does the boundary stop at the tree trunk or the diffuse layering of leaves? A recent tweet from MapMaker David - one of my Twitter follow recommendations! This question actually poses itself to even the most experienced of GIS-ers and cartographers! David Garcia (aka Mapmaker David) is a Filipino Geographer and Cartographer and someone who I would highly advocate following on Twitter. He often raises a lot of questions about the epistemological and ontological aspects of GIScience and their development from essentially cartography and the role this has in minimising indigenous knowledge - he also makes beautiful maps. This critical approach to GIScience is something that we’ll look into a bit more in Week 5, in order to have time to give this content due justice! Assignment 1: Discrete Objects and Continuous Fields Let’s think about spatial representation models in more detail. Below are links to four spatial datasets that I’d like you to think about whether they represent discrete objects or continuous fields. Click on each link and note down your answer - I’ll be asking for these in our seminar in Week 4: Dataset Spatial Model Type USA Tree Canopy Cover ? Global Land Cover ? OS Open Rivers ? World Population Density Estimate ? Ultimately though, continuous fields and discrete objects only define two conceptual views of geographic phenomena, but do not solve the problem of digital representation, i.e. how do we capture this representation using computers. A continuous field view still potentially contains infinite amount of information as it aims to defines the values of the variable at every point – and there are an infinite number of points in any defined geographic area. In contrast, discrete objects can also require an infinite amount of information in order to provide a full description (e.g. our train station dataset above!). Neither of these approaches are designed to deal with the limitations of computers and the need to store this representation digitally - for this, we need to understand the spatial structure of the phenomena or process at study alongside the scale at which we want to represent them in order to devise a sampling scheme behind our data creation. Spatial Structure, Sampling and Scale Why do we need to sample our data? Well – if we try to include everything in our representation, we’d end up with a map the size of the world, which would be pretty useless! This issue is quite eloquently expressed by the Argentine writer, Jorge Luis Borges, who made up a fictional short story of the issue of an Empire aiming to create a map that was so perfect it could represent the whole empire - because it was the size of the Empire itself, coinciding point for point. As a result, the map, whilst perfect, was useless and was offered up to the elements to essentially destroy by the following generations! A short story on the issue of representation in science, geography and map-making On Exactitude in Science Jorge Luis Borges, Collected Fictions, translated by Andrew Hurley. …In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast Map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography. —Suarez Miranda, Viajes de varones prudentes, Libro IV,Cap. XLV, Lerida, 1658 (Borges’ fictional character of “the time”) Find more here: There is No Perfect Map by Marcelo Gleiser (5 mins) and Why a 70 year-old short story goes to the heart of modern map making by Ian Delaney (3 mins). To be able to create accurate representations of our geographic phenomena and processes, we therefore need to find a way to sample our phenomena or process to reduce the information whilst still retaining the most important pieces of information. You have probably come across the concept of sampling before when it comes to surveys and statistics and the need to create samples from a population. In this case, whenever we look to derive an accurate sample from a population, we look to create a sample frame or scheme to extract statistically significant information. In your previous research experience, you may have come across the ideas of random, systematic and stratified sampling - and that you choose the sampling approach that most reflects the likely structure or distirbution of the population you are targeting to sample. We can think of converting our geographic representations into digital data as a similar kind of sample, in that the elements of reality that are retained are abstracted from the observable real-world in accordance with some overall design. Therefore, to create digital data from our representation, we need to design a way to sample it. To do this, we first need to understand the structure of the data in order to deduce a good ‘sampling strategy’. The next lecture in this workshop provides an introduction to how we can use the structure of spatial data to determine appropriate sampling schemes. Understanding the structure of spatial data to determine sampling schemes Slides | Video on Stream When looking at the representation of geographic phenomena as digital data, the scale and level of detail of that is needed for the analysis will therefore determine the spatial sample design and how we can then generalise from these measurements. As a result, scale and level of detail are key to building appropriate representations of the world. Assignment 2: Digitising the River Thames, London, U.K We can put these ideas into practice by thinking about how we could create our own digital data. Let’s take what should be a straight-forward example of digitising the River Thames in London. The River Thames in London. Image: Esri. We’re going to use a very light online tool that allows us to create digital data (and as you’ll see later in the workshop, export the data we create as actual raw files). Head to geojson.io - it should load directly, zoomed into London. In the bottom left-hand corner, select Satellite as your map option. Next, click on the Draw a Polyline tool: Now digitise the river - simply click from a starting point on the left- or right-hand side of the map, and digitise the whole river. Once you’re done, simply double-click your final point to end your line. You can then click on the line and select info to find out how long the line is. For this assignment, I’d like you take a screenshot of you final line. When you click on the line, you can use Properties to style the line to make it more visible, e.g. change the colour and the width of the line. Please then post your screenshot on a new slide in your respective group’s Powerpoint you can find here and add a text-box stating how long your line is (in Km) (don’t worry, you don’t need to add your name). We’ll look at each other’s digitisation attempts during our seminar this week – but the questions to think about are: How easy did you find it to digitise the data and what decisions did you make in your own ‘sample scheme’? How close together are your clicks between lines? Did you sacrifice detail over expediency or did you spend perhaps a little too long trying to capture ever small bend in the river? How well do you think your line represents the River Thames? In the activity above, we were looking at the river as a discrete field – imagine then if I asked you to find a way to collect data on and then digitise the air quality over the same area of London? How would you go about creating an appropriate sample scheme to accurately represent air quality – without spending too much time on collecting the data that is becomes almost redundant? In both of these scenarios, you are using your a priori knowledge of the spatial structure of the phenomena to determine your spatial sampling scheme. However, in some scenarios, we may not know this structure before sampling nor can you always control for all variations in all characteristics. When looking to record a phenomenom as digital data at a fine scale, i.e. a high spatial resolution, we need to ensure our sample scheme reflects the minimal variation in the spatial autocorrelation with a feature. To record digital data at a coarse scale, i.e. a low spatial resolution, we can be more flexible with our sample scheme – but should ensure it reflects larger changes within our phenomenom. Whilst ideally we would want to capture our representation in as fine scale as possible as this is likely to be the most accurate, this sometimes can be detrimental to our capturing and storage of the representation as digital data (see the next section). Ultimately, a sampling scheme will be a best guess: we must remember that GIScience is about representing spatial and temporal phenomena in the observable world, and because the observable world is complicated (and does not always adhere to Tobler’s principles), this task is difficult, error prone, and often uncertain. As a result, with any data you use from other sources, always remember to consider its quality, accuracy and precision in representing geographic phenomena. Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 2: The Nature of Geographic Data. Computational considerations of the impact of scale and sampling One final thing to note when it comes to sampling spatial data at various scales is that if we try to sample and study complex phenomena at fine spatial resolutions but over significant extents, we may ultimately create many issues from a computational perspective. Whilst we may be able to sample our spatial phenomenon at a increasingly fine detail (e.g. satellite imagery can now collect data at less than a meter precision), this data ultimately has to be stored digitally. As a result, when looking to use increasing levels of precision over vast scales in terms of spatial coverage/extent, we can inadvertently create substantially large datasets that computers can struggle to visualise and process. As a result, we need to be conscientious about the data we are trying to create and use - for example, the Ordnance Survey’s MasterMap topography layer contains 400 million individual features (i.e. records). Trying to load even a subset of this on your computer can often cause significant processing problems! Usually, this means you have a choice. You can study something at a fine resolution, but you’ll need to keep your spatial coverage small. In comparison, you can expand your coverage if you reduce the resolution of your data. This all depends on the computational processing capability and capacity you have at your disposal, as well as what you are trying to achieve with your analysis, i.e. what detail do you need to answer your research questions. In addition, generalising is a key approach within GIScience that focuses on removing detail that is unnecessary for an application, in order to reduce data volume and speed up our processing. There are many approaches to generalising spatial data, which we come across in more detail over the coming weeks including simplification, smoothing, aggregation and amalgamation. Ultimately, we need a priori information to inform our understanding of whether our sampling scheme and resulting digital data is suitable for our analysis, i.e. it is accurate enough without hindering processing power. Determining an appropriate sampling scheme and resulting method of capturing this representation as digital data will therefore be determined by the phenomenom at study – and the limitations of those using and processing the resulting data. Spatial Data Models We can now see how we convert the observable world around us into spatial representations – and how we then need to consider scale and level of detail, alongside spatial structure, to determine our spatial sampling scheme. The next step is to convert our sampled observations (how ever they are collected) into digital data. Digital data at its basics is a form of binary data entry: the representation system in digital computers uses only two numbers (0 and 1). As a result, “Every item of useful information about the Earth’s surface is ultimately reduced by a GI database to some combination of 0s and 1s.” Longley et al, 2015 To create our modern day digital geographic data, we need to devise spatial formats that can ultimately be ‘written’ (or rather, ‘coded’) using this binary entry. Many of these decisions formed much of Roger Tomlinson’s original body of work (and others!). In this thesis, he outlined how to capture “real world data elements” as digitised geometries (points, lines, polygons) and grids - and how to store them in a coded digital format: Tomlinson’s original proposal for coding spatial data formats. Image: Tomlinson, 1974 These formats are the basis to the two main spatial data models we use. These are called raster and vector data formats, which are explained in further detail in this short video: Raster and Vector Spatial Data The below text summarises what was presented in the above video. Raster Data Format A raster dataset is a pixel-based grid data format. For any variable studied, a grid is created within which each pixel represents a value or measure for the variable: A raster grid and pixel. Image: QGIS Raster data only contain a single “attribute” for the variable it represents – and the attribute will be coded according to the data measurement scale and attribute type (see below). Rasters are primarily stored as a type of image file, that is either geo-referenced (e.g. a GeoTIFF) or comes with an additional georeferencing file (normally called a World file). Vector Data Format In comparison, vector data contains geometries: the points, lines and polygons we’ve seen earlier in the workshop. To provide the “geographic” component of these geometries, they actual geometry itself is specified using a pair of coordinates, preferably assigned to a specific coordinate reference system (the below diagrams simply use a graph!): Vector data: points, lines (polylines) and polygons (on a graph). Image: mgimond. As you can see, the three types of vector geometries are: A point dataset, which will have at least a single pair of coordinates for each point (or more generally “record”) within its dataset A single line, which will have two pairs of coordinates, whilst a polyline (multiple lines connected together) will have a minimum of three pairs. A polygon, which will have a minimum of three pairs (forming some sort of triangle!). Alongside containing these geometries, a vector dataset can also contain multiple attributes for each the records it contains. These attributes are stored in what is known as an Attribute Table. An Attribute Table consists of a set of records/observations (the rows) and attributes/fields (the columns): An example of an attribute table in ArcMap. Source: Esri Each record within the dataset will refer to one point, polygon or line (polyline) and will contain a value for each attribute/field that is part of the dataset. This includes a geometry field, which will contain the coordinates required to map and display the dataset correctly within its Coordinate Reference System (CRS) - more on these next week. The use of “field” for attribute tables At this point, it is important to note that you should not confuse the use of field here with our previous use of field in terms of spatial representation models. ‘Field’ and ‘scale’, as you can tell, have many meanings when used in GIS – but the more you come across the terms within context, the easier you’ll find it to understand which meaning is being referred to! These attributes will be stored in the field as a specific attribute measurement scale and as a specific data type - depending on the variable or data that they represent. Attribute Data Measurement Scales and Types For any data, whether spatial or not, it will collected against a specific measurement scale and, in its digital form, be stored as a specific type of data type. This measurement scale is a classification that describes the nature of the information of the values assigned to the specific variables. Data can be: Measurement Scale Explanation Nominal Has labels without any quantitative value. Ordinal Has an order or scale. Interval Numeric and have a linear scale, however they do not have a true zero and can therefore not be used to measure relative magnitudes. Ratio Interval data with a true zero. In addition, data may also be: Measurement Scale Explanation Binary Can have only two possible outcomes, yes and no or true and false, etc. Image: Allison Horst For example, for our point data set of train stations mentioned earlier: A field that contains the name of each train station would be nominal data. A field that details the class of the train station, e.g. whether it is a mainline, secondary or tertiary line as a type of order or rank, would be ordinal data. A field that details the temperature of the train station in celsius would be interval data A field that details the number of tracks the station contains would be ratio data A field that details whether the station is operational or not could be binary data (a ‘yes’ or ‘no’ or ‘operational’ or ‘non-operational’) Depending on the measurement scale, the attribute data will be stored as one of several data types: Type Stored Values Character Formats Short integer -32,768 to 32,768 Whole numbers Long integer -2,147,483,648 to 2,147,483,648 Whole numbers Float -3.4 * E-38 to 1.2 E38 Real numbers Double -2.2 * E-308 to 1.8 * E308 Real numbers Text Up to 64,000 characters Numbers, letters and words Knowing your measurement scale and data type level are essential to working accurately and effectively with spatial data. If you inadvertently store a float (e.g. values of 1.021, 1.222, 1.456, 1.512, 1.888) as an integer, your number will be rounded (e.g. it would become: 1, 1, 1, 2, 2) which can impact the accuracy of your work. Conversely, while storing whole numbers (integers) as a float or a double would not have an accuracy issue, it will come at a computational cost in terms of storage space. This may not be a big deal if the dataset is small, but if it consists of tens of thousands of records the increase in file size and processing time may become an issue. Being aware of (and checking!) your data types can also help solve initial bugs when loading and trying to analyse or visualise data in both GIS software and programming. For example, one commmon issue with data types when using table data within Excel prior to ingesting your data a GIS software or program is that Excel often converts British National Grid coordinate codes (which are integers) into text - therefore, when you come to display your point data, for example, by their coordinates, this field is not readable by your software or program. You therefore need to force your program to recognise that field as a numeric field - we’ll come across this issue and ways to solve it in Week 5. In addition to these attributes that contain variable information that might be used for analysis or visualisation purposes, each record should contain its own ID that will be used for indexing purposes in both GIS software and programming. This can help you select certain rows for analysis or order your data. Finally, in some cases, a dataset may contain a unique identifier (UID) for each record that can be used for data management purposes. These UID can be used to match with another dataset containing the same UID. In this latter scenario, this helps us join data that we may download as table data (e.g. a spreadsheet of population numbers for the different wards in London) with spatial data (e.g. a spatial dataset that shows the outlines of the wards in London) to create a new spatial dataset that contains the population data as an attribute, ready for its analysis and/or mapping. We’ll see this in action in today’s pratical. Don’t worry if this is a lot to take in right now, we’ll be utilising a lot of what you are reading about here in practice in the coming weeks! Rasterising vector and vectorising raster One additional thing to know about vector and raster data is that, in some cases, it is possible for both data formats to represent the same geographic feature, process or phenomena – but how they do so will look very different: Differences in capturing and storing geographic phenomena as vector and raster data. Image: vebuso. And also each data models comes with both advantages and limitations: Summarising key advantages and vector and raster data. Image: vebuso. There are also tools within our GIS software and programming software that will allow us to convert between the two data formats. This can be of use when we wish to process data faster (e.g. rasterising vector data) or we wish to add attributes to what was a continuous field (i.e. vectorising raster) for analysis. There will, of course, be considerations and limitations when switching between data formats, such as loss of accuracy in either direction of conversion. The results of vectorising elevation represented as a Digital Elevation Model Left image: DEM, Right image: vector version. Image: Esri. You will find more information on Spatial Data Models and the raster and vector data formats in the following two chapters in the Geographic Information Science &amp; Systems (GISS) book: Key Reading(s) Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 3: Representing Geography. Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 7: Geographic Data Modeling. Spatial Data File Formats The final part to our introduction to spatial data is understanding the different file formats in which spatial data is stored. There are a number of commonly used file formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them. Shapefiles Perhaps the most commonly used spatial data file format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications. A shapefile is not a single file, but a collection of files of which at least three are needed for the data to be displayed in GIS software. The files include: File Type Description Required? .shp Contains the feature geometry Mandatory .shx Index file which stores the position of the feature IDs in the .shp file Mandatory .dbf Stores all of the attribute information associated with the records Mandatory .prj Contains all of the coordinate system information. Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used. Optional, but important .xml General metadata Optional, but important .cpg Encoding information Can also be included .sbn Optimization file for spatial queries Can also be included When using shapefiles, it is good to get into a habit of creating zipped archives of your file that you can share with yourself and others – this means selecting all the related files, right-clicking and choosing to compress or archive your data. This creates a single ‘file’ to move, for example across folders, so you do not end up losing any of the files that are critical for the shapefile to display! Copying and pasting the .shp file alone is not enough! This is one of the main criticisms of the shapefile – it is easy to lose files and as a result render your data useless. Other GIS formats such as GeoJSON and the increasingly popular GeoPackage include all of this information in a single file, reducing this risk substantially of this happening. Despite these issues, the shapefile still remains an ever-popular GIS format, and one you’ll use the most in this course. On Twitter and want to see the love for shapefiles….have a look at the shapefile account: GeoJSON GeoJSON (Geospatial Data Interchange format for JavaScript Object Notation) is becoming an increasingly popular spatial data file, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file. Comparing Shapefile and GeoJSON file formats If you would like, you can explore a shapefile (.shp ) and GeoJSON (.geojson) in action - we’ll use the light digitising tool, that we used earlier to digitise the River Thames: Head to: http://geojson.io/#map=16/51.5247/-0.1339 Image: Digitised point, line and polygon examples. 2. Using the drawing tools to the right of the map window, create 3 objects: a point, line and a polygon as shown above. Click on your polygon and colour it red and colour your point green. Using the ‘Save’ option at the top of the map, save two copies of your new data – one in .geojson format and one in .shp format. Open your two newly saved files in a text editor such as notepad or notepad++ on your computer. For the shapefile you might have to unzip the folder then open each file individually. What do you notice about the similarities or differences between the two ways that the data are encoded? I won’t ask you about this in our seminar, but it’s a good way to start getting familiar with the actual structure of our data. If you do end up having issues with your datasets, this may give you an idea of where you might find out if there’s an issue with your raw data itself. Geodatabase A geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab. GeoPackage A GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move. Raster Data Most raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostationary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type. Other Data Formats The aforementioned file types and formats are likely to be the ones you predominately encounter. However there are several more used within spatial analysis. These include: Vector GML (Geography Markup Language —- gave birth to Keyhold Markup Language (KML)) SpatialLite PostGIS Raster Band SeQuential (BSQ) - technically a method for encoding data but commonly referred to as BSQ. Hierarchical Data Format (HDF) Arc Grid There are normally valid reasons for storing data in one of these other file formats, however you do not need to read or know about these for now! In the end, the variety of data formats can be a bit overwhelming. But don’t worry, most of the time in this course you’ll be using shapefiles, table (in the form of csvs) or raster data. Table Data: Comma Separated Values (.csv) v. Excel Spreadsheet (.xls) In addition to spatial data, you will find that in this module (and for your dissertations), you will download and use a lot of table (tabular/spreadsheet) data. When you download this data, you can first inspect that data in Excel or Numbers (or another spreadsheet application of your choice), prior to loading it into either GIS software or programming software (such as R-Studio). The reason why is a lot of the time, you will need to clean this dataset prior to using it within these software/programs. Often the data comes formatted with too many rows, additional formatting, or generally just a lot of additional stuff we just don’t need. We’ll take a deeper look at this need for cleaning in Week 4 as we tackle using R-Studio. One thing to note though is that there are differences between a csv and an Excel spreadsheet, particularly if the latter is contained in a Workbook. There are a few summaries of these differences available online and we will go over the differences in further detail again in Week 4. For now, please be aware that we will be using csv as our default table data format, so if you need to save anything at any point in our practical, please save your file as a csv. GIS Software - a more thorough introduction (moved to Week 5) As outlined last week, this week, we were going to provide you with a more thorough introduction to the different types of GIS software available to you - but we’ve decided that you’ve read/listened/learned enough about spatial data that more information on GIS software is not going to help. Instead, we’ll cover this in Week 5 and now move onto our Practical. If you’d like to get ahead, you can read the following chapter in GISS: Book (15 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 6: GI System Software. Practical 1: Exploring Population Changes Across London The first half of this workshop has given you an in-depth introduction into how we can represent the world around us and turn it into digital geographic data – and how we store this data from a technical perspective. The practical component of the week puts some of these learnings into practice with an exploration of population data within London. The datasets you will create in this practical will be used in Week 3 practicals, so make sure to follow every step and export your data into your working folder at the end. The practical component introduces you to attribute joins. You’ll be using these joins throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. A Typical Spatial Data Analysis Workflow When using spatial data, there is generally a very specific workflow that you’ll need to go through - and believe it or not, the majority of this is not actually focused on analysing your data. Along with last week’s “80% of data is geographic data”, the second most oft-quoted GIS-related unreferenced ‘fact’ is that anyone working with spatial data will spend 80% of their time simply finding, retrieving, managing and processing the data – before any analysis can be done. One of the reasons behind this need for a substantial amount of processing is that the data you often need to use is not in the format that you require for analysis. For example, for our investigation, there is not a ‘ready-made’ spatial population dataset (i.e. population shapefile) we can download to explore popuation change across England: Image: Alas a quick google search shows that finding a shapefile of England’s population is incredibly difficult! Instead, we need to go and find the raw datasets and create the data layers that we want. As a result, before beginning any spatial analysis project, it is best-practice to think through what end product you will ultimately need for your analysis. A typical spatial analysis workflow usually looks something like this: Identify the data you need to complete your analysis i.e. answer your research questions. This includes thinking through the scale, coverage and currency of your dataset. Find the data that matches your requirements - is it openly and easily available? Download the data and store it in the correct location. Clean/tidy the data - this may be done before or after ingesting your data into your chosen software/program. Ingest/load the data into your chosen software/program. Transform &amp; process the data - this may require re-projection (next Week), creating joins between datasets, calculating new fields and/or creating selections of the data that you want to work with (Week 5). Run Analysis on your data, whatever technique you are using. Visualise your data and results, including maps, graphs and statistics. Communicate your study and outputs - through good write-ups and explanations of your visualisations. As you can see, the analysis and visualisation part comes quite late in the overall spatial analysis workflow - and instead, the workflow is very top-heavy with data management. Wrangling data is often the most time-consuming part of any spatial analysis project! Image: Allison Horst Often in GIS-related courses, you’ll often be given pre-processed datasets ready to go ahead with analysing the data. Instead, we’re going to start cleaning (the majority of) our data from the get-go. This will help you understand the processes that you’ll need to go through in the future as you search for and download your own data, as well as deal with the data first-hand before ingesting it within our GIS software.Good thing you’ll be learning a lot about these aspects over the coming weeks! Setting the scene: why investigate population change in London? For this practical, we will investigate how population has changed over the last ten years in London. Understanding population change - over space - is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure. We can also see first-hand the impact of wider socio-economic processes, such as urbanisation, or, in the case of the predicted population movements currently, relocation of a certain demographic of urban dwellers to rural areas. For us, the aim for our practical is to actually create population data for London in 2011, 2015 and 2019 at the ward scale that we can use within our future analysis projects, starting next week. This data will be used in our future practicals to normalise certain data, such as the crime datasets for next week. Why do we need to normalise by population? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event, such as crime. We’ll look at this in greater detail next week. Finding our datasets In the U.K, finding authoritative data on population and Administrative Geography boundaries is increasingly straight-forward. Over the last ten years, the UK government has opened up many of its datasets as part of an Open Data precedent that began in 2010 with the creation of data.gov.uk and the Open Government Licence (the terms and conditions for using data). Data.gov.uk is the UK government’s central database that contains open data that the central government, local authorities and public bodies publish. This includes, for example, aggregated census and health data – and even government spending. In addition to this central database, there are other authoritative databases run by the government and/or respective public bodies that contain either a specific type of data (e.g. census data, crime data) or a specific collection of datasets (e.g. health data direct from the NHS, data about London). Some portals are less up-to-date than others, so it’s wise to double-check with the ‘originators’ of the data to see if there are more recent versions. For our practical, we will access data from three portals: For our administrative boundaries, we will download the spatial data from the London Datastore (which is exactly what it sounds like!). For population, we will download table data from the Office of National Statistics (ONS) (for 2019 data to represnt 2020) and the London Datastore (only contains these data until 2018). In our extension activity (available later this week), we will also download a gridded spatial dataset showing how population can be represented in the raster data format from the Worldpop research group at the University of Southampton. Download and process datasets The first step in our practical is to download and process our two main datasets: administrative geography boundaries and population. Administrative Geography Boundaries For our administrative boundaries, we’ll download the ‘Statistical GIS Boundary Files for London’ dataset(s) found in the London Datastore. Navigate to the datasets, here: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london. If you navigate to this page, you will find multiple choices of data to download. We wil need to download the all three zipfiles: statistical-gis-boundaries-london.zip, London-wards-2014.zip and London-wards-2018.zip. The first dataset contains ALL levels of London’s administrative boundaries. In descending size order: Borough, Ward, Middle Super Output Area / MSOA, Lower Super Output Area / LSOA, and Output Area / OA) from 2011. The second dataset contains an UPDATED version of the Ward boundaries, as of 2014. The third dataset contains an UPDATED version of the Ward boundaries, as of 2020. As we will be looking at population data for 2015 and 2020, it is best practice to use those boundaries that are most reflective of the ‘geography’ at the time; therefore, we will use these 2014 / 2018 ward boundaries for our 2015 / 2020 population dataset respecitvely. When downloaded, depending on your operating system, the zip may unzip itself (or you may need to do this manually). When open, you’ll find two folder options: Esri and MapInfo. These folders contain the same set of data, but simply in two data formats: Esri shapefile and MapInfo TAB. MapInfo is another proprietary GIS software, which has historically been used in public sectors services in the UK (and many councils still use the software!), although has generally been replaced by either Esri’s Arc ecosystem or open-source software GIS. The TAB format is the main format that the software uses for vector data, similar to Esri and its shapefile format. In your GEOG0030/data/raw/ folder, create a new folder called boundaries. Within this folder, create three new folders: 2011, 2014 and 2018. Copy the entire contents of Esri folder of each year into their respetive year folder within your new boundaries folder: Note, we do not want to add the additional Esri folder as a step in our file sytem. I.e. your file path should read: GEOG0030/data/raw/boundaries/2011 for the 2011 boundaries, and GEOG0030/data/raw/boundaries/2014 for the 2014 boundaries etc. We now have our Administrative Geography files ready for use. We will ingest these directly into Q-GIS and do not need to do any cleaning at this stage. What are wards and boroughs? A short introduction to Administrative Geographies. Put simply, administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. Administrative areas ensure that each public body has a clearly defined area of responsibility, which can be measured and budgeted for appropriately. They originate from the Roman era who used these geographies, usually defined by topographical and geographical features, to administer these regions including collecting the relevant tax from those living in these areas. These geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. The UK has quite a complex administrative geography (see more here), particularly due to having several countries within one overriding administration and then multiple ways of dividing the countries according to specific applications. For the majority of your practicals, we will be keeping it simple with a focus on London, which is divided into: Boroughs -&gt; Wards OR Boroughs –&gt; Middle Super Output Areas -&gt; Lower Super Output Areas -&gt; Output Areas. We’ll be looking at wards in our practical analysis – although even at this fine scale, the City of London is a little pesky and introduces complexities into our analysis, which we’ll see. We’ll learn more about Administrative Geographies next week. Population Datasets For our population datasets, we will use the ONS mid-year estimates (MYE). These population datasets are estimates that have been modelled based on the previous 2011 census count and then forecasted population growth (plus some additional data). They are released a year, with a delay of a year, i.e. we can only access data for 2019 at the moment, so we’ll use this as our most recent year. As the London Datastore only has these MYE for up to 2018, we’ll need to download the data from ONS directly. It’s always worth checking the ‘originators’ of the data to see if there are more recent versions. Navigate to the Ward level datasets: https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/wardlevelmidyearpopulationestimatesexperimental When you navigate to this page, you will find multiple choices of data to download. We will need to download the estimates for 2011, 2015 and 2019. Click to download each of the zipfiles. Choose the revised versions for 2015 and the (Census-based) on 2011 wards edition for 2011. In your GEOG0030/data/raw/ folder, create a new folder called population and copy the three spreadsheets into this folder. Now it’s time to do some quite extensive data cleaning. Cleaning our Population Datasets If you open up the 2011 ward spreadsheet in Excel (or another spreadsheet program: this could be Numbers or you can upload your data to use it with Google Docs, for example), you’ll quickly see that there are several worksheets to this workbook. We are interested in the Mid-2011 Persons. Click on the Mid-2011 Persons tab and have a look at the data. As you should be able to see, we have a set of different fields (e.g. Ward Code, Ward Name), including population statistics. Right now, we have too much data - so what we will want to do is simplify and extract only the data we need for analysis. For this, we need the total population (All Ages), alongside some identifying information that distinguishes each record from one another. Here we can see that both Ward Code and Ward Name suit this requirement. We can also think that the Local Authority column might be of use - so it might be worthwhile keeping this information as well. Create a new spreadsheet within your program. From the Mid-2011 Persons spreadsheet, copy over all cells from columns A to D and rows 4 to 636 into this new spreadsheet. Row 636 denotes the end of the Greater London wards (i.e. the end of the Westminster LA) which are kept (in most scenarios) at the top of the spreadsheet as their Ward Codes are the first in sequential order. Before we go any further, we need to format our data. First, we want to rename our fields to remove the spaces and superscript formatting. Re-title the fields as follows: ward_code, ward_name, local_authority and POP2011. One further bit of formatting that you MUST do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave this commas in our values, Q-GIS will read them as decimal points, creating decimal values of our population. There are many points at which we could solve this issue, but the easiest point is now - we will strip our population values of the commas and set them to integer (whole numbers) values. To format this column, select the entire column and right-click on the ‘D’ cell. Click on Format Cells and set the Cells to Number with 0 decimal places. You should see that the commas are now removed from your population values. Save your spreadsheet into your working folder as ward_population_2011.csv. We now need to copy over the data from the 2015 and 2019 datasets as well into their own csvs. Open the Mid-2015 ward population xls from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2015 Persons tab. We now need to copy over the data from our 2015 dataset to a new spreadsheet again. However, At first instance, you’ll notice that the City of London wards are missing from this dataset. Then if you scroll to the end of the London Local Authorities, i.e. to the bottom of Westminster, what you should notice is that the final row for the Westminster data is in fact row 575 - this means we’re missing nearly other LAs in addition to our COL LAs and we will need to determine which ones are missing and if we can find them in the 2015 spreadsheet. With this in mind, first copy from row 5 to the end of the grouped London Local Authorities, i.e. to the bottom of Westminster, for columns A to D into a new spreadsheet. Through a quick scroll of the Local Authorities, a.k.a as Boroughs, (and with the extensive knowledge that you will soon build about London Local Authorities!) we can quickly find that we are missing the wards for: Hackney Kensington and Chelsea Tower Hamlets. If we head back to the original 2015 raw dataset, we can actually find this data (as well as the City of London) further down in the spreadsheet. It seems like these LAs had their codes revised in the 2014 revision and are no longer in the same order as the 2011 dataset - oh, the joys of using data! Locate the data for the City of London, Hackney, Kensington and Chelsea and Tower Hamlets and copy this over into our new spreadsheet. Double-check that you now have in total 636 wards within your dataset. Remember to rename the fields as above, but change your population field to POP2015. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2015.csv. We now need to repeat this for our 2019 data. I wonder what surprises this dataset has in store for us! Open the Mid-2019 ward population spreadsheet from your population folder. As you’ll see again, there are plenty of worksheets available - again, we want to select the Mid-2019 Persons tab. Let’s have a look at our data - once again, there’s a lot to take in - but what we’re interested is in columns A, B, and now D and G. Let’s follow the same process we used above to copy our data across. To make our processing easier, first hide columns C, E and F in our spreadsheet - right-click on the columns at select Hide. Next, copy the data from row 5 to the final row for the Westminster data for columsn A, B, D and G over into a new spreadsheet. Look at the total number of rows you’ve copied over. We can see that we have even fewer wards than the 2015 dataset - yikes! We need to go hunting again for our missing data in the 2019 dataset. For expediency, you need to find and copy over the data for: City of London Hackney Kensington and Chelsea Tower Hamlets (as per 2015) and Bexley Croydon Redbridge Southwark Perhaps now you see why so much time is spent on processing data for spatial analysis! Copy over the remaining wards for these Local Authorities/Boroughs. Once you’ve copied them over - you should now have 640 wards - delete columns C, E and F and rename the remaining fields as you have done previously. Remember to reformat your population values. Once complete, save your spreadsheet into your working folder as ward_population_2019.csv. You should now have your three population csv datasets in your working folder. We’re now ready to start using our data within Q-GIS. Using Q-GIS to map our population data We will now use Q-GIS to create population maps for the wards in London across our three time periods. To achieve this, we need to join our table data to our spatial datasets and then map our populations for our visual analysis. Because, as we have seen above, we have issues with the number of wards and changes in boundaries across our three years, we will not (for now) complete any quantitative analysis of these population changes - this would require significant additional processing that we do not have time for today. *Data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to Administrative Geographies. In our extension activity Extension: Population as a Raster Dataset we show how we can complete this calculation easily when we use raster data that has a standardised grid format.* If you do not have access to Q-GIS, please click here to go to the alternative option: Week 2 Practical Alternate: Using AGOL for Population Mapping Start Q-GIS If you are not familiar with the Q-GIS environment, please watch our short video that explains its main components: Let’s start a new project. Click on Project –&gt; New. Save your project into your qgis folder as w2-pop-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. We will explain CRSs and using CRSs in GIS software v. programming in more detail next week. We will first focus on loading and joining the 2011 datasets. Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your 2011 boundary files. Here, we will select the London_Ward.shp dataset: Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. We can take a moment just to look at our Ward data - and recognise the shape of London. Can you see the City of London in the dataset? It has the smallest wards in the entire London area. With the dataset loaded, we can now explore it in a little more detail. We want to check out two things about our data: first, its Properties and secondly, its Attribute Table. The following short video explains these main components to using spatial data within Q-GIS. Right-click on the London_Ward layer and open the Attribute Table and look at how the attributes are stored and presented in the table. Explore the different buttons in the Attribute Table and see if you can figure out what they mean. Once done, close the Attribute Table. Right-click on the London_Ward layer and select Properties. Click through the different tabs and see what they contain. Keep the Properties box open. Before adding our population data, we can make a quick map of the wards in London - we can add labels and change the symbolisation of our wards. In the Properties box, click on the Symbology tab - this is where we can change how our data layer looks. For example, here we can change the line and fill colour of our Wards utilising either the default options available or clicking on Simple Fill and changing these properties directly. Keep the overall styling to a Single Symbol for now - we’ll get back to this once we’ve added the population data. You can also click on the Labels tab - and set the Labels option to Single labels. Q-GIS will default to the NAME column within our data. You can change the properties of these labels using the options available. I’ll add a thin buffer to my labels and change the font to Futura and size 8. You can click Apply to see what your labels look like. In my case, incredibly busy!: As its very busy, you may actually want to remove the labels from your dataset for the remaining processing - but hopefully this helps you understand how to add simple labels to your data. We’ll show you some more complex approaches in Week 10. Click OK once you’re done changing the Symbology and Label style of your data to return to the main window. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using Q-GIS right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 shapefile. First, we need to add the 2011 population data to our project. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text Layer. Click on the three dots button again and navigate to your 2011 population data in your working folder. Your file format should be set to csv. You should have the following boxes clicked: Decimal separator is comma; First record has field names’ Detect field types; Discard empty fields. Q-GIS does many of these by default, but do double-check! Set the Geometry to No geometry (attribute only table). Then click Add and Close*. You should now see a table added to your Layers box. We can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datatsets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. Right-click on your London_Ward layer –&gt; Properties and then click on the Joins tab. Click on the + button. Make sure the Join Layer is set to ward_population_2011. Set the Join field to ward_code. Set the Target field to GSS_code. Click the Joined Fields box and click to only select the POP2011 field. Click on the Custom Field Name Prefix and remove the pre-entered text to leave it blank. Click on OK. Click on Apply in the main Join tab and then click OK to return to the main Q-GIS window. We can now check to see if our join has worked by opening up our London_Ward Attribute Table and looking to see if our wards now have a Population field attached to it: Right-click on the London_Ward layer and open the Attribute Table and check that the population data column has been added to the table. As long as it has joined, you can move forward with the next steps. If your join has not worked, try the steps again - and if you’re still struggling, do let us know. Now, the join that you have created between your ward and population datasets in only held in Q-GIS’s memory. If you were to close the program now, you would lose this join and have to repeat it the next time you opened Q-GIS. To prevent this from happening, we need to export our dataset to a new shapefile - and then re-add this to the map. Let’s do this now: Right-click on your London_Ward shapefile and click Export –&gt; Save Vector Layer as... The format should be set to an ESRI shapefile. Then click on the three dots buttons and navigate to your final folder and enter: ward_population_2011 as your file name. Check that the CRS is British National Grid. Leave the remaing fields as selected, but check that the Add saved file to map is checked. Click OK. You should now see our new shapefile add itself to our map. You can now remove the original London_Ward and ward_population_2011 datasets from our Layers box (Right-click on the layers –&gt; Remove Layer). The final thing we would like to do with this dataset is to style our dataset by our newly added population field to show population distribution around London. To do this, again right-click on the Layer –&gt; Properties –&gt; Symbology. This time, we want to style our data using a Graduated symbology. Change this option in the tab and then choose POP2011 as your column. We can then change the color ramp to suit our aesthetic preferences - Viridis seems to be the cool colour scheme at the moment, and we’ll choose to invert our ramp as well. The final thing we need to do is classify our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation. We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on the drop-down next to Mode, select Natural Breaks, change it to 7 classes and then click Classify. Finally click Apply to style your dataset. A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using Q-GIS, which makes it a good playground to see how certain parameters or settings can change your output. You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Load the respective Ward dataset as a Vector Layer Load the respective Population dataset as a Delimited Text File Layer (remember the settings!) Join the two datasets together using the Join tool in the Ward dataset Properties box. Export your joined dataset into a new dataset within your final folder. Style your data appropriately. To make visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with Q-GIS, so for now, you can leave your symbolisation to the default settings. Alternatively, to set all three datasets to the same breaks, you can do the following: Right-click on the ward_population_2019 dataset and navigate to the Symbology tab. Double-click on the Values for the smallest classifcation group and set the Lower value to 141 (this is the lowest figure across our datasets, found in the 2015 data). Click OK, then Click Apply, then Click OK to return to the main Q-GIS screen. Right-click again on the ward_population_2019 dataset but this time, click on Styles –&gt; Copy Styles –&gt; Symbology. Now right-click on the ward_population_2015 file, but this time after clicking on Styles –&gt; Paste Style –&gt; Symbology. You should now see the classification breaks in the 2015 dataset change to match those in the 2019 data. Repeat this for the 2011 dataset as well. The final thing you need to do is to now change the classification column in the Symbology tab for the 2015 and 2011 datasets back to their original columns and press Apply. You’ll see when you first load up their Symbology options this is set to POP2019, which of course does not exist within this dataset. And that’s it - you can now make direct visual comparisons against your three maps. As you’ll be able to see, population has grown considerably in the London wards and there is are a few spatial patterns to this. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint: Click on Project –&gt; Import/Export –&gt; Export to Image and save your final map in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) QGIS, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. Acknowledgements Part of this page is adapted from CASA0005 and Introduction to GIS by Manuel Gimond. "],["cartography-and-visualisation-i.html", "3 Cartography and Visualisation I", " 3 Cartography and Visualisation I Welcome to Week 3 in Geocomputation! Well done on making it through Week 2 - and welcome to what is a more practical introduction to GIScience where we will be focusing on: how to make a good map. It’s not quite as “light” as promised, but this and the previous week will hold you in good stead as you come to learn about more technical analytical techniques after Reading Week. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. If you do not get through everything this week, do not worry. Week 4 will be shorter in content, therefore you will have time to catch up after the seminars at the start of Week 4. The seminar will go through aspects of this week’s work, so it will still be incredibly useful if you do not manage to complete everything we outline in this workshop. Week 3 in Geocomp Video on Stream This week’s content introduces you to foundational concepts associated with Cartography and Visualisation, where we have three areas of work to focus on: Map Projections Data Visualisation The Modifiable Areal Unit This week’s content is split into 4 parts: Coordinate Systems and Map Projections (40 minutes) Effective Data Visualisation (40 minutes) The Modifiable Areal Unit Problem (20 minutes) Practical 2: Mapping Crime Across London Wards and Boroughs (1 hour) Videos can be found in Parts 1-3, alongisde Key and Suggested Reading. This week, your 1 assignment is creating the final output from our practical. Part 4 is our Practical for this week, where you will be introduced to using the Map Composer with Q-GIS and apply the knowledge gained in the previous parts from Parts 1-3 in a practical setting. If you have been unable to download Q-GIS or cannot access it via Desktop@UCL Anywhere, we have provided an alternative browser-based practical but we recommend reading through the Q-GIS practical as unfortunately we are unable to repeat everything within the AGOL practical. Learning Objectives By the end of this week, you should be able to: Explain what a Geographic Coordinate System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. We will build on the data analysis we completed last week and create accurate maps that show changes in crime across our London wards. Coordinate Systems and Map Projections Maps, as we saw last week, are representations of reality. But not only are they are designed to represent features, processes and pheonomena in their ‘form’, they also need to represent, with fidelity, their location, shape and spatial arrangement. To be able to locate, integrate and visualise spatial data accurately within a GIS system or digtal map, spatial data needs to have two things: 1. A coordinate reference system (often written as CRS) 2. An associated map projection A CRS is a reference system that is used to represent the locations of the relevant spatial data within a common geographic framework. It enables spatial datasets to use common locations for co-location, integration and visualisation. Each coordinate system is defined by: Its measurement framework Unit of measurement (typically either decimal degrees or feet/metres, depending on the framework) Other measurement system properties such as a spheroid of reference, a datum, and projection parameters Its measurement framework will be one of two types: Geographic: in which spherical coordinates are measured from the earth’s center Planimetric: in which the earth’s coordinates are projected onto a two-dimensional planar surface. For planimetric CRS, a map projection is required. This projection details the mathematical transformation to project the globe’s three-dimensional surface onto a flat map. As a result, there are two common types of coordinate systems that you will come across when using spatial data: 1. Geographic Coordinate Systems (GCS): a global or spherical coordinate system such as latitude-longitude. 2. Projected Coordinate System (PCS): a CRS which has the mechanisms to project maps of the earth’s spherical surface onto a two-dimensional Cartesian coordinate plane. These PCS are sometimes reference to as map projections, although combine both location and the projection in their use. Understanding Coordinate Systems Slides | Video on Stream In summary, a GCS defines where the data is located on the earth’s surface, whereas a a PCS tells the data how to draw on a flat surface, like on a paper map or a computer screen. As a result, a GCS is spherical, and so records locations in angular units (usually degrees). Conversely, a PCS is flat, so it records locations in linear units (usually meters): Visualising the differences between a GCS and a PCS. Image: Esri For a GCS, graticules are used as the referencing system, which are tied directly to the Earth’s ellipsoidal shape. In comparison, within a PCS, a grid is a network of perpendicular lines are used, much like graph paper, which are then superimposed on a flat paper map to provide relative referencing from some fixed point as origin. Your data must have a GCS before it knows where it is on earth. But, whilst theoretically projecting your data is optional, projecting your map is not. Maps are flat, so your map will have a PCS in order accurately draw the data. In most GIS systems, a default projection will be used to draw the map and therefore the system will project your data to match this projection. For example, if you do not specify the projection of the map or data, both ArcGIS and Q-GIS will draw your map and corresponding data using a pseudo Plate Carrée or ‘geographic’ projection. The Plate Carrée Projection This projection is actually just latitude and longitude represented as a simple grid of squares and called pseudo because it is measured in angular units (degrees) rather than linear units (meters). It is easy to understand and easy to compute, but it also distorts all areas, angles, and distances, so it is senseless to use it for analysis and measurement and as a result, before you start your work, you should choose a different PCS! Which CS you will choose will depend on where you are mapping: most often, you will not need to choose a GCS as the data you are using was already collected and/or stored in a pre-selected system. For example, all GPS receivers collect data using only one datum or coordinate system, which is WGS84. Therefore any GPS data you use will be provided in the WGS84 GCS. However, you will often need to choose your PCS: which PCS you use depends on where you are mapping, but also the nature of your map — for example, should you distort area to preserve angles, or vice versa? For example, if you are using GPS data from the U.K, it is likely that you will transform this data into British National Grid (a PCS). Understanding Map Projections Either CS provides a framework for defining real-world locations - however, when it comes to much of GIScience and spatial analysis work, we will use a PCS to help locate, project, analyse and visualise our data in 2D. To locate, project, analyse and visualise our data in 2D, the PCS has, through mathematical transformations known as map projections, transformed the surface of our three-dimensional earth into a two-dimensional map canvas (whether paper or digital). This ability to create a flat surface from a 3D sphere is however not so simple! From a classic geographical metaphor, the easiest way to think about this is to think about peeling an orange - how could you peel an orange to ultimately result in a flat (preferably square/rectangular - computers really like squares!) shape? Well, luckily, you don’t need to think too hard about it - as Esri’s resident cartographer John Nelson (another Twitter recommendation) has done it for us: Trying to flatten an orange - our earth - into a flat map. Images: John Nelson, Esri As he shows, to create just a flat version of our earth from the spheriod itself, it takes some very interesting shapes and direction maniuplation - let alone achieving a rectangle! (You can see the original blog post these images are taken from here.) To create a classic square or rectangular map that we are so used to seeing, we have to use other geometric shapes that can be flattened without stretching their surface to help determine our projection. These shapes are called developable surfaces and consist of three types: Cylindrical Conical Plane The three types of projection families: cyclindrical, conical and plane. Image: QGIS However when any of using these shapes to representing the earth’s surface in two dimensions, there is always some sort of distortion in the shape, area, distance, or direction of the data. This distortion is explained through Vox’s excellent video: Why all world maps are wrong We can actually test out this distortion ourselves. You can head to The True Size (https://thetruesize.com) and see how our use of the Web Mercator has skewed our understanding of the size of countries in respect to one another. In addition, I highly recommend looking through this short (2 minutes!) blog post where a keen mapper got creative with his own orange peel: Blog post: Visualising the distortion of web mercator maps with an orange peel, Chris M. Whong, Online here Different projections can therefore cause different types of distortions. Some projections are designed to minimize the distortion of one or two of the data’s characteristics. A projection could, for example, maintain the area of a feature but alter its shape. Our second short lecture explains how to think through choosing a map projection: Choosing a Map Projection Slides | Video on Stream As explained in our lecture, each map projection therefore has advantages and disadvantages. Ultimately, the best projection for a map depends on the scale of the map, and on the purposes for which it will be used. As the excellent Q-GIS Projection documentation explains: For example, a projection may have unacceptable distortions if used to map the entire African continent, but may be an excellent choice for a large-scale (detailed) map of your country. The properties of a map projection may also influence some of the design features of the map. Some projections are good for small areas, some are good for mapping areas with a large East-West extent, and some are better for mapping areas with a large North-South extent. When it comes to choosing your map projection, think about: Is there a default projection for your area of study (e.g. London and British National Grid)? What analysis are you completing? What properties are important to this analysis? At what scale and direction are you visualising your data? What is critical to remember though, is that map projections are never absolutely accurate representations of our spherical earth. As a result of the map projection process, every map shows distortions of angular conformity, distance and area. Why should we care about projection systems? In summary, the projection system you use can have impact on both analytical aspects of your work, e.g. using measurement tools effectively, such as buffers, alongside visualisation. It is usually impossible to preserve all characteristics at the same time in a map projection. This means that when you want to carry out accurate analytical operations, you will need to use a map projection that provides the best characteristics for your analyses. For example, if you need to measure distances on your map, you should try to use a map projection for your data that provides high accuracy for distances. Furthermore, you need to be aware of the CS that your data is in, particularly when you are using multiple datasets. In order to analyse and visualise data accurately together, they must all be in the same CS. Transforming/Reprojecting Data If you are using datasets that are based on different geographic or projected coordinate systems, you will need transform all your data to one singular system: these are known as transformations. Between any two coordinate systems, there may be zero, one, or many transformations. Some geographic coordinate systems do not have any publicly known transformations because that information is considered to have strategic importance to a government or company. For many GCS, multiple transformations exist. They may differ by areas of use or by accuracies. Accuracies will usually reflect the transformation method. A geographic transformation is always defined in a particular direction, like from NAD 1927 to WGS 1984. Transformation names will reflect this: NAD_1927_To_WGS_1984_1. The name may also include a trailing number, as the above example has _1. This number represents the order in which the transformations were defined. A larger number does not necessarily mean a more accurate transformation. Even though a geographic transformation has a built-in directionality, all transformation methods are inversible. That is, a transformation can be used in either direction. Moving for with CRS in Geocomputation Keep in mind that map projection is a very complex topic. There are hundreds of different projections available that aim to portray a certain portion of the earth’s surface as accurately as possible on a digital screen/flat paper. In reality, the choice of which projection to use will often be made for you. When it comes to geocomputation and spatial analysis, you need to choose your CRS carefully - thinking through what is appropriate for your dataset, incuding what analysis you are completing and at what scale. You will find there are specific recommendations by country and, fortunately for us, most countries have commonly used projections. This is particularly useful when data is shared and exchanged as people will follow the national trend. Often, most countries will utilise the relevant zone within the Universal Transverse Mercator. In addition, a great resource is Esri’s documentation on Choosing a Map Projection. The Tyranny of Web Mercator One thing to watch out for though is the general (over)reliance on what is known as the Pseudo-Mercator projection (EPSG:3857) by web applications such as Google Maps. The projected Pseudo-Mercator coordinate system takes the WGS84 coordinate system and projects it onto a square. (This projection is also called Spherical Mercator or Web Mercator.) This method results in a square-shaped map but there is no way to programmatically represent a coordinate system that relies on two different ellipsoids, which means software programs have to improvise. And when software programs improvise, there is no way to know if the coordinates are consistent across programs. This makes EPSG:3857 great for visualizing on computers but not reliable for data storage or analysis. Luckily for us in Geocomputation, for the majority of our work, we will be using the British National Grid for our mapping and analysis as we are focusing on analysis on London. In this week’s practical, we will look at how we can reproject our spatial data from one a GCS to a PRS (in this case WGS84 to OSGB1936). Key Reading(s) Book (30 mins): Longley et al, 2015, Geographic Information Science &amp; Systems, Chapter 4: Geo-referencing. Optional: The Power of the Map Maps and map projections have had a long and complicated history with our politics and geopolitics. For example, whilst maps have existed in many forms prior to the periods, we cannot ignore their signficant use for land acquisition and resource exploitation during the “Age of Discovery” and resulting colonialism eras. There is significant power embedded within a map and, even to this day, as we see with the use of the Mercator projection in web technology, a map can be a substantial propaganda tool when it comes to political issues. Google Maps, for example, has found itself at the centre of various border disputes across the world - resulting, in several occasions, with troop mobilisation and threats of war: By misplacing a portion of the border between Costa Rica and Nicaragua, Google effectively moved control of an island from one country to the other and was cited as the justification for troop movements in the region in 2010. The Washington Post, 2020 To further avoid this, Google has created a new techno-political approach within its Google Maps platform in that the world’s borders will look different depending on where you’re viewing them from. You can read more about this a recent article by The Washington Post: Google redraws the borders on maps depending on who’s looking (10 minutes). Maps therefore are never true representations of reality, but will always include some bias - after all, maps are still very much made by humans. Whilst we won’t cover this in any more detail in our lecture or practical content this week, we do hope you enjoy discussing these issues in your Study Group sessions. In addition, there are many excellent books on this power of maps, including Denis Wood’s The Power of Maps and follow-up, Rethinking the Power of Maps and Mark Monmonier’s How to Lie with Maps. These books all outline how both paper and modern digital maps offer opportunities for cartographic mischief, deception, and propaganda. If you’d like to avoid reading for a little longer, I would also highly recommend this excerpt from the “before your time” show, the West Wing, which summarises quite a few of the debates well: Effective Data Visualisation In addition to choosing the correct map projection for your spatial data and map, to visualise your data correctly as a map - for visual analysis and publishing - you need to consider: How you represent your spatial data effectively. How you present this data on a map that communicates your data and analysis accurately. We will first focus on the latter aspect and look at how you can achieve effective data visualisation, including how to make a good map as well as detailing the common cartographic conventions we’d expect you to include in your map. Then we look at common types of spatial data and focus on how we can accurately represent event and survey data that are commonly aggregated to areal units (such as the Administrative Geographies we came across last week) for use within choropleth maps. Cartographic Conventions Making a good map is a highly subjective process - what you think looks good versus what someone else thinks looks good maybe entirely different. That’s why there is a whole discipline out there on cartography - it’s also why good data visualisation skills are becoming essential within data scientist roles. As a result, I can highly recommend taking the Cartography and Visualisation module by Prof James Cheshire next year! At its most fundamental, a map can be composed of many different map elements. They may include: The main map Map graticules A legend (including symbols) A title A scale bar or indicator An orientation indicator, i.e. a North Arrow An inset map (to locate your map within a wider area) Data Source information Any ancillary information These elements are all part of the expected cartographic conventions, i.e. what should be included on/within your map in order to accurately convey all the information contained within your visualisation. Map elements. Image: Manuel Gimond However, not all elements need to be present in a map at all times. In fact, in some cases they may not be appropriate at all. A scale bar, for instance, may not be appropriate if the coordinate system used does not preserve distance across the map’s extent. Knowing why and for whom a map is being made will dictate its layout: If it’s to be included in a paper as a figure, then simplicity and restraint should be the guiding principles. If it’s intended to be a standalone map, then additional map elements may be required, such as customised borders, graphics etc. Knowing the intended audience should also dictate what you will convey and how: If it’s a general audience with little technical expertise then a simpler presentation may be in order. If the audience is well versed in the topic, then the map may be more complex. Ultimately, to make a good map there are several rules you can follow: Visual hierarchy: Making sure the most important elements are the most visible on the map (e.g. size, placement on map, colour scheme). Colour schemes: Keeping colour schemes simple (less than 12 colour at max) and representative of the data you are showing (more on this later) as well as suitable to all audiences (e.g. being aware of mixing colours indetectable to those colourblind/visually impaired) Scale bars and north arrows: Should be used judiciously! They are not needed in every map, nor do they need to be extremely large - just readable. I advise trying to locate the two together and keeping their design as simple as possible. Title and other text elements: Again, less is more! Never use “A map of…” in your title - we know it’s a map! Keep font choices simple and reflective of the topic you are mapping. Titles are not needed on maps with figure captions. Make legends readable - including simplifying their values. Utilise font size effectively to ensure communication of the most important aspects. The following short lecture explains in more detail how to make a good map: Cartographic Conventions and Effective Data Visualisation Slides | Video on Stream Representing Spatial Data The second aspect of creating effective maps is to ensure that you are representing the type of data you are using effectively and accurately. As we saw last week, spatial data itself is only a representation of reality. Some of the types of data we use can be very close representations of reality, such as ‘raw’ geographic data (including satellite imagery or elevation models), whilst other datasets, when used in maps, may be far abstract representations of reality. The different common types of spatial data you might come across in spatial analysis are outlined in the table below: Common Types of Spatial Data Data Type Examples Digital Representation ‘Raw’ Geographic Data Satellite Imagery LIDAR/RADAR imagery Environmental Measurements (e.g. elevation, air quality, water levels) Raster/Grids Coordinates / Point Data, with attributes Processed or Derived Spatial Data Geographic Reference Data (e.g. buildings, roads, rivers, greenspace) Gridded Population (Density) Data Digital Elevation Models Air Quality Maps Points, Lines and Polygons Raster/Grids (Spatial) Event (Count) Data Human Activities ( e.g. crime, phone calls, house sales) Scientific Recordings (e.g. animal and plant sightings) Coordinates / Point Data, with attributes Statistical Survey or Indicator Data Human Characteristics (e.g. demographic, socio-economic &amp; health information) Scientific Recordings (e.g. total animal counts, leaf size measurements) Voting Tabular Data, representative at a specific spatial aggregate scale, i.e. areal unit Whilst we will come across a variety of these types of spatial data on this course, our main focus for the first few weeks are looking at Event and Statistical data - because these are the two types of data that are primarily used within the most common data visualisation map tool: a choropleth map. Choropleth Maps At its most basic, a choropleth map is a type of thematic map in which a set of pre-defined areas is colored or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population density or crime rate. When using either Event Data or Statistical Data, we tend to aggregate these types of data into areal units, such as the Administrative Geographies we came across last week, in order to create these choropleth maps. Because we see choropleth maps in our everyday lives, choropleth maps, I would say, out of any type of map-based data visualisation are the maps most vulnerable to poor use and data representation. We often think it’s a simple case of linking some table data with our areal units and then choosing some pretty colour scheme… An Example Choropleth: London’s Wasted Heat Energy at the MSOA scale. The question is: do you think it looks good? What would you change? Image: Mapping London …However, within a choropleth map, many decisions need to be made in terms of thinking through their classification (categorical or continuous/graduated), the ‘class breaks’ used, as well as the type of colour schemes used. Furthermore, a key challenge to using choropleth maps is that often the areal units we use are not of equal area - as a result, we have to be careful in how we represent our chosen dataset. Showing population as a ‘raw’ geographic fact across London Wards as we did last week, for example, would actually be a big no-no in terms of mapping population. Instead, we would want to show the population density - by normalising our population by the area of each ward. What’s still missing from this map? London Ward Population Density 2019. Data: ONS Without taking these normalisation approaches, we can create incredibly misleading maps. At the most basic, our brain sees the larger areal units within our map as having more of whatever quantity we are representing, irrespective of thinking through the underlying area (and/or population) it is actually representing. This was common amongst the US election maps, for example, where many of the Republican states have a large landmass - but ultimately a low population. Therefore, when representing the results of the election as a categorical choropleth, it presents an overwhelming Republican landslide. However, as we all know, whilst the Party won the Electoral College vote, the Democrats actually won the Popular Vote by 3 million votes. Hence, when mapping by number of votes rather than state outcome, a different message is conveyed, as we see below. Alas, despite this difference in total votes, the US runs an Electoral College System and in the end, the winner is the winner of the Electoral College vote and no map coud or can change that! Different approaches to mapping the 2016 election result in different information communicated (L-&gt;R: Business Insider, Time, xkcd) Despite their various challenges, choropleth maps can be increidbly useful tools. We provide a more detailed introduction to how to create choropleth maps in the following lecture: An Introduction to Choropleth Maps Slides | Video on Stream The Modifiable Areal Unit Problem The final aspect of good map-making we will cover actually focuses on how we process and resultantly analyse our data when we aggregate individual event or statistical data to areal units. When using choropleth maps to represent aggregated data, there are three key analytical challenges you need to be aware of, in order to not fall into the “trap” of the first two, whilst also thinking about ways to address the allter. The are three key challenges: Ecological Fallacy (EF): EF occurs when you try to make inferences about the nature of individuals based on the group to which those individuals belong (e.g. administrative unit). This applies when looking at correlations between two variables when using administrative geographies or looking at averages within these units. Whilst your areal unit may represent the aggregation of individual level data, you can not apply your findings from the analysis of this map to the individuals directly. You can only apply your conclusions to the area that you have aggregated by, e.g. at the Ward scale. The Modifiable Areal Unit Problem (MAUP): Spatial data is scale dependent - when data are tabulated according to different zonal systems at different scales and are then analyzed, it is unlikely that they will provide consistent results - even though the same variables are used and the same areas are ultimately analyzed. As a result, the results from your analysis are only relatable to those precise areal units used. This variability or inconsistency of the analytical results is mainly due to the fact that we can modify areal unit boundaries and thus the problem is known as the MAUP. It is one of the most stubborn problems in spatial analysis when spatially aggregated data are used. Fundamentally, you cannot extrapolate your findings at one scale to another, i.e. any conclusions drawn at the Ward level in London cannot be applied to the Borough level, even though, for example, your wards may “fit” within the Borough scales. Boundary Issues: Spatial data does not have “boundaries” - the use of artifical boundaries such as Administrative Geographies are indiscriminate to the spatial prcoesses that may actually underline the distribution of these phenomena at study. As a result, simply using these boundaries per se can bring about different spatial patterns in geographic phenomena - or simply disregard them in their entirety. We have to use Administrative Boundaries with care and think about the underlying processes we are trying to measure to see if we can account for these discriminatory issues. In summary, whenever you conduct spatial analysis using areal units – you cannot infer about the individuals within those units nor can you assume your findings will apply at coarser scales. You also need to take into account the “decisive and divisive” nature the use of areal units can have on individual level data when aggregating. We will begin to look at MAUP in this week’s practical and Week 4’s seminar and continue accounting for and considering its impact over the next few weeks of our analysis. A more detailed introduction to Administrative Geographies As we read and saw last week, an administrative geography is a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. In the modern spatial analysis, we use administrative geographies to aggregate individual level data and individual event data. One of the motivations for this is the fact that census data (and many other sources of socio-economic and public health data) are provided at specific administrative levels, whilst other datasets can often be easily georeferenced or aggregated to these levels. Furthermore, administrative geographies are concerned with the hierarchy of areas – hence we are able to conduct analyses at a variety of scales to understand local and global trends. Generally, they contain 4-5 levels of administrative boundaries, starting at Level 0, with the outline of the country, Level 1, the next regional division, Level 2, the division below that etc. Each country will have a different way of determining their levels and their associated names – and when you start to add in differentiating between urban and rural areas, it becomes a whole new level of complexity. What is important to know is that these geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. For any country in which you are using administrative geographies, it is good practice therefore to research into their history and how they have changed over the period of your dataset. For the U.K, we can access the spatial data of our Administrative Geographies from data.gov.uk (and a few other sources). Any country with their own statistics or spatial office should have these datasets available. If not, you can find data (for pretty much all countries) at gadm.org, which allows you to download and use the data for non-commercial purposes. As a note of interest at this point, in the U.K., it is generally understood that for publishable research, we do not analyse data at a smaller scale than something called the Lower Super Output Area (LSOA). There is another administrative unit below the LSOA, known as the Output Area, which (again due to ensure confidentiality of data) has a minimum size of 40 resident households and 100 resident people but for particular types of research, this level of detail can still lead to unintended consequences, such as households being identified within the data. Practical 2: Mapping Crime Across London Wards and Boroughs The first half of this workshop has given you an in-depth introduction into how we can create a successful map, including understanding map projections, cartographic conventions and issues faced with the analysis of aggregated data at areal units. The practical component of the week puts some of these learnings into practice as we analyse crime rates within London at two different scales. The datasets you will create in this practical will be used in the Week 4 practical, so make sure to follow every step and export your data into your working and final folders (respectively) at the end. The practical component introduces you to point-in-polygon counts. You’ll be using these counts throughout this module, so it’s incredibly important that you understand how they work – even as simple as they may be! If you can’t access Q-GIS for this practical… For those of you who have been unable to access Q-GIS through your own computer or Desktop@UCL Anywhere, we have provided an alternative browser-based practical, which requires you to sign-up for a free but temporary account with ArcGIS Online. You will first need to complete this first half of the practical on this page - there is a link later on in our practical to the alternate tutorial at the point at which you’ll need to switch. Setting the scene: why investigate crime in London? Over the next few weeks, we will look to model driving factors behind crime across London from both a statistical and spatial perspective. As Reid et al (2018) explain: Spatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime. Crime mapping allows researchers and practitioners to explore crime patterns, offender mobility, and serial offenses over time and space. Within the context of local policing, crime mapping provides the visualization of crime clusters by types of crimes, thereby validating the street knowledge of patrol officers. Crime mapping can be used for allocating resources (patrol, specialized enforcement) and also to inform how the concerns of local citizens are being addressed. Mapping crime and its spatial distribution is of significant interest to a variety of stakeholders - it also serves as a relatable and understandable geographical phenomena for learning different types of spatial analysis techniques as well as many of the ‘nuances’ analysts face when using this type of ‘event’ data. As a result, within this practical, we are actually going to answer a very simple question: Does our perception of crime (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the ‘ecological fallacy’ mistake of assuming patterns at the ward level are the same at the borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. To test this, we will use these two administrative geographies (borough and ward) to aggregate crime data for London in 2020. Here we will be looking specifically at a specific type of crime: the theft from a person. Finding our datasets As we saw last week, accessing data within the UK, and specifically for London, is relatively straight-forward - you simply need to know which data portal contains the dataset you want! Crime Data For our crime data, we will use data directly from the Police Data Portal, which you can find at https://data.police.uk/. This Data Portal allows you to access and generate tabular data for crime recorded in the U.K. across different the different Police Forces since 2017. In total, there are 45 territorial police forces (TPF) and 3 special police forces (SPF) of the United Kingdom. Each TPF covers a specific area in the UK (e.g. the \"West Midlands Police Force), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a speciic area, we need to know which Police Force covers the Area of Interest (AOI) for our investigation. When you look to download crime data for London, for example, there are two territorial police forces working within the city and its greater metropolitan area: The Metropolitan Police Force (The Met), which covers nearly the entire London area, including Greater London The City of London (COL) Police, which covers the City of London. The Met has no juridiction in the COL. You therefore need to decide if you want to include an analysis of crime in the City of London or not - we will in our current study. We’ll get to download this dataset in a second! Population Data After what we’ve learnt about above, we know that if we want to study a phenomena like crime (and aggregate it to an areal unit as we will do today!), we will need to normalise this by our population. Luckily, we already have our Ward Population sorted from last week, with our ward_population_2019.shp that should be currently sitting in your final data folder. If it is not, you can download our shapefile here. Remember to unzip it and, for now, store it in your final data folder. In addition to our ward level dataset, we also want to generate the same type of shapefile for our London boroughs, i.e. a borough_ward_population_2019.shp, utilising the same approach as last week, joining our population table data to our borough shape data. To do this, we need to know where to get both our required datasets from - luckily, you’ve already got borough shape data in your raw/boundaires/2011 folder. Therefore, it is just a case of tracking down the same Mid-Year Estimates (MYE) for London Boroughs as we did for the wards, which with the ONS’s central MYE database, this also won’t be too difficult! So let’s get going! Download and process datasets As outlined above, to get going with our analysis, we need to download both the population data for our boroughs and the 2020 crime data for our two police forces in London. Let’s tackle the population data first. 1) Borough Population Through a quick search, we can find our Borough Population table data pretty much in the same place as our Ward data - however it is a separate spreadsheet to download. Navigate to the data here. Download the Mid-2019: April 2020 local authority district codes xls. Open the dataset in your spreadsheet editing software. Navigate to the MYE2-Persons tab. Utilising your preferred approach, extract: Code, Name, Geography and All ages data for all London boroughs. For me, the simplest way is to add a filter to row 5, and from this filter, in the Geography column select only London Boroughs: You should have a total of 33 boroughs. Once you have your 33 boroughs separated from the rest of the data, copy the columns (Code, Name, Geography and All ages) and respective data for each borough into a new csv. Remember to format the field names as well as the number field for the population as we did last week. Save as a new csv in your working population folder: borough_population_2019.csv. 2) Ward Population As mentioned above, you should have a ward_population_2019.shp file within your final data folder. As we’ll be using this dataset in our practical, we would like to make sure that we keep a version of this data in its current state, just in case we make a mistake whilst processing our dataset. As a result, we should create a copy of this dataset within our working folder, that we can use for this practical. Copy and paste over the entire ward_population_2019.shp from your final data folder to your working data folder. Don’t forget to copy over ALL the files. 3) Crime Data We will now head to the Police Data Portal and download our crime data… …or maybe not! As I said at the start of last week’s practical: We’re going to start cleaning (the majority of) our data from the get-go. However, with our crime data, the processing that is required from you right now is exhaustive to do manually - and far (far!) easy to do using programming. Essentially, all of our data that we will download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: For our data processing therefore, you would need to merge all of this crime into a single csv. Now you could do this manually by copying and pasting each csv into a new csv (24 times) - or you can do it through a few lines of code. However, you’ve already read through a lot today, so we’ll save learning Command Line tools for next week, where we’ll find out just how quick it can be to merge csvs! Instead, you can find the pre-merged and pre-filtered spreadsheet here. Note, I filtered the data to only contain data on theft crime, rather than all types of crime in London. There are however a few caveats in our crime data, that we’ll explain below - but these might not become clear until you start using the raw dataset yourself next week. For now, make sure you have downloaded the london_crime_theft_2020 csv linked here. Copy this csv into a new folder in your raw data folder called: crime. Downloading and using crime data from data.police.uk To download data for all of London for 2020, you follow these simple steps: As you can see, it is a simple process of selecting the Police Forces and months for which you want data for - and then a csv for each of these will be generated. 1) Data Structure Once downloaded, you can open up the csv to see what the data contains. Each crime csv contains at least 9 fields: Field(s) Meaning Reported by The force that provided the data about the crime. Falls within At present, also the force that provided the data about the crime. Longitude and Latitude The anonymised coordinates of the crime. LSOA code and LSOA name References to the Lower Layer Super Output Area that the anonymised point falls into, according to the LSOA boundaries provided by the Office for National Statistics. Crime type One of the crime types used to categorise the offence. Last outcome category A reference to whichever of the outcomes associated with the crime occurred most recently. Context A field provided for forces to provide additional human-readable data about individual crimes. For us, the main fields we are interested include: Longitude and Latitude (for plotting as points) LSOA code/name (for aggregating to these units without plotting) Crime Type (to filter crime based on our investigation) 2) Location Anonymisation When mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime — not the exact place that it happened. This displacement occurs to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household. Interestingly enough, the police also convert the data from their recorded BNG eastings and northings into WGS84 latitude and longitude ( hence why we’ll need to re-project our data in this practical). 3) Coding of Crimes into 14 Categories Each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. We can use these crime types to filter our crime specific to our investigation - in our case theft. Now we have all our data ready, let’s get mapping! Using Q-GIS to map our crime data If you do not have access to Q-GIS, please click here to go to the alternative option: Week 3 Practical Alternate: Using AGOL for Crime Mapping Start Q-GIS Click on Project –&gt; New. Save your project into your qgis folder as w3-crime-analysis. Remember to save your work throughout the practical. Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on Project –&gt; Properties – CRS. In the Filter box, type British National Grid. Select OSGB 1936 / British National Grid - EPSG:27700 and click Apply. Click OK. Compared to last week, you should now know what EPSG:27700 means! Shortcut to CRS on Q-GIS To access and set the project CRS quickly in Q-GIS, you can click on the small CRS button in the bottom-right corner in Q-GIS: Now we have our Project CRS set, we’re now ready to start loading and processing our data. Load Ward Population data Click on Layer –&gt; Add Layer –&gt; Add Vector Layer. With File select as your source type, click on the small three dots button and navigate to your ward_population_2019.shp in your working folder. Click on the .shp file of this dataset and click Open. Then click Add. You may need to close the box after adding the layer. Load Borough shape and population data and join! We now need to create our Borough population shapefile - and to do so, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Load the respective Borough dataset as a Vector Layer (found in your raw data folder -&gt; boundaries -&gt; 2011 -&gt; London_Borough_Excluding_MHW.shp). Load the respective Population dataset as a Delimited Text File Layer (Remember the settings, including no geometry! This one is found in your working folder) Join the two datasets together using the Join tool in the Borough dataset Properties box (remember which fields to use, which to add and to remove the prefix - look back at last week’s instructions if you need help). Export your joined dataset into a new dataset within your working folder: borough_population_2019.shp. Make sure this dataset is loaded into your Layers / Added to the map. Remove the original Borough and population data layers. Load and project our crime data We now are ready to load and map our crime data. We will load this data using the Delimited Text File Layer option you would have used just now to load the borough population - but this time, we’ll be adding point coordinates to map our crime data as points. Click on Layer –&gt; Add Layer –&gt; Add Delimited Text File Layer. With File select as your source type, click on the small three dots button and navigate to your all_theft_2019.shp in your raw -&gt; crime folders. Click on the .csv file of this dataset and click Open. In *Record and Fields Options**, ensure it is set to CSV, untick Decimal separator is comma and tick First record has field names, Detect field types and Discard empty fields. In Geometry Definition, select Point coordinates and set the X field to Longitude and the Y field to Latitude. The Geometry CRS should be: EPSG:4326 - WGS84, a.k.a. the GCS of lat and lon! Click Add. But WAIT! We are using the wrong CRS for our project?! Surely, we need everything to be in BNG? As you click Add, you should see that you get a pop-up from Q-GIS asking about transformations - we read about these earlier and they are the mathematical algorithms that convert data from one CRS to another. And this is exactly what Q-GIS is trying to do. Q-GIS knows that the Project CRS is BNG but the Layer you are trying to add has a WGS84 CRS. Q-GIS is asking you what transformation it should use to project the Layer in the Project CRS! This is because one key strength (but also problem!) of Q-GIS is that it can project \"on the fly - what this means is that Q-GIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use. But you must note that this transformation is only temporary in nature and as a result, it is not a full reprojection of our data. Map Projections in Q-GIS The following is taken from the Q-GIS’s user manual section on Working with projections. Every project in QGIS also has an associated Coordinate Reference System. The project CRS determines how data is projected from its underlying raw coordinates to the flat map rendered within your QGIS map canvas. By default, QGIS starts each new project using a global default projection. This default CRS is EPSG:4326 (also known as “WGS 84”), and it is a global latitude/longitude based reference system. This default CRS can be changed both permanently, for example, to British National Grid for all future projects, or for that specific project, as we have done in our two practicals. QGIS supports “on the fly” CRS transformation for both raster and vector data. This means that regardless of the underlying CRS of particular map layers in your project, they will always be automatically transformed into the common CRS defined for your project. Behind the scenes, QGIS transparently reprojects all layers contained within your project into the project’s CRS, so that they will all be rendered in the correct position with respect to each other! These reprojections are only temporary and are not permanently assigned to the dataset it is reprojecting - only to the project. As a result, we should be aware of this when using data across different projects and/or GIS systems and always remember what the data’s original or “true” CRS is! This reprojection is also using computer memory, therefore, if you are to analyse large datasets (such as our crime dataset), it makes sense to reproject our data to have it permanently in the same CRS as our project. For now, let’s use the on-the-fly projection for now and utilise Q-GIS’s recommendation of the +towgs84=446.448.... transformation. This transformation should be built-in to your Q-GIS transformation library, whereas some of the more accurate options would need installation. For now, given the displacement of our data in the first place, this transformation is accurate enough for us! Click to use the +towgs84=446.448.... transformation and click through the OKs to return to the main Q-GIS screen. You should now see your crime dataset displayed on the map: We can test the ‘temporary’ nature of the projection by looking at the CRS of the all_theft_2020 layer: Right-click on the all_theft_2020 layer then select Properties -&gt; Information and then look at the associated CRS. You should see that the CRS of the layer is still WGS84. Yup, Q-GIS is definitely projecting our data on-the-fly! We want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the same CRS as our administrative datasets, i.e. British National Grid. This also means we’ll have the dataset to use in other projects, just in case. Back in the main Q-GIS window, click on Vector -&gt; Data Management Tools -&gt; Reproject Layer. Fill in the parameters as follows: Input Layer: all_theft_2020 Target CRS: Project CRS: EPSG: 27700 Reprojected: Click on the three buttons and Save to File to create a new data file. Save it in your working folder as all_crime_2019_BNG.shp Click Run and then close the tool box. You should now see the new data layer added to your Layers. Q-GIS can be a little bit buggy so when it creates new data layers in your Layers box, it often automates the name, hence you might see your layer added as Reprojected. It does this with other management and analysis tools, so just something to be aware of! For now, let’s tidy up our map a little. Remove the all_theft_2020 original dataset. Rename the Reprojected dataset to all_theft_2020. Now we have an organised Layers and project, we’re ready to start our crime analysis! Counting Points-in-Polygons with Q-GIS The next step of our analysis is incrediby simple - as Q-GIS has an in-built tool for us to use. We will use the Count Points in Polygons in the Analysis toolset for Vector data to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Click on Vector -&gt; Analysis Tools -&gt; Count Points in Polygons. Within the toolbox, select the parameters as follows: Polygons: borough_population_2019 Points: all_theft_2020 (Note how both our data layers state the same CRS!) No weight field or class field Count field names: crimecount Click on the three dot button and Save to file: working -&gt; borough_crime_2020.shp Click Run and Close the box. You should now see a Count layer added to your Layers box. Let’s go investigate. Click the checkbox next to all_theft_2020 to hide the crime points layer for now. Right-click on the Count layer and open the Attribute Table. You should now see a crimecount column next to your POP2019 column. You can look through the column to see the different levels of crime in the each borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software. Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. To get our crime rate statistic, we’re going to do our first bit of table manipulation in Q-GIS, woohoo! With the Attribute Table of your Count layer still open, click on the pencil icon at the start of the row. This pencil actually turns on the Editing mode in Q-GIS. The editing mode allows you to edit both the Attribute Table values and the geometry of your data. E.g. you could actually move the various vertex of your boroughs whilst in this Editing mode if you like! When it comes to the Attribute Table, it means you can directly edit existing values in the table or create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake. Using the Editing mode is the correct approach to editing your table, however, it might not always be the approach you use when generating new fields and, as we all are sometimes, a little lazy. (This may be a simple case of “Do what I say, not what I do!”) Let’s go ahead and add a new field to contain our Crime Rate. Whilst in the Editing mode, click on New Field button (or Ctrl+W/CMD+W) and fill in the Field Parameters as follows: Name: crime_rate Comment: leave blank Type: Decimal number Length: 10 Precision: 0 Click OK. You should now see a new field added to our Attribute Table. What did all this mean? Understanding how to add new fields and their parameters rely on you understanding the different data types we covered last week - and thinking through what sort of data type your field needs to contain. In our case, we will store our data as a decimal to enable our final calculation to produce a decimal (an integer/integer is likely to produce a decimal) but we will set the precision to 0 to have zero places after our decimal place when the data is used. That’s because ultimately, we want our crime rate represented as an integer because, realistically, you can’t have half a crime! Calculating a decimal however will allow us to round-up within our calculations. The empty field has NULL populated for each row - so we need to find a away to give our boroughs some crime rate data. To do this, we will calculate a simple Crime Rate using the Field Calculator tool provided by Q-GIS within the Attribute Table. We will create a crime rate that details the number of crimes per 10,000 people in the borough. In most cases, a crime rate per person will create a decimal result less than 1 which not only will not be stored correctly by our crime_rate field but, for many people, a decimal value is hard to interpret and understand (yes, I know, but we are aiming to make maps that are accessible to everyone…). Therefore going for a 10,000 person approach allows us to calculate and represnt the crime rate using full integers for both our borough and ward scales as we’ll see later. This calculation was determined by a bit of a trial and error by me within this practical, so it is something you’d need to consider and change for future research you might do! Whilst still in the editing mode, click on the Abacus button (Ctrl + I / Cmd + I), which is actaully the Field Calculator. A new pop-up should load up. We can see there are various options we could click at the top - including Create a new field. Ah! So we could in fact create a new field directly from the field calculator which would help us combine these two steps in one and quicken our workflow! For now, in the Field Calculator pop-up: Check the Update existing field box. Use the drop-down to select the crime_ratefield. In the Expression editor, add the following expression: ( “crimecount” / “POP2019” ) * 10000 You can type this in manually or use the Fields and Values selector in the box in the middle to add the fields into the editor. Once done, click OK. You should then return to the Attribute Table and see our newly populated crime_rate field - at the moment, we can see the resulting calculations stored as decimals. Click on the Save button to save these edits - you’ll see the numbers turn to integers. Click again on the Pencil button to exit Editing mode. We now have a crime_rate column to map! Before moving to the next step, if you would like, go ahead and symbolise your boroughs by this crime_rate. Tips for Symbolisation When in the Symbology tab and after selecting Graduated as your symbolisation option, click on the histogram tab and load the values to see the distribution of your data. You can also edit the lines of the borough to a colour of your choice. You should also make sure your new borough crime rate layer has been renamed from the default Count layer name Q-GIS has given it. Rename your borough crime rate layer has been renamed from the default Count layer name to Borough Crime Rate. Great! We now have our Borough crime rate dataset ready for mapping and analysis - we just now need to repeat this process to have our Ward dataset. Repeat the above processes to create a crime_rate column within our Ward dataset ready for mapping and analysis. Tips for Repetition in Q-GIS Remember, you can use the field calculator straight away to shorten the field creation process by selecting to create a new field whilst completing the field calculation (still using the same parameters though!). One additional small tip is that in the middle box in the Field Calculator, you can load Recent field calculations and double-click on your prior calculation to automate the creation of the crime_rate calcuation! Now you have both datasets ready, it’s time to style the maps! Remember to use the Properties box to first symbolise your maps. Think through using the appropriate colour scheme - and perhaps have a look online for some examples, if you don’t want to use the defaults! Once you’re happy with their symbolisation, we’ll turn them into proper publishable maps using Q-GIS’s Print Layout. Making our Crime Rate Maps for analysis in Q-GIS and the Print Layout To create proper publishable maps in Q-GIS, we use what Q-GIS calls its Print Layout window (formely Map Composer). If you’ve ever used ArcMap, this is similar to switch the view of your map canvas to a print layout within the main window - but in Q-GIS’s case, it loads up a new window. From the main Q-GIS window, click on Project -&gt; New Print Layout. In the small box that first appears, call your new print layout: crime_map_borough_ward A new window should appear. Oh great, another tool I need to learn how to use… well yes, but learning how to use the Print Layout window will help you make great maps. A short introduction to the Print Layout As the Q-GIS documentation on Print Layout explains: The print layout provides growing layout and printing capabilities. It allows you to add elements such as the QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. You can size, group, align, position and rotate each element and adjust their properties to create your layout. The layout can be printed or exported to image formats, PostScript, PDF or to SVG. Initially, when opening the print layout provides you with a blank canvas that represents the paper surface when using the print option. On the left-hand side of the window, you will find buttons beside the canvas to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item. On the right-hand side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. For our practical today, we’re most interested in the bottom panel as Layout will control the overall look of our map, whilst Item properties will allow us to customise the elements, such as Title or Legend, that we may add to our map. In the bottom part of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level, the number of selected items if applicable and, in the case of atlas generation, the number of features. In the upper part of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar. Getting started with creating our map Working with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. To start with creating a map, you use the Add Map tool to draw a box in which a snapshot of the current active map you have displayed in your Q-GIS main window will be loaded. Let’s try this now: Click on the Add Map tool and draw a box in the first half of our map to load our current map, in my case, Ward Crime Rate: Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc. As you can see, the map currently does not look that great - we could really do with zooming in, as we do not need all of the white space. With your map selected, head to the Items Properties panel and look for the Scale parameter. Here we can manually edit the scale of our map to find the right zoom level. Have a go at entering different values and see what level you think suits the size of your map. Keep a note of the scale, as we’ll need this for the second map we’ll add to our map layout - our borough map. Next, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map. In the same panel, note down the size of your map - we want to make sure the next map we add is of the same size. Note, if you need to move the position of the map within the box, look for the Move Item Content tool on the left-hand side toolbar. Once you are done, finally click on the Lock Layers and Lock Style for layers. By locking the Layers (and their symbology) in our map, it means we can change our data/map in our main Q-GIS window without changing the map in the Print Layout - as we’ll see in a minute when adding our Borough Crime Rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main Q-GIS window. Now we’ve added our first map to our Map Layout, we want to add a Legend for this specific map. Click on the Add Legend tool and again, draw a box on your map in which your legend will appear. As you’ll see, your Legend auto-generates an entry for every layer in our Layers box in the main Q-GIS application: In Item Properties, uncheck auto-update - this stops Q-GIS automatically populating your legend as it has done current, and enables you to customise your legend. First, let’s rename our Layer in the legend to: Ward Crime Rate (per 10,000 people). Next, we want to remove all othe Layers, using the - button We can also customise the Legend further, including type, size and alignment of font - go ahead and style your legend as you would prefer. Move the Legend to an appropriate part of the layout near your Ward Crime Rate map - resize if necessary. Now we are finished with the Ward map, we want to make sure we don’t change any aspect of its layout. To do so, we need to lock both the Map and Legend in the Items panel - this prevents us accidently moving items in our layout. Note, this is different to locking your layers in the Items Properties as we did earlier. In the Items panel, click the Lock checkbox for both our map and legend. Once locked, we can now start to add our Borough map. In the main Q-GIS window, uncheck your Ward_crime_rate layer and make sure your Borough_crime_rate layer is now visble. Return to the Print Layout window. Repeat the process above of adding a map to the window - this time, you should now see your Borough map loaded in the box (and you should see no changes to your Ward map). Place your Borough map next to your Ward map - use the snap grids to help. Set your Borough map to the same zoom level as your Ward map. Make sure your Borough map is the same size as your Ward map. Set your Borough to the same extent as your Ward map (extra neatness!). Add a frame if you want. Lock your layer and its symbology in the Items Properties once ready and the lock your layer in the Items panel. We now just need to add a second legend for our Borough map. If we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there is a difference in the values, we need to have two legends. The Extension Activity to this practical, on the other hand, may result in a single legend - as a clue to help complete the activity! Repeat the process as above to add a Legend for our Borough map. Remember to re-title the Legend to make it more legible/informative. Match the same formatting for a clean look. Once complete, lock these two items in the Items panel as well. Locking/Unlocking and Visibility If you hadn’t noticed in the Items panel, you have the ability not only to lock/unlock different items (unlock to edit any items again), but also turn on/off the visibility of your layers: Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on the left-hand tool bar: Add a scale bar: use the Item Properties to adjust the Style, number of segments, font etc.. Add a north arrow: draw a box to generate the arrow and then use the Item Properties to adjust. I typically place the two side by side - and you can select both, right-click and group so you can then treat them as a single item when moving them around the page. Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Once you have added these properties in, you should have something that looks a little like this: Export map We are finally ready to export our map! To export your map to a file go: Layout -&gt; Export as Image and then save in your maps folder as London_2020_Crime_Rate.png. You can also export your map as a PDF. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in Q-GIS. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. This is all possible using the field calculator in Q-GIS, but will require some thinking about the right expression. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "],["programming-for-data-analysis-using-r-and-r-studio.html", "4 Programming (for Data Analysis) using R and R-Studio", " 4 Programming (for Data Analysis) using R and R-Studio Welcome to Week 4 in Geocomputation! Well done on making it through Week 3 - and welcome to our introduction to using programming, in the form of R and R-Studio, for data analysis. This week is heavily practical oriented - with many aspects of your practical integrated at various points in the workshop - as well as, of course, a data analysis section towards the end. As always, we have broken the content into smaller chunks to help you take breaks and come back to it as and when you can over the next week. Week 4 in Geocomp Video on Stream This week’s content introduces you to the foundational concepts associated with Programming for Data Analysis, where we have three areas of work to focus on: General principles of programming How to use R and R-Studio effectively for programmatical data analysis The ‘Tidyverse’ philosophy This week’s content is split into 4 parts: An Introduction to Programming (40 minutes) Using R and R-Studio for Data Analysis (60 minutes) The Tidyverse Philosophy and Principles (40 minutes) Practical 3: Analysing Crime in 2020 in London (30 minutes) This week, we have a slightly different approach to our workflow structure, with a mixture of short lectures, instructional videos and activities to complete throughout each part. A single Key Reading is found towards the end of the workshop. This week, you have 1 assignment, which will be highlighted in the workbook. Part 4 is the main part of analysis for our Practical for this week, but you will find aspects of programming in Parts 1-3 that you will need to do in order to prepare our data for the final part. If you have been unable to download R-Studio Desktop or cannot access it via Desktop@UCL Anywhere, you will have access to our R-Studio Server website instead. Instructions on how to access this are provided below. Learning Objectives By the end of this week, you should be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R and produce a bar chart We will build on the data analysis we completed last week and look to further understand crime in London by looking at its prevalence on a month-by-month basis. An Introduction to Programming Programming is our most fundamental way of interacting with a computer - it was how computers were first built and operated - and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking - and not typing. However, programming and computer code underpin every single application that we use on our computers… or really any technological device. After all, programming is used for so many purposes and applications, that, we as users take for granted - from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages (and ways of using programming languages!) have developed - Wikipedia, for example, has a list of 50 different types of languages, although there is some overlap between many of these and some are used for incredibly niche activties. In general, the main programming languages that people focus on learning at the moment include: Top 10 programming languages and their applications according to DZone in 2017. Some can be used for a range of purposes – others are more specific, e.g. HTML for website building. There are also different ways in which programming languages work, which give some advantages over others. This is to do with how the code is written and ‘talks to the computer’ - behind our main programming languages there is something called a compiler that takes the code we write in our various programming languages and translates it into machine code, a.k.a. the code our computers know how to understand. This code is written purely in binary and, as a result, looks a lot different to the code we’ll be writing in our practicals (think a lot of 1s and 0s!). For some languages, this translation is completed when your code is compiled before it is run, i.e. the ‘compiler’ will look through all your code, translate to machine code and then execute the machine code according. These languages are known as compiled, or low-level languages and can, at first, be slow to write but are incredibly efficient when executing huge amounts of code (e.g. when creating software). They however require an understanding of things called registers, memory addresses, and call stacks and are, as a result, a lot more complicated to learn and use (and no, I personally do not know how to code in any low-level languages…nor do I particularly want to!). For other languages, such as R and Python, these fall into the interpreted language category. Here, each line of code is executed without a pre-runtime translation. In this case, a program called an interpreter reads each program statement and, following the program flow, then decides what to do, and does it. The issue with these high-level programming languages is that this approach can be costly in computational resources (i.e. processing time and memory space). As there is no pre-run time compilation, bugs are not found before the code is run but instead as the code is run - as a result (and what you might see happen in your own code), your computer can get stuck trying to execute code which is either completely unfeasible for your computer to execute (e.g. your computer cannot handle the size of data you are feeding it) or it ends up in a loop with no way out - except for you stopping the code. However, the advantage of using these languages is that their main focus is on things like variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts - all of which we’ll use within our module, believe it or not! These languages have a primary focus on usability over optimal program efficiency, which, when we’re just learning to code, are ideal for us in Geocomputation! Don’t worry if you don’t understand what any of this means, you will do by the end of this module! As we’re not taking a Computer Science degree here, we won’t go into any more detail about this, but suffice to say, there is a lot more to programming then what we’ll cover in Geocomputation. But what is important to recognise is that a lot of work went into creating the programming environments that we get to use today - and I, for one, am extremely glad I never had to learn how to write a compiler! If understanding a little more about compilers and machine code is of interest to you, the below video provides an accessible explanation - although you might want to come back to it at the end of the practical: How do computers read code? The Command Line Interface The most basic of programming we can use without installing anything on our computers is using the Command Line Interface(CLI) already built in, known as the shell. The shell is a simple program that lets you interact with your computer using text commands (Command Line Interface) instead of a point &amp; click Graphical User Interface (GLI). The shell simply takes your commands and provides them to the operating system of your computer. Each operating system has its own shell program: Mac / Linux = zsh (new) / bash (Bourne-again Shell) (previous) Microsoft = PowerShell (and a few others). For most operating systems, you can access the shell using a window-based program, known as a terminal emulator (TE). The default TE for Mac &amp; Linux users is: Terminal. The default TE for Windows users use Command Prompt (old) or Terminal (new). (If you remember my introductory lecture, this is how I used to have to interact with my first computer at a very young age of probably 5 when attempting to load a computer game!) The shell is an incredibly useful program - and if you take programming further in the future (e.g. analysis of big datsets through servers, running multiple scripts, dealing with version control, software engineering), it will become a tool that you’ll become incredibly familiar with. But for now, we just want to illustrate how cool it can be for us to be able to tell our computer to do things in a few lines of code - rather than having to click and point - particularly once you know how to use the CLI-shell and can remember specific commands. Let’s take a look by completing a simple task with our shell to tell our computer to do something - let’s do some file management for our practical today. Using the Command Line Interface On Your Computer As you may remember from last week’s practical, I provided you with our crime data in a single processed csv file. However, when you download the data from data.police.uk, the data is not so nicely formatted! Essentially, all of our data that we download for crime in London will be provided in individual csvs, according first to month, and then to the police force as so: To be able process this data easily in R, we want to move all of our files into a single folder. And believe it or not, it only takes a couple of lines of code to do so. Let’s take a look: Using the Command Line to copy and move files. Video on Stream As we’ve seen in the video above, it can really useful - and quick - to use our Shell to organise our data files prior to loading them in R-Studio. Just to proove this, the first piece of programming we will do today is use your built-in shell on your computer to repeat the same process and copy our crime data into a single folder. File Management using the Command Line Now you’ve watched how I copy the files over, let’s go ahead and do this ourselves. Head to data.police.uk and download all available crime data for London in 2020 (this may only include up until November) and for both Police Forces. The below video shows you how to do this. You should now have your data downloaded in a single main folder in your downloads (it may be given a ridiculously long coded filename). Copy this folder into your GEOG0030 -&gt; data -&gt; raw -&gt; crime folder using copy and paste / drag-drop (whatever your preference). Next, open up your shell on your computer. On mac, hold CMD and hit space to open up your search bar and type terminal, and a terminal window should appear. On windows, press the windows button and do the same. Alternatively search for shell or command prompt. With your shell open, we will navigate to your raw data folder and copy over the crime data into a single folder. To do so, we’ll use six main terminal commands: Common Shell/Terminal Commands pwd (both Mac and Windows): Print Working Directory - this will show us where we are in our computer’s file system. By default, you should be located at the “base” of your user file. dir (Windows) or ls (Mac): Lists all files in the directory you are currently located. cd (both Mac and Windows): Change into a new directory (i.e. folder) - this can be a single path ‘step’ or several steps to get to the directory you want. md (Windows) or mkdir (both Mac and Windows): Make a new directory (i.e. folder) where you are currently located. cp (both Mac and Windows): Copy content of a directory or specific file to a new location. This command can take wildcards to help search multiple folders at once, as we’ll see in our query. move (Windows) or mv (Mac): Move directory to a new destination. Let’s get going. In your shell, type the command pwd and press return. You should now see a file path appear under your command - the computer is telling you where you are currently located in your computer system. Next, type dir OR ls (OS-dependent) and press return. The computer now lists all of the folders in your directory that you can move into as a next step from your current folder. You now need to identify what folder your GEOG0030 work is contained in - and what your “path” is to get there as we now need to continue changing directories to get to our GEOG0030 folder. Next, type cd followed by the folder(s) you need to change into in order to get to your main GEOG0030 folder. Remember to press return to execute your code. In my case, the command is: cd Code. Each time you change folder, this folder is added to the file path next to your Prompt - have a quick look at this now. Keep going changing folders until you are in the folder that contains your downloaded crime data. Auto-Population of File Paths A tip is that your terminal can auto-populate your folder names for you when there is enough information for them to determine the unique folder. To do this, press tab on your keyboard. E.g. your crime folder is likely to be a long list of numbers and letters if you haven’t renamed it whilst copying it over to your raw folder. Therefore I recommend using this approach will save you time entering all these numbers. You can also change into this folder in one command, simply keep adding to your folder path as so: Type in (and use tab) cd GEOG0030/data/raw/crime/52c6b758bceaf2244fc1b6f93e85d7f00f234ccf/ and then press return. Note, if you are using a WINDOWS PC, you need to use a backslash (\\) in your file path, not a forward-slash. Note, you do not need a slash at the start of your file paths.* Once you are in the correct folder, we first want to make a new folder to contain all the crime csvs (without their current subfolder system): Type in mkdir all_crime and press return. If you now type dir or ls, you should now see the new folder listed within your current folder. Let’s go ahead and copy all of our csvs into this single folder. Type cp **/*.csv all_crime and press return. Again use a backslash ()) if on a Windows PC. This command uses the wildcard * to search for any file in any folder, as long as it has a .csv file type. Using wildcards is a very common programming tool and we are likely to use them in our practicals moving forward. You can also use them in searches on search engines such as Google! We can now change into our new folder and then list its contents to check that our files have moved across. Type cd all_crime and press return. Then type dir or ls. Check that you have all your files (either 22 or 24, depending on when you are completing this practical!). Great, we have our files all in a single folder which will make using them in R much easier. We’ll do one final thing - and that is move this folder out of this original and into the main crime folder. Still in the terminal, type: cd .. to take a step back into our police crime data folder. Next, to move our all_crime folder: (WINDOWS) move all_crime .. or (MAC): mv all_crime .. Finally, type cd .. and press return. You should now find yourself one step back in your file system in the main crime folder. We can check that our move worked by again listing the contents of the folder. Type dir or ls and press return. Check that your all_crime folder is now listed. ::: Great! You’ve just done your first bit of command line programming! Think about how quick was it to type those commands and get the files moved - rather than have to do all of that by hand. Of course it helped that I told you what commands to write - but the more time you spend with programming, the quicker (and more familiar) you will get at (with) coding and executing these commands. Command Line Paths Note, the use of .. in our two commands above means to take a step back in your file system path, as we did in both of these cases here (i.e. the ‘parent’ folder. In addition, two further commands to be aware of include: ~ (Mac) to denote the root or home directory, e.g. cd ~. In Windows, there is not really a shortcut for this. .: a single full-stop means “this folder”. The command line is just one aspect of programming - but we also want to have the ability to create and run scripts. Scripts are incredibily important for us when completing data anlaysis and, as such, we’ll look at the differences between the two as we start to use R/R-Studio for our data analysis today. One cool thing about the terminal is that we actually have the ability to create and run scripts just within the terminal itself. We can do this by opening up a text editor in our terminal to write a script in any programming language and then execute our script within the terminal. We execute our script by actually setting the terminal to that programming language and then calling the script. This all sounds extremely complicated - but it really is not once you’ve spent a bit of time working with the CLI. We can have a quick look here: Using the Command Line to create and run scripts (in Python). Video on Stream Whilst we could use this type of approach for data analysis for conducting actual spatial analysis, we won’t be doing so in Geocomptuation (you are probably happy to read!). This is because, quite frankly, the terminal is pretty limited in its display of maps and charts, a key output of our work here in Geocomputation, and general user-friendly functionality. In fact, we’d need to save our outputs to a file each time to go view them, which would end up being a pretty clunky workflow… Instead, what’s great is that we have several different types of software and Integrated Develoement Environments that bring the functionality of running scripts together with the visualisation capacity we like in our GIS software. For us in Geocomputation, our tool of choice for this is R-Studio. Using R and R-Studio for Data Analysis Before we go any further, what I want to make clear from this workshop - and the remainder of the module - is that programming using R and R-Studio is ultimately a tool we will use to complete specific tasks we need to do for our data analysis. There are a lot of different tools out there that you can use to achieve the same outcomes (as you’ve seen with Q-GIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools - more on this next week. With this tool though, there is a lot to learn about the principles and the theory behind programming languages. As evident above, whilst we could look at this in a lot of detail (there is a lot of theory behind programming which we just won’t cover - that’s for computer scientists), we will instead focus on the aspects most important to our use, which is covered in our main lecture video below: Principles of Programming for Data Analysis in the Programming for Data Analysis section. The second thing to make clear is that R and R-Studio are two different things: R is our programming language, which we need to understand in terms of general principles, syntax and structure. R-Studio is our Integrated Development Environment, which we need to understand in terms of functionality and workflow. Integrated Development Environment An Integrated Development Environment (IDE) is simply a complicated way of saying “a place where I write and build scripts and execute my code”. Nowadays, we have some really fancy IDEs that, when they know what language you are coding in, will highlight different types of code according to what they represent (e.g. a variable, a function) as well as try to proof-read/de-bug your code “on-the-fly” before you’ve even run it. R-Studio is definitely a very fancy IDE - as it offers a lot of functionality beyond just writing scripts and execute code as we’ll see over the coming weeks. As you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the R-Studio IDE (or really software now!) to execute this statisitcal programming. Together, R and R-Studio has grown into an incredibly success partnership of analytical programming language and analysis software - and is widely used for academic research as well as in the commercial sector. One of R’s great strength is that it is open-source, can be used on all major computer operating systems and is free for anyone to use. It, as a result, has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis. Believe it or not, the entire workbook you are reading right now has been created in R-Studio, utilising a mixture of programming languages, including R, HTML, CSS and Markdown. R-Studio has the flexibility to understand programming languages other than R (including Python!), whilst R can be deployed outside of the R-Studio environment in standalone scripts and other IDEs. However, for us, the partnership between R and R-Studio works pretty well for what we want to achieve - so this is what we’ll be using for the remainder of the Geocomputation module. How do I use R-Studio? Unlike traditional statistical analysis programmes you may have used such as Microsoft Excel or even SPSS, within the R-Studio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script (or similar document, more on this in Week 10), that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks. We can also use the R Console to execute simple instructions that do not need repeating - such as installing libraries or quickly viewing data (we’ll get to this in a second). In addition, R, its various graphic-oriented “packages” and R-Studio are capable of making graphs, charts and maps through just a few lines of code (you might notice a Plots window to your right in your R-Studio window) - which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted. Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and R-Studio. These include: It is broadly intuitive with a strong focus on publishable-quality graphics. It is ‘intelligent’ and offers in-built good practice – it tends to stick to statistical conventions and present data in sensible ways. It is free, cross-platform, customisable and extendable with a whole swathe of packages/libraries (‘add ons’) including those for discrete choice, multilevel and longitudinal regression, and mapping, spatial statistics, spatial regression, and geostatistics. It is well respected and used at the world’s largest technology companies (including Google, Microsoft and Facebook, and at hundreds of other companies). It offers a transferable skill that shows to potential employers experience both of statistics and of computing. The intention of the practical elements of this week is to provide a thorough introduction to R-Studio to get you started: 1. The basic programming principles behind R. 2. Loading in data from csv files, filtering and subsetting it into smaller chunks and joining them together. 3. Calculating a number of statistics for data exploration and checking. 4. Creating basic and more complex plots in order to visualise the distributions values within a dataset. What you should remember is that R/R-Studio has a steep learning curve, but the benefits of using it are well worth the effort. I highly recommend you take your time and think through every piece of code you type in - and also remember to comment your code (we’ll get to this in a bit!) . The best way to learn R is to take the basic code provided in tutorials and experiment with changing parameters - such as the colour of points in a graph - to really get ‘under the hood’ of the software. Take lots of notes as you go along and if you are getting really frustrated take a break! This week, we focus solely on using R and R-Studio (from now on, this may be simply denoted as R) for basic statistical data analysis. Next week, we will introduce using R for spatial (data) analysis - but there’s lots to get on with today to understand the fundamental principles of using R (and programming in general). Accessing R-Studio for Geocomputation You have two options for using R-Studio in this module. Using R-Studio Desktop: You should have installed this in Week 1 as per the software installation instructions. Using R-Studio Server: First sign in to the UCL VPN or UCL China Connect. To use R-Studio Server, open a web browser and navigate to: https://rstudio.data-science.rc.ucl.ac.uk/ Log in with your usual UCL username and password. You should see the RStudio interface appear. If it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the start a new session button. You can use either approach - but do recognise their may be some differences oh how our code appears. The code below has been created on an R-Studio Desktop Version 1.2.5033 and tested on the R-Studio Server. Note RStudio server will only work with an active VPN connection that links your personal computer into UCL’s network. Students in mainland China may want to use UCL China Connect. Students that use a Mac computer that is running on the latest version of MacOS (MacOS Big Sur), are advised to use Desktop@UCL as the Cisco AnyConnect VPN application may not work. If you are completely unable to access the server (e.g. your browser displays a This site can’t be reached message), it means that your VPN connection is not working correctly. Please ensure that your VPN is working correctly or use Desktop@UCL Anywhere instead. An Introduction to R-Studio and its interface Let’s go ahead and open R-Studio (Desktop or Server) and we’ll first take a quick tour of the various components of the R-Studio environment interface and how and when to use them: Introducing the R-Studio Interface Video on Stream As you’ve heard, R-Studio has various windows that you use for different purposes - and you can customise its layout dependent on your preference. When you first open R-Studio, it should look a little something like this: The main windows (panel/pane) to keep focused on for now are: Console: where we write “one-off” code, such as installing libraries/packages, as well as running quick views or plots of our data. Files: where our files are stored on our computer system - can help with checking file paths as well as file names, and general file management. Environment: where our variables are recorded - we can find out a lot about our variables by looking at the Environment window, including data structure, data type(s) and the fields and ‘attributes’ of our variables. Plots: the outputs of our graphs, charts and maps are shown here. Help: where you an search for help, e.g. by typing in a function to find out its parameters. You may also have your Script Window open, which is where we build up and write code, to a) keep a record of our work, b) enable us to repeat and re-run code again, often with different parameters. We will not use this window until we get to the final practical instructions. We’ll see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as ‘attributes’ (do not get confused here with the Attribute Table for Q-GIS) and data structures. Programming for Data Analysis Before we get started with using R-Studio, we first need to take a few steps back and address the bigger learning curve in the room, that is: How do I program?. As stated earlier, R/R-Studio is just a tool - but to use it, you need to understand how to write code in R effectively and, of course, accurately to get your analysis to work. This means we need to learn about and understand: Basic Syntax Data Structures and Types Functions and Libraries/Packages Object-Oriented Programming Here, we provide a short introduction to the basic principles of programming, with a focus on Object Oriented Programming. This is a video you might want to re-watch after completing today’s practical. Principles of Programming for Data Analysis Slides | Video on Stream In the above lecture, you heard about the different including: Syntax using variables and functions Importance of data types and data structures The role of packages/libraries in expanding R’s functionality And a brief introduction to Object-Oriented Programin (OOP) We can put some of these principles into action by testing some of R-Studio’s capability with some short pieces of coding now. Using the Console in R-Studio We’ll first start off with using R-Studio’s console to test out some of R’s in-built functionality by creating a few variables as well as a dummy dataset that we’ll be able to analyse - and to get familiar with writing code. Note, you might need to click on the console window to get it to expand - you can then drag it to take up a larger space in your R-Studio window. The video below provides an overview of the short tutorial with additional explanations, so if you’re already a bit stuck, you can watch this as you start to complete the following instructions. Using the Console in R-Studio for programming Video on Stream In your R-Studio console, you should see a prompt sign - &gt; to the left - this means we’re ready to start writing code (a bit like earlier in the shell). Error Warnings in the Console Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you see a &gt; on the left it means you can type in your next line, a + means that you haven’t finished the previous line of code. As will become clear, + signs often appear if you don’t close brackets or you did not properly finish your command in a way that R expected. In your console, let’s go ahead and conduct some quick maths - at their most basic, all programming langauges can work like calculators! Command Input Type in 10 * 12 into the console. # Conduct some maths 10 * 12 ## [1] 120 Once you press return, you should see the answer of 120 returned below. Great, you’ve now learnt how to enter code into the R-Studio console! Pretty similar to your computer’s CLI right?! Storing Variables But rather than use ‘raw’ or ‘standalone’ numbers and values, we primarily want to use variables that stores these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called creating an object and this object becomes stored as a variable. We do this by using the &lt;- symbol is used to assign the value to the variable name you have given. Let’s go ahead and try this. Let’s create two variables for experimenting with: 2.Type in ten &lt;- 10 into the console and execute. # Store our ten variable ten &lt;- 10 You have just created your first variable. You will see nothing is returned in the console - but if you check your Environment window, it has now appeared as a new variable that contains the associated value. Type in twelve &lt;- 12 into the console and execute. # Store our ten variable twelve &lt;- 12 Once again, you’ll see nothing returned to the console but do check your Environment window for your variable. We’ve now stored two numbers into our environment - and given them pretty good variable names for easy reference. R stores these objects as variables in your computer’s RAM so they can be processed quickly. Without saving your environment (we will come onto this below), these variables would be lost if you close R (or it crashes). Now we have our variables, let’s go ahead and do the same simple multiplication maths: Type in ten * twelve into the console and execute. # Conduct some maths again using our variables ten * twelve ## [1] 120 You should see the output in the console of 120 (of course..!). Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these variables can be treated in the same way as the values they contain. Next, type in ten * twelve * 8 into the console and execute. # Conduct some more maths with variables and raw values ten * twelve * 8 ## [1] 960 You should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable. Type output &lt;- ten * twelve * 8 into the console and execute. # Conduct some maths and store it as output output &lt;- ten * twelve * 8 As we’re storing the output of our maths to a new variable, the answer won’t be returned to the screen. Accessing and returning variables We can ask our computer to return this output by simply typing it into the console. Ask the computer to return the variable output. You should see we get the same value as the earlier equation. # Return the variable, output output ## [1] 960 Variables of different data types We can also store variables of different data types, not just numbers but text as well. Type in str_variable &lt;- \"This is our first string variable\" into the console and execute. # Store a variable str_variable &lt;- &quot;This is our 1st string variable&quot; We have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores “words” (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of the \" \". Let’s access our variable to see what is now stored by our computer. Type in str_variable into the console and execute. # Return our str_variable str_variable ## [1] &quot;This is our 1st string variable&quot; You should see our entire sentence returned - and enclosed in \"\". Again, by simply entering our variable into the console, we have asked R to return our variable to us. Calling functions on our variables We can also call a function on our variable. This use of call is a very specific programming term and generally what you use to say \"use\" a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to print our variable, which will give us the same output as accessing it directly via the console: Type in print(str_variable) into the console and execute. # Print str_variable to the screen print(str_variable) ## [1] &quot;This is our 1st string variable&quot; We have just used our first function: print(). This function actively finds the variable and then returns this to our screen. You can type ?print into the console to find out more about the print() function. # Gain access to the documentation for our print function ?print This can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output. In many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one argument. Returning functions When a function provides an output, such as this, it is known as returning. Not all functions will return an output to your screen - they’ll simply just do what you access them to do, so often we’ll use a print() statement or another type of returning function to check whether the function was successful or not - more on this later in the workshop. Examining our variables using functions Within the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is: Type in typeof(str_variable) into the console and execute. # Call the typeof() function on str_variable to return the data type of our variable. typeof(str_variable) ## [1] &quot;character&quot; You should see the answer: \"character\". As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables. Type in typeof(ten) into the console and execute. # Call the typeof() function on ten variable to return the data type of our variable. typeof(ten) ## [1] &quot;double&quot; You should see the answer: \"double\". As evident, our ten is a double data type. For high-level objects that involve (more complicated) data structures, such as when we load a csv into R as a data frame, we are also able to check what class our object is, as follows: Type in class(str_variable) into the console and execute. # Call the class() function on str_variable to return the object of our variable. class(str_variable) ## [1] &quot;character&quot; In this case, you’ll get the same answer - “character” - because, in R, both its class and type are the same: a character. In other programming languages, you might have had \"string\" returned instead, but this effectively means the same thing. Let’s try testing our ten variable: Type in class(ten) into the console and execute. # Call the class() function on ten to return the object of our variable. class(ten) ## [1] &quot;numeric&quot; In this case, you’ll get a different answer - \"numeric\" - because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our ten variable is an integer or not. Type in is.integer(ten) into the console and execute. # Test our ten variable by asking if it is an integer is.integer(ten) ## [1] FALSE You should see we get the answer FALSE - as we know from our earlier typeof() function, our variable ten is stored as a double and therefore cannot be an integer. Whilst knowing this might not seem important now, but when it comes to our data analysis, the difference of a decimal number vs. a whole number can quite easily add bugs into our code! We can incorporate these tests into our code when we need to evaluate an output of a process and do some quality assurance testing of our data analysis. We can also ask how long our variable is - in this case, we’ll find out how many different sets of characters (strings) are stored in our variable, str_variable. Type in length(str_variable) into the console and execute. # Call the length() function on str_variable to return the length of our variable. length(str_variable) ## [1] 1 You should get the answer 1 - as we only have one set of characters. We can also ask how long each set of characters is within our variable, i.e. ask how long the string contained by our variable is. Type in nchar(str_variable) into the console and execute. # Call the nchar() function on str_variable to return the length of each of our elements within our variable. nchar(str_variable) ## [1] 31 You should get an answer of 31. Creating a two-element object Let’s go ahead and test these two functions a little further by creating a new variable to store two string sets within our object, i.e. our variable will hold two elements. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute. # Store a new variable with two items using the c() function two_str_variable &lt;- c(&quot;This is our second string variable&quot;, &quot;It has two parts to it&quot;) In this piece of code, we’ve created a new variable using the c function in R, that stands for \"combine values into a vector or list. We’ve provided that function with two sets of strings, using a comma to separate our two strings - all contained within the function’s (). You should now see a new variable in your Environment window which tells us it’s a) chr: characters, b) contains 2 items, and c) lists those items. Let’s now try both our length() and nchar() on our new variable and see what the results are. # Call the length() function and nchar() function on our new variable length(two_str_variable) ## [1] 2 nchar(two_str_variable) ## [1] 34 22 Did you see a difference? You should have seen that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22. There is one final function that we often want to use with our variables when we are first exploring them, which is attributes() - as our variables are very simple, they currently do not have any attributes (you are welcome to type in the code and try) but it is a really useful function, which we’ll come across later on. # Call the attributes() function on our new variable attributes(two_str_variable) ## NULL We’ve had fun experimenting with simple variables in our console - and learnt about many new functions we can use with our code. In fact, we’ve learnt 7 functions - can you name/remember them all without scrolling up? If you can’t, I highly recommend taking notes on each of the functions - even if it is just a short list of the functions and what they do. We’re now going to move on to creating and analysing our dummy dataset - so fingers crossed you’ll remember these as we move forward. Using comments in our code In addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code I have written, I have provided an additional comment to explain what the code does. Comments are denoted using the hash symbol #. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code - as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments as you code and not leave it to do retrospectively - because I can tell you from experience - you most certainly will not. Whilst we are using the console, using comments is not necessary - but as we start to build up a script in our full practical, you’ll find them essential to help understand your workflow in the future! Analysing dummy data in R-Studio using the Console The objects we created and played with above are very simple: we have stored either simple strings or numeric values - but the real power of R comes when we can begin to execute functions on more complex objects. As we heard in our lecture, R accepts four main types of data structures: vectors, matrices, data frames, and lists. So far, we have dabbled with a single item or a dual item vector - for the latter, we used the c() function to allow us to combine our two strings together within a single vector. We can use this same function to create and build more complex objects - which we can then use with some common statistical functions. We’re going to try this out by using a simple set of dummy data: we’re going to use the total number of pages and publication dates of the various editions of Geographic Information Systems and Science (GISS) for our brief dummy analysis: Book Edition Year Total Number of Pages 1st 2001 454 2nd 2005 517 3rd 2011 560 4th 2015 477 As we can see, we will ultimately want to store the data in a table as above (and we could easily copy this to a csv to load into R if we wanted). But we want to learn a little more about data structures in R, therefore, we’re going to go ahead and build this table “manually”. Let’s get going. Clearing our Environemnt workspace First, let’s clear up our workspace and remove our current variables: Type rm(ten, twelve, output, str_variable, two_str_variable) into the console and execute. # Clear our workspace rm(ten, twelve, output, str_variable, two_str_variable) Note, of course you can either copy and paste this code - or try out using the tab function to autocomplete your variable names in the console as you start typing them in, just as we did when using the Command Line. You should now see we no longer have any variables in our window - we just used the rm() function to remove these variables from our environment. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later - but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment. Creating an atomic vector of multiple elements The first complex data object we will create is a vector. A vector is the most common and basic data structure in R and is pretty much the workhorse of R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types: Atomic vectors (all elements are of the same data type) Lists (elements can be of different data types) Although in practice the term “vector” most commonly refers to the atomic types and not to lists. The variables we created above are actually vectors - however they are made of only one or two elements. We want to make complex vectors with more elements to them. Let’s create our first official “complex” vector, detailing the different total page numbers for GISS: Type giss_page_no &lt;- c(454, 517, 560, 477) into the console and execute. # store our total number of pages, in chronological order, as a variable giss_page_no &lt;- c(454, 517, 560, 477) Let’s check the results. Type print(giss_page_no) into the console and execute. # print our giss... variable print(giss_page_no) ## [1] 454 517 560 477 We can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object: Type our various statistical functions (detailed below) into the console and execute. # calculate the arithmetic mean on our variable mean(giss_page_no) ## [1] 502 # calculate the median on our variable median(giss_page_no) ## [1] 497 # calculate the range numbers of our variable range(giss_page_no) ## [1] 454 560 We have now completed our first set of descriptive statistics in R! We now know that the average number of pages the GISS book has contain is 497 pages - this is of course truly thrilling stuff, but hopefully an easy example to get onboard with. But let’s see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note, I entered the total number of pages in a specific order to correspond to these publishing dates (i.e. chronological), as outlined by the table above. As a result, I’ll enter the publication year in the same order. Type giss_year &lt;- c(2001, 2005, 2011, 2015) into the console and execute. # store our publication years, in chronological order, as a variable giss_year &lt;- c(2001, 2005, 2011, 2015) Let’s check the results. Type print(giss_year) into the console and execute. #print our giss_year variable print(giss_year) ## [1] 2001 2005 2011 2015 Again, truly exciting stuff. Of course, on their own, the two vectors do not mean much - but we can use the same c() function to combine the two together to create a matrix. Creating a matrix from two vectors In R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both rows and columns. As with atomic vectors, the elements of a matrix must be of the same data type. As both our page numbers and our years are numeric (we can check this using which function?), we can add them together to create a matrix using the matrix() function: Type giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) into the console and execute. # create a new matrix from our two vectors with two columns giss_year_nos &lt;- matrix(c(giss_year, giss_page_no), ncol=2) # note the inclusion of a new argument to our matrix: ncol=2 #this stands for &quot;number of columns&quot; and we want two. Again, let’s check the results. Type print(giss_year_nos) into the console and execute. print(giss_year_nos) ## [,1] [,2] ## [1,] 2001 454 ## [2,] 2005 517 ## [3,] 2011 560 ## [4,] 2015 477 The thing about matrices - as you might see above - is that, for us, they don’t have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we wouldn’t really know what to do with it. As a result, we tend to primarily use Data Frames in R as they offer the opportunity to add field names to our columns to help with their intepretation. Arguments/Parameters in Functions The function we just used above, ‘matrix()’, was the first function that we used that took more than one argument. In this case, the arguments the matrix needed to run were: What data or dataset should be stored in the matrix. How many columns (ncol=) do we need to store our data in. The function can actually accept several more arguments - but these were not of use for us in this scenario, so we did not include them. For almost any R package, the documentation will contain a list of the arguments that the function will takes, as well as in which format the functions expects these arguments and a set of usage examples. Understanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function - and whether we will need to transform our variable into a different data structure to be used for that specific function. For any function, there will be mandatory arguments (i.e. it will not run without these) or optional arguments (i.e. it will run without these, as the default to this argument has been set usually to FALSE, 0 or NULL). Creating a Data Frame from our matrix A data frame is an extremely important data type in R. It is pretty much the de-facto data structure for most tabular data and what we use for statistics. It also is the underlying structure to the table data (what we would call the attribute table in Q-GIS) that we associate with spatial data - more on this next week. A data frame is a special type of list where every element of the list will have the same length (i.e. data frame is a “rectangular” list), Essentially, a data frame is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply NULL. Data frames can have additional attributes such as rownames(), which can be useful for annotating data, like subject_id or sample_id or even UID. In statistics, they are often not used - but in spatial analysis, these IDs can be very useful. Some additional information on data frames: They are usually created by read.csv() and read.table(), i.e. when importing the data into R. Assuming all columns in a data frame are of same type, a data frame can be converted to a matrix with data.matrix() (preferred) oras.matrix(). You can also create a new data frame with data.frame() function, e.g. a matrix can be converted to a data frame, as we’ll see below. You can find out the number of rows and columns with nrow() and ncol(), respectively. Rownames are often automatically generated and look like 1, 2, …, n. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset. Let’s go ahead and create a new data frame from our matrix: Type giss_df &lt;- data.frame(giss_year_nos) into the console and execute. # Create a new dataframe from our matrix giss_df &lt;- data.frame(giss_year_nos) We now have a data frame, we can finally use the View() function in R. Still in your console, type: View(giss_df) # View our data frame View(giss_df) You should now see a table pop-up as a new tab on your script window. It’s now starting to look like our original table - but we’re not exactly going to be happy with X1 and X2 as our field names - they’re not very informative. Renaming our column field names Instead, what we can do is rename our data frame column field names by using the names() function. Before we do this, have a read of what the names() function does. Still in your console, type: ?names # Get the help documentation for the names function ?names As you can see, the function will get or set the names of an object, with renaming occuring by using the following syntax: names(x) &lt;- value The value itself needs to be a character vector of up to the same length as x, or NULL. This is one of the cool aspects of OOP, in that we can access specific parts of our object and change it without changing the object as a whole or having to create a new object/variable to enact our changes. We have two columns in our data frame, so we need to parse our names() function with a character vector with two elements. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, new_names &lt;- c(\"year\", \"page_nos\"), and then the names() function containing this vector for renaming, names(giss_df) &lt;- new_names: # Create a vector with our new column names new_names &lt;- c(&quot;year&quot;, &quot;page_nos&quot;) #Rename our columns with our next names names(giss_df) &lt;- new_names You can go and check your data frame again and see the new names using either View() function or by clicking on the tab at the top. Adding a column to our data frame We are still missing one final column from our data frame - that is our edition of the textbook. As this is a character data type, we would not have been able to add this directly to our matrix - and instead have waited until we have our data frame to do so. This is because data frames can take different data types, unlike matrices - so let’s go ahead and add the edition as a new column. To do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our data frame by storing this vector as a new column in our data frame. We use the $ sign with our code that gives us “access” to the data frame’s column - we then specify the column edition, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column. This $ is another feature of OOP. Let’s take a look. Create a edition vector variable containing our textbook edition numbers - type and execute edition &lt;- c(\"1st\", \"2nd\", \"3rd\", \"4th\"). We then store this as a new column in our data frame under the column name edition by typing and executing giss_df$edition &lt;- edition: # Create a vector with our editions edition &lt;- c(&quot;1st&quot;, &quot;2nd&quot;, &quot;3rd&quot;, &quot;4th&quot;) # Add this vector as a new column to our data frame giss_df$edition &lt;- edition Again, you can go and check your data frame and see the new column using either View() function or by clicking on the tab at the top. You should now have a data frame that looks like: Now we have our data frame, let’s find out a little about it. We can first return the dimensions (the size) of our data frame by using the dim() function (dim simply stands for dimensions in this case.). In your console, type dim(giss_df) and execute: # Check our data frame dimensions dim(giss_df) ## [1] 4 3 We can see we have four rows and three columns. And we can finally use our attributes() function to get the attributes of our data frame. In your console, type attributes(giss_df) and execute: # Check our data frame attributes attributes(giss_df) ## $names ## [1] &quot;year&quot; &quot;page_nos&quot; &quot;edition&quot; ## ## $row.names ## [1] 1 2 3 4 ## ## $class ## [1] &quot;data.frame&quot; You should see that we now get a list of the column and row names, plus the class of the data frame. There is a lot more we could now do with our data frame but we simply do not have time - and we’d much rather implement some of these functions or data management techniques with a much more exciting dataset than the details of the GISS textbook. Hopefully though, this has served as a good introduction to the different data structures you’ll be coming across over the next 6 weeks as we use R - and provided you with some simple code you can return to time and time again for reminders, such as how to create a new column in your data frame. Before we leave the console (and to be honest, we won’t exactly leave it behind), we’ll enter one last line of code for now: Type in install.packages(\"tidyverse\") into the console and execute. # Install the tidyverse library install.packages(&quot;tidyverse&quot;) Leave this code to run - it might take some time but you won’t need to worry about this until you’ve moved onto the practical section. Now we’re ready to move onto our next section, but first - after reading the Tips and recap below - I recommend you take a long break from this workbook! Tips &amp; Tricks R is case-sensitive so you need to make sure that you capitalise everything correctly if required. The spaces between the words don’t matter but the positions of the commas and brackets do. Remember, if you find the prompt, &gt;, is replaced with a + it is because the command is incomplete. If necessary, hit the escape (esc) key and try again. It is important to come up with good names for your objects. In the case of the majority of our variables, we used a underscore _ to separate the words. It is good practice to keep the object names as short as posssible but they still need to be easy to read and clear what they are referring to. Be aware: you cannot start an object name with a number! If you press the up arrow in the command line you will be able to edit the previous lines of code you have inputted. Coding Breakthroughs In this section you have: Entered your first commands into the R command line interface. Created objects in R. Created a vector of values. Executed some simple R functions. Created a data frame. Now, please, make sure you go ahead and take a break! The Tidyverse Philosophy and Principles Over the past weeks a lot of information has come your way, diving deep into the world of GIScience…and now programing. However, whilst you are slowly becoming proficient in using spatial data and hopefully enjoying learning about how to code, we also need to learn about how our data is structured aand organised. I told you there were quite a few learning curves in this module! This is crucial for when you are moving on to working on your own projects where you have to source data yourselves: the vast majority of the data you will find in the public domain (or private domain for that matter) will be what’s becoming colloquially called: dirty data. What we mean by dirty data is data that needs some form of pre-processing, cleaning, and linkage before you can use it for your analysis. Let’s think back to the Ward and Borough Population data that you downloaded from the ONS - we could not use the Excel Spreadsheet straight away as it was within a large workbook with a) many tabs and b) lots of additional formatting (e.g. empty rows, “whitespace”). Instead, we extracted the data we wanted and formatted it into a very simple table, consisting of only fields that contained individual records for each of our Wards or Boroughs. This table would fit what is understood as the tidy data approach, which is a general perspective on how to structure your data in R to ease its analysis. Tidy data was formalised by R Wizard Hadley Wickham in his contribution to the Journal of Statistical Software as a way of consistently structuring your data. In the words, of the Wizard: Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This tidy data approach is very much at the core of the tidyverse R package that we just installed - and for us as soon-to-be connoisseurs of secondary data, is also of significant importance when organising your data for future projects. So what do tidy data look like? Tidy Data In Practice Believe it or not, you can often represent the same underlying data in multiple ways. The example below, taken from the the tidyverse package and described in the R for Data Science book, shows that the same data can organised in four different ways. The data shows the population and cases (of something, e.g. malaria) for each country, for 1999 and 2000: None of these representations are wrong per se, however, not are equally easy to use. Only Table 1 can be considered as tidy data because it is the only table that adheres to the three rules that make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In the case of Table 4 - we even have two tables! These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a table/data frame/tibble. Put each variable in a column. Figure 4.1: A visual representation of tidy data by Hadley Wickham. Why ensure that your data is tidy? Well, there are two main advantages (according to Hadley Wickham): There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in mutate and summary functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. We’ll see all of this in action over the coming weeks - but if you’d like, you can hear this explanation directly from one of R-Studio’s very own resident Data Scientists below. You don’t need to listen to the whole video, but from the beginning until approximately 7:40mins. Tidy Data in R* {-} The R-Studio Youtube channel is also generally a great resource for you to be aware of as you pursue your learning in R. So what is the Tidyverse? The tidyverse is a collection of packages that are specifically designed for these data sciencey tasks of data wrangling, management, cleaning, analysis and visualisation within R-Studio. Our earlier lecture introduced you to the concept of a package - but they are explained in more detail below. Whilst in many cases different packages work all slightly differently, all packages of the tidyverse share the underlying design philosophy, grammar, and data structures as we’ll see over the coming weeks. The tidyverse itself is treated and loaded as a single package, but this means if you load the tidyverse package within your script (through library(tidyverse)), you will directly have access to all the functions that are part of each of the packages that are within the overall tidyverse. This means you do not have to load each package seperately - saving us lines of code, sweet! We’ve already gone ahead and executed the code to install the tidyverse within our various versions of R - but because the tidyverse consists of multiple packages, it may take a little while before everything is installed so be patient! For more information on tidyverse, have a look at https://www.tidyverse.org/. How does the tidyverse help with tidying data? There are some specific functions in tidyverse suite of packages that will help us cleaning and preparing our datasets to create a tidy dataset. The most important and useful functions, from the tidyr and dplyr packages, are: Package Function Use to dplyr select select columns dplyr filter select rows dplyr mutate transform or recode variables dplyr summarise summarise data dplyr group by group data into subgropus for further processing tidyr pivot_longer convert data from wide format to long format tidyr pivot_wider convert long format dataset to wide format These functions all complete very fundamental tasks that we need to manipualte and wrangle our data. We will get to use these over the coming weeks, so do not panic about trying to remember them all right now. Installing and using Libraries/Packages for Data Analysis As you will have heard in our earlier lecture, our common Data Analysis languages, including Python and R, have developed large community bases and as a result there are significant amount of help and support resources for those working in data science. Beyond help and support, these large community bases have been essential for expanding the utility of a programming language for specific types of data analysis. This is because of how programming languages work – they have a core library of functions to do certain things, such as calculate the mean of a dataset as we did earlier. But to do more specific or specialized analysis, such as create a buffer around a point, a function needs to be written to enable this. You either need to write the function yourself – or hope that someone else has written it – plus you need to know that there is the supporting functions around it. E.g. your code can “read” your spatial data and know a) its spatial and b) the projection system its in to calculate a distance. Without this, you won’t be able to run your function or do your analysis. These community bases have identified these gaps, such as for spatial data reading and analysis, and spent considerable amount of time writing these functions and supporting functions to add to the core library. These functions often get packaged into an additional library (or can be called a package) that you add to your own core library by installing this library to your computer AND then importing it to your work through your script. The tidyr and dplyr packages with the tidyverse are just two examples of these additional libraries created by the wider R community. The code you just ran asked R-Studio to fetch and install the tidyverse into your R-Studio - so this means we’ll be able to use these libraries in our practical below simply by using the library(tidyverse) code at the top of our script. One thing we need to be aware of when it comes to using functions in these additional libraries, is that sometimes these functions are called the same thing as the base R package, or even, in some cases, another additional library. We therefore often need to specify which library we want to use this function from, and this can be done with a simple command (library::function) in our code - as we’ll see in practice over the next few weeks, so just make a mental note of this for now. Whilst we’ve gone ahead and installed the tidyverse, each time we start a new script, we’ll need to load the tidyverse. We are going to show all of this in our next Prcatical, which gets you to analyse crime in London whilst putting into place everything we’ve been dicussing today. Practical 3: Analysing Crime in 2020 in London Wow, we’ve got through a lot today - and barely even started our practical! But, what I can say, is that there is not a substantial more to learn in terms of principles and practicalities of programming beyond building up your “dictionary/vocabulary” of programming libraries and respective commands. There are some more complicated coding things we can do, such as for and while loops and if statements, but, for now, consider yourself a solid beginner programmer. As a result, we’re ready to put this all into practice in our practical today, which will be relatively short in comparison to everything you’ve been through above. What we’ll be doing today is running an exploratory data analysis, using basic statistics, of crime in London over a monthly basis. Let’s get started. Setting Up R-Studio for GEOG0030 In the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this isn’t routinely the case. In RStudio, we can use scripts to build up our code that we can run repeatedly - and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by creating a new project. To put it succintly, projects in R-Studio keep all the files associated with a project together — input data, R scripts, analytical results, figures. This means we can easily keep track of - and access - inputs and outputs from different weeks across our module, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with directories and paths a whole lot easier - particularly if you have followed the folder structure I advised at the start of the module. Let’s go ahead and make a New Project directly within our GEOG0030 folder. Click on File -&gt; New Project –&gt; Existing Directory and browse to your GEOG0030 folder. Click on Create Project. You should now see your main window switch to this new project - and if you check your Files window, you should now see a new R Project called GEOG0030: We are now “in” the GEOG0030 project - and any folders within the GEOG0030 project can be easily accessed by our code. Furthermore, any scripts we create will be saved in this project. Note, there is not a “script” folder per se, but rather your scripts will simply exist in this project. You can test this change of directly by selecting the Terminal window (next to your Console window) to access the Terminal in R-Studio and type our pwd command. You should see that our current directory is your GEOG0030 folder. R for Data Science by Hadley Wickham and Garret Grolemund Your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. I’d stick with these sections for now, but have a quick glance at what’s available in the book. Setting up our script In our shorter practical sessions above, we’ve had a bit of fun playing with the R code within the R console and seeing how we can store variables and access information about them. Furthermore, we’ve had a look at the different data structures we may use moving forward. But ultimately this really doesn’t offer the functionality that we want for our work - or even the reality of what we need to do with spatial analysis. What we really want to do is to start building scripts and add start analysing some data! Therefore, for the majority of our analysis work, we will type our code within a script and not the console. Let’s create our first script: Click on File -&gt; New File –&gt; R Script. You can also use the plus symbol over a white square as a shortcut or even Ctrl/CMD + Shift + N. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date. Let’s go ahead and save this script straight away. Save your script as: wk4-csv-processing.r. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with csv processing. This will help us a lot in the future when we come to find code that we need for other projects. I personally tend to use one script per type of processing or analysis that I’m completing. For example, if you are doing a lot of data cleaning to create a final dataset that you’ll then analyse, its best practice to separate this from your analysis script so you do not continually clean your raw datasets when you run your script. Giving our script some metadata The first bit of code you will want to add to any script is to add a TITLE. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (#) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run. Let’s go ahead and give our script a TITLE - and maybe some additional information: Add the following to your script (substitute accordingly): # Combining Police Data csvs from 2020 into a single csv # Followed by analysis of data on monthly basis # Script started January 2021 # NAME Save your script. Load Our Libraries Now we have our title, the second bit of code we want to include in our script is to load our libraries (i.e. the installed packages we’ll need in our script): Type the following into the script: # Libraries used in this script: # Load the tidyverse library library(tidyverse) By loading simply the tidyverse, we have a pretty good estimate that we’ll be able to access all the functions that we’re going to need today. However, often when developing a script, you’ll realise that you’ll need to add libraries as you go along in order to use a specific function etc. When you do this, always add your library to the top of your script - if you ever share your script, it helps the person you are sharing with recognise quickly if they need to install any additional packages prior to trying to run the script. It also means your libraries do not get lost in the multiple lines of code you are writing. Setting Up Our Directory Understanding and accessing our directory path used to be the worst part of programming. And if you do not use the Project approach advocated above, it certainly will continue to be. If, for example, we did not use a project approach, we would need to set our working directory directly within our script using the command: setwd(\"file/path/to/GEOG0030) We’d therefore need to know what our path is and hope we do not make any mistakes. There are some automated shortcuts to doing this in R using the Files window, but ultimately, having to set a working directory is becoming a thing of the past. Because we are using a project approach - we do not need to set a working directory - because we’re already in it! Therefore, when looking for data in our folders, we know pretty much the path we’ll need to take. However, we still might need to access data from another folder outside of our GEOG0030 folder - so we need to know how to do this. To help with this, we’re going ot add one more library to our library list, called the here library. We won’t go into too much detail what this library does per se, but essentially it alows you to direct R to a specific area on your computer and a specific file with relative ease. We actually won’t use it in this practical, but I wanted to get you into the habit of adding it to your scripts by default. First, you’ll need to install this library to your computer. In your Console window, type and execute: # Install the here library via your console install.packages(&quot;here&quot;) Once installed, we can go ahead and load this after our tidyverse library - your script should look like so: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) One thing to note, not only does installing and loading libraries need to occur in two different parts of R-Studio, but when installing, your library needs to be in \"\" but when loading, it does not. File and folder names best practice Please ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; &amp; $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ if you like. Remember to save your script. We’re now ready to run these first two lines of code. Running a script in R-Studio There are two main ways to run a script in R-Studio - all at once or by line/chunk by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code iteratively. To run line-by-line: By clicking: Select the line or chunk of code you want to run, then click on Code and choose Run selected lines. By key commands: Select the line or chunk of code you want to run and then hold Ctl or Cmd and press Return. To run the whole script By clicking: Click on Run on the top-right of the scripting window and choose Run All. By key commands: Hold Option plus Ctl or Cmd and R. Stopping a script from running If you are running a script that seems to be stuck (for whatever reason) or you notice some of your code is wrong, you will need to interrupt R. To do so, click on Session -&gt; Interrupt R. If this does not work, you may end up needing to Terminate R but this may lose any unsaved progress. Run your code line-by-line In this practical, I recommend running each line (or set of lines) of code you enter as you go - rather than wait til the end and execute the whole script. This way you will be able to find any bugs as you go along. Don’t forget to scroll to the top of your script and execute your library loads! Data Import and Processing We’re now ready to get started with using the crime data csvs currently sat in our all_crime folder. To do so, we need to first figure out how to import the csv and understand the data structure it will be in after importing. Importing and loading Data To read in a csv into R requires the use of a very simple function: read_csv(). We can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single csv, we’ll go ahead and just use the function with a simple parameter. # Read in a single csv from our crime data crime_csv &lt;- read_csv(&quot;data/raw/crime/all_crime/2020-11-metropolitan-street.csv&quot;) We can explore the csv we have just loaded as our new crime_csv variable and understand the class, attributes and dimensions of our variable. # Check class and dimensions of our data frame # you can also check the attributes if you would like - this will load up a huge list of every row though! class(crime_csv) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; dim(crime_csv) ## [1] 96914 12 We’ve found out our variable is a data frame, containing 96914 rows and 12 columns. We can also tell it’s a big file - so best not load it up right now. We however do not want just the single csv and instead what to combine all our csvs in our all_crime folder into a single dataframe - so how do we do this? Joining all of our csvs files together into a single data frame This will be the most complicated section of code you’ll come across today, and we’ll use some functions that you’ve not seen before - we also need to install and load an additional library to use something known as a pipe function which I’ll explain in more detail next week. In your console, install the magrittr package: # Install the magrittr library via your console install.packages(&quot;magrittr&quot;) And in your # Load libraries section of your script, add the magrittr library. Your library section should look like this: # Libraries used in this script: # Load the tidyverse library # Load the here library library(tidyverse) library(here) library(magrittr) Remember to execute the loading of the magrittr library by selecting the line and running the code. Now we’re ready to add and run the following code: # Read in all csvs and append all rows to a single data frame all_crime_df &lt;- list.files(path=&quot;data/raw/crime/all_crime&quot;, full.names=TRUE) %&gt;% lapply(read_csv) %&gt;% bind_rows This might take a little time to process (or might not), as we have a lot of data to get through. You should see a new datafarme appear in your global environment called all_crime, for which we now have 1,099,507 observations! Explaining the above code It is a little difficult to explain the code above without going into a detail explanation of what a pipe is (next week) but essentially what these three lines of code does is: List of of the files found in the data path: \"data/raw/crime/all_crime Read each of these as a csv (this is the lapply() function) in as a dataframe And then bind the rows of these dataframes to a single dataframe called all_crime_df We’ll go into more detail about pipes next week. We can now have a look at our large dataframe in more detail. Let’s have a look # Understand our all_crime_df cols, rows and print the first five rows ncol(all_crime_df) ## [1] 12 nrow(all_crime_df) ## [1] 1099507 head(all_crime_df) ## # A tibble: 6 x 12 ## `Crime ID` Month `Reported by` `Falls within` Longitude Latitude Location ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 37c663d86… 2020… City of Lond… City of Londo… -0.106 51.5 On or n… ## 2 5b89923fa… 2020… City of Lond… City of Londo… -0.118 51.5 On or n… ## 3 fb3350ce8… 2020… City of Lond… City of Londo… -0.113 51.5 On or n… ## 4 07172682a… 2020… City of Lond… City of Londo… -0.112 51.5 On or n… ## 5 14e02a604… 2020… City of Lond… City of Londo… -0.111 51.5 On or n… ## 6 &lt;NA&gt; 2020… City of Lond… City of Londo… -0.0980 51.5 On or n… ## # … with 5 more variables: `LSOA code` &lt;chr&gt;, `LSOA name` &lt;chr&gt;, `Crime ## # type` &lt;chr&gt;, `Last outcome category` &lt;chr&gt;, Context &lt;lgl&gt; You should now see with have the same number of columns as our previous single csv, but 1,099,507 rows! You can also see that the head() function provides us with the first five rows of our dataframe. You can conversely use tail() to provide the last five rows. Filtering our data to a new variable For now in our analysis, we only want to extract the theft crime in our dataframe - so we will want to filter our data based on the Crime type column. However, as we can see, we have a space in our field name for Crime type and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding (or else our code will break!), we need to rename our fields. To do so, we’ll first get all of the names of our fields so we can copy and paste these over into our code: # Get the field names of our all_crime_df names(all_crime_df) ## [1] &quot;Crime ID&quot; &quot;Month&quot; &quot;Reported by&quot; ## [4] &quot;Falls within&quot; &quot;Longitude&quot; &quot;Latitude&quot; ## [7] &quot;Location&quot; &quot;LSOA code&quot; &quot;LSOA name&quot; ## [10] &quot;Crime type&quot; &quot;Last outcome category&quot; &quot;Context&quot; We can now copy over these values into our code to create a new vector variable that contains these field names, updated without spaces. We can then rename the field names in our dataset - just as we did with our GISS table earlier: # # Create a new vector containing updated no space / no capital field names no_space_names &lt;- c(&quot;crime_id&quot;, &quot;month&quot;, &quot;reported_by&quot;, &quot;falls_within&quot;, &quot;longitude&quot;,&quot;latitude&quot;, &quot;location&quot;, &quot;lsoa_code&quot;, &quot;lsoa_name&quot;, &quot;crime_type&quot;, &quot;last_outcome_category&quot;, &quot;context&quot;) # Rename our df field names using these new names names(all_crime_df) &lt;- no_space_names Note, we could have cleaned our data further and so would only needed to rename a few columns using slicing - but we’ll save data frame slicing for next week!. We now have our dataframe ready for filtering - and to do so, we’ll use the filter() function for the dplyr library. This function is really easy to use - but there is also a filter() function in the R base library - that does something different to the function in dplyr. As a result, we need to use a specific type of syntax - library::function - to tell R to look for and use the the filter function from the dplyr library rather than the default base library. We then also need to populate our filter() function wsith the necessary paramteres to extract only the “Theft from the person” crime type. This includes providing the function with our main dataframe plus the filter query, as outliend below: # Filter all_crime_df to contain only theft, store as a new variable: all_theft_df all_theft_df &lt;- dplyr::filter(all_crime_df, crime_type == &#39;Theft from the person&#39;) You should now see the new variable appear in your Environment with 28,476 observations. Great, you’ve completed your first ever filter using programming. We now want to follow the tidy data philosophy and create one final dataframe to allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London - and luckily for us dplyr has another function that will do this for us, known simply as count(). You perhaps can see already that dplyr is likely to become well-used library by us in Geocomputation…! Go ahead and search the help to understand the count() function - you’ll also see that there is only one function called count() so far, i.e. the one in the dplyr library, so we do not need to use the additional syntax we used above. Let’s go ahead and count the number of thefts in London by month. The code for this is quite simple: # Count in the all_theft_df the number of crimes by month and store as a new dataframe theft_month_df &lt;- count(all_theft_df, month) We’ve stored the output of our count() function to a new dataframe: theft_month_df. Go ahead and look at the dataframe to see the output - it’s a very simple table containing simply the month and n, i.e. the number of crimes occuring per month. We can and should go ahead and rename this column to help with our interpretation of the dataframe. We’ll use a quick approach to do this, that uses selection of the precise column to rename only the second column: # Rename the second column of our new data frame to crime_totals names(theft_month_df)[2] &lt;- &quot;crime_totals&quot; This selection is made through the [2] element of code added after the names() function we have used earlier. We’ll look more at selection, slicing and indexing in next week’s practical. Data Analysis: Distribution of Crime in London by month in 2020 We now have our final dataset ready for our simple analysis for our Assignment. Assignment 1: Analysis of crime per month in London in 2020 For your assignment this week, I would like you to complete two tasks. 1. Basic statistics First what I would like you to do, using the code you’ve written previously is to find out: What is the mean average crime per month? What is the median average crime per month? You may also automate the collection of the max, min and range of our crime per month dataframe. 2. Monthly graph The second thing I would like you to do is present our data on a simple bar chart. The basic code to generate a bar chart is provide here: # Read in a single csv from our crime data barplot(theft_month_df$crime_totals, main=&quot;Crime distribution in London by month in 2020&quot;, names.arg = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;)) As you’ll see, we have added a Title to our graph (main= argument), whilst used the names.arg parameter to add the months of the year in along the x-axis. Using the barplot() documentation, I would like you to figure out how to change the bar chart fill and borders another colour from grey and black respectively. You may also look to customise your chart further, but primarily I’d like you to work out how to change these colours to something more aesthetically appealing! You do not need to submit your bar chart this week, but have it ready for your work in Week 5. Beyond the assignment, just take a look at your bar chart and how the distribution of crime changed last year… Well that’s pretty cool, huh?! I wonder what happened in March to make theft from a person decrease so substantially? Next week, we’ll be doing a lot more with our dataset - including a lot more data wrangling and of course spatial anlaysis, but hopefully this week has shown you want you can achieve with just a few lines of code. We’ve managed to take a dataset of over 1 million records and clean and filter it to provide a chart that actually shows the potential impact of the COVID-19 lockdown on theft crime in London. Of course, there is a lot more research and exploratory data analysis we’d need to complete before we could really substantiate our findings, but this first chart is certainly a step in the right direction! Extension Activity: Mapping Other Crime Type Distributions Across London If you’ve whizzed through this workshop and would like an additional challenge, you are more than welcome to deploy the code you’ve used above on one or more other crime types in London. If you remember, each crime is categorised into one of 14 types. These include: Crime Type Description All crime Total for all categories. Anti-social behaviour Includes personal, environmental and nuisance anti-social behaviour. Bicycle theft Includes the taking without consent or theft of a pedal cycle. Burglary Includes offences where a person enters a house or other building with the intention of stealing. Criminal damage and arson Includes damage to buildings and vehicles and deliberate damage by fire. Drugs Includes offences related to possession, supply and production. Other crime Includes forgery, perjury and other miscellaneous crime. Other theft Includes theft by an employee, blackmail and making off without payment. Possession of weapons Includes possession of a weapon, such as a firearm or knife. Public order Includes offences which cause fear, alarm or distress. Robbery Includes offences where a person uses force or threat of force to steal. Shoplifting Includes theft from shops or stalls. Theft from the person Includes crimes that involve theft directly from the victim (including handbag, wallet, cash, mobile phones) but without the use or threat of physical force. Vehicle crime Includes theft from or of a vehicle or interference with a vehicle. Violence and sexual offences Includes offences against the person such as common assaults, Grievous Bodily Harm and sexual offences. You can conduct the same analysis on one or more of these categories in addition to theft, to see if you can find a similar pattern in their prevalance/distribution over the same months. R for Data Science by Hadley Wickham and Garret Grolemund As highlighted earlier, your only key reading for this week is to read through the R for Data Science handbook - although you can take each section at your own leisure over the coming weeks. For this week, I’d highly recommend reading more about why we use Projects, whilst this section tells us more about Scripts. In addition, you can look at Sections: 1, 2, 4, 11 and 12. I’d stick with these sections for now, but have a quick glance at what else is available in the book. We’ll be looking at Data Transformation (5) and Visualisation (3) next week, plus more on Exploratory Data Analysis (7). Don’t worry about completing the exercises - unless you would like to! Recap In this section you have learnt how to: Create an R script. Load a csv into R, perform some analysis, and write out a new csv file to your working directory. Subset R data frames by name and also column and/or row number. Created a simple graph to plot crime in London by month. Learning Objectives You should now hopefully be able to: Understand the basics of programming and why it is useful for data analysis Recognise the differences and purpose of a console command versus the creation of a script Explain what a library/package is and how to use them in R/R-Studio Explain the tidyverse philosophy and why it is useful for us as data analysts Wrangle and manage tabular data to prepare it for analysis Conduct basic descriptive statistics using R-Studio and R Acknowledgements Part of this page is adapted from POLS0008: Understanding Data and GEOG0114: Exploratory spatial data analysis by Justin Van Dijk at UCL as well as Software Carpentry’s Introduction to R Data Types and Data Structures Copyright © Software Carpentry. The examples and datasets used in the workbook are original to GEOG0030. "],["programming-for-giscience-and-spatial-analysis.html", "5 Programming for GIScience and Spatial Analysis", " 5 Programming for GIScience and Spatial Analysis Content for this week will be released at 10am on the 9th February 2021. "],["analysing-spatial-patterns-i-spatial-auto-correlation-regression.html", "6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression", " 6 Analysing Spatial Patterns I: Spatial Auto-Correlation &amp; Regression Content for this week will be released at 10am on the 23rd February 2021. "],["analysing-spatial-patterns-ii-clusters.html", "7 Analysing Spatial Patterns II: Clusters", " 7 Analysing Spatial Patterns II: Clusters Content for this week will be released at 10am on the 2nd March 2021. "],["rasters-zonal-statistics-and-interpolation.html", "8 Rasters, Zonal Statistics and Interpolation", " 8 Rasters, Zonal Statistics and Interpolation Content for this week will be released at 10am on the 9th March 2021. "],["geodemographics.html", "9 Geodemographics", " 9 Geodemographics Content for this week will be released at 10am on the 16th March 2021. "],["optional-road-network-analysis.html", "OPTIONAL: Road Network Analysis", " OPTIONAL: Road Network Analysis Content for this week will be released at 10am on the 16th March 2021. "],["cartography-and-visualisation-ii.html", "10 Cartography and Visualisation II", " 10 Cartography and Visualisation II Content for this week will be released at 10am on the 23rd March 2021. "],["assessment-information.html", "Assessment Information Useful additional resources", " Assessment Information Geocomputation is assessed through two separate Assessments: Social Atlas: The first assessment will involve the completion of a spatial analysis project, based on the theory, concepts and application learnt during the module. For this coursework you are required to create a small “social atlas” on a topic or area that interests you. Exam: The second assessment will take the form of an Exam, the exact format to be confirmed. More information on your Assessments will be provided at the end of Week 5 (i.e. 12th February 2021). Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out that may be useful for your first Assessment: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 "],["week-2-practical-alternate-using-agol-for-population-mapping.html", "Week 2 Practical Alternate: Using AGOL for Population Mapping", " Week 2 Practical Alternate: Using AGOL for Population Mapping For this week’s Practical Alternate, we’ll be using ArcGIS Online.The instructions below outline how to complete the same processing as the Q-GIS practical conducts. It is also includes the all extra information included in the Q-GIS tutorial about Attribute Joins and Classification Schemes. One thing I would recommend is to watch the two videos within the practical: a short introduction to Q-GIS and an introduction to Attribute Tables and Properties. These are not included within this practical. A short introduction to ArcGIS Online Feel free to skip this part and head straight to the Sign Up to ArcGIS Online section. What is ArcGIS Online? ArcGIS Online (AGO) is Esri’s “Software-as-a-service” GIS offering, that enables you to conduct some basic (as well as some quite advanced!) spatial analysis, as well as create interactive maps for sharing with others. It has some very similar features to Esri’s GIS Desktop software (ArcMap and ArcPro) discussed in last week’s lecture, but it does not have all of their capabilities, for example, it is not a tool I would use to create paper maps/ones for use in publication. In contrast, it does offer a lot of web interactivity, as we’ll see when we share our maps with one another at the end of the practical. It also has some really useful analysis tools that are quick and easy to use, in compared to their counterparts in the Desktop software, such as creating something called “drive-time” or “network” buffers – we’ll have a look at these next week when looking at spatial properties. The Esri Ecosystem AGO is just one of the may additional tools Esri offers. Their entire ecosystem of products is huge - you can see their list of products here. Whilst many of the products and/or extensions are created for specific industries and purposes, there are other web-based tools that I can recommend you looking into during your time on this course, to at least be aware of the capabilities moving forward. The first would be ArcGIS StoryMaps, where you can create a webpage a bit like the ones you are using for these workshops, but also integrate any maps you make within the page as well! In addition to StoryMaps, Esri has its own survey collector application – ArcGIS Survey123. Within this application, you can create online forms to collect spatial and non-spatial data – which can then be directly used as inputs within AGO or StoryMap applications. You might see why I call this an “ecosystem” – Esri have constructed their software, tools and applications to work well together and sync across their respective platforms (e.g. web, desktop and mobile)! You just need to be able to afford the license to use them in the first place – we have an educational license which enables ArcMap usage, whilst Esri (as you’ll see) offers AGO for free for non-commercial purposes. Using ArcGIS Online – limitations to be aware of! AGO is a very useful solution to conducting GIS and spatial analysis within the Esri ecosystem when you, as an analyst, are in a scenario where computing resources may be restrictive (and therefore downloading Q-GIS, or Esri’s ArcMap or ArcPro is not a good idea) but internet access is ok – or if, for example, you own a Mac and do not want to split your hard drive to install a Windows operating system, or, finally, when Virtual Machine alternatives may not meet your needs. One thing to flag before we get started with AGO though, is that the platform does simplify some aspects of the traditional GIS workflow – for example, defining your Coordinate Reference Systems and Projection Systems (CRS/PS). This will be an issue in next week’s practical - but I will address this in more detail then. Another aspect of using AGO instead of Desktop software is that your data is ultimately hosted on the AGO server, rather than on your hard drive. One critical aspect of GIS is to practice good file management, including establishing a good use of folders and data storage protocols, so you know where to access your data and where your outputs from any analysis are stored. Normally in Desktop GIS, or even in R-Studio, you would establish a project folder, and within this folder create folders for your data, scripts and outputs (e.g. maps, figures). With ArcGIS specifically, you can use geodatabases to store any spatial data you use or create, whilst R-Studio can create a project in which your work will be saved. QGIS in comparison will rely primarily on your use of folders. For AGO, your data and layers will be managed in their server, under your content page - so in a way you still need to organise your files somewhat. Sign Up to ArcGIS Online With all of this in mind, let’s get ourselves set up to continue with the practical! First head to: https://learn.arcgis.com/en/become-a-member/ and fill in your details as below: By signing up here, you will become part of the Learn ArcGIS organisation, which Esri has created to help support teaching of GIS online for non-commercial purposes, i.e. what we’re doing here! Once you’ve clicked on Join, you’ll need to go authorise your account from your UCL email. The sign-up box may not disappear (it did not for me), but check your emails first before clicking on Join again! Once you’ve authorised your account, you’ll be taken to the ArcGIS online home screen – feel free to navigate around the website yourself before starting the practical. Practical Instructions Open your ArcGIS Online (AGO) home webpage and click on the Map tab. This is the main interface we will use to import and analyse our data and is a light version of a traditional Desktop GUI-GIS. Save your map as Population Change in London. You can add as many tags as you like – I used: population | London | analysis. Let’s go ahead and start adding data to our map. Click on the Add button and select Add Layer from File: 3. You should then see the following pop-up: As you can see from the instructions, AGO requires the shapefile to be provided in the zipfile format, rather than the individual files. As a result, what we need to do is navigate to our raw data folder and compress our wardLondon_ward` shapefile to create a zipped version. For now, close down the pop-up. Navigate to your boundaries folder in your file management system and then to 2011 folder. Select all files related to the London_ward shapefile and right-click and select compress or archive (depending on your Operating System): 6. Back in AGO, click back on the Add button and select Add Layer from File. Navigate to your London_ward zipfile and select this as your file to import. + Click the option to ‘Keep original features’ and then import the layer. You should see the data appear on your map as such: The data is current styled according to the different names in our Name field. Before we go ahead and import our population data, let’s first change the symbolisation of our dataset to only a Single Symbol. In the Change Style option appearing on the left of the screen, select the option to show the Location (Single symbol). All of your wards should now be displayed in a single colour – but we would prefer to see them as simple grey polygons with a black outline. Click on the Options button hovering over the Single Location box that should have appeared. Click on the blue Symbols button – for FILL, select a light grey colour, for OUTLINE, choose a colour of your choice and make sure to reduce the transparency of your lines. Once you are happy with your symbolisation, click through (i.e. click the OKs and DONEs) until you are presented with the main screen. You should now see your Ward data in the main map canvas of AGO - you should also see what looks like a table of contents on the left-hand side which now contains the layers for the London_Ward data and the base map. If you hover over the layer, you’ll see the various options we have through AGO to interact with our dataset. These options include: Displaying the legend (i.e. how the data is symbolised) Show table (i.e. displaying the Attribute Table) Change style (i.e. return to the Symbology options you were just using) Perform analysis (i.e. what we’ll use to perform different types of analysis on our layers, including our attribute join) More options (i.e. other tools you might want to use, such as zooming to a layer or saving your content) We’ll utilise a few of these options over the practical. Turning layers on/off &amp; drawing orders The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS. The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you’re used to working with if you’ve played with interactive web mapping applications before! The second concept is the order in which your layers are drawn – and this is relevant for both GUI GIS and when using plotting libraries such as ggplot2 in R-Studio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or ‘called’ in your function in code. Being aware of this need for “order” is important when we shift to using R-Studio and ggoplot2 to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped! For us using AGO right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 ward files, it is useful to know about this order as we’ll need to display them individually to export them at the end. Joining our population data to our ward shapefile We’re now going to join our 2011 population data to our 2011 ward shapefile to create our Ward Population dataset. To do this, we need to add the 2011 population data to our map. In AGO, import the 2011 population csv from your working folder by using the Add data button as before. Note for csvs, the population data can be imported as the original file and there is no need to zip it. For the csvs, add the layer just as a table. Now we have it loaded on our map, we can now join this table data to our spatial data using an Attribute Join. What is an Attribute Join? An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we’ll look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the ‘matching’ of rows: Figure from Esri documentation on Attribute Joins Essentially you need a single identifying ID field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always join our table data to our shape data (I like to think about it as putting the table data into each shape). As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code. To make a join work, you need to make sure your ID field is correct across both datasets, i.e. no typos or spelling mistakes. Computers can only follow instructions, so they won’t know that St. Thomas in one dataset is that same as St Thomas in another, or even Saint Thomas! It will be looking for an exact match! As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, when creating the join, we will always prefer to use the CODE over their names. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling! Common errors, such as adding in spaces or using 0 instead O (and vice versa) can still happen – but it is less likely. To make our join work therefore, we need to check that we have a matching UID across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match. Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. Use the Show Table option to open the Attribute Tables for both the Ward and Population data layers. We can see that both our respective *_Code fields have the same codes so we can use these to create our joins. To create an Attribute Join in AGO, you need to click on the Perform Analysis button when hovering over the London_Ward dataset and then open the Summarise Data drop-down to find the Join Features tool. Click on the Join Features tool and add the appropriate inputs for each step (again make sure you get your target and join layer and their respective fields correct and also select to keep all target features): 6. Click Run Analysis! AGO will then return to the original layer screen and create your new layer! It might take a little time for AGO to create this join - just be patient. But, if, after ten minutes, your join has still not worked, you may download the complete 2011 ward population dataset here. It is provided as a zipfile, which you’ll then need to add/upload to your AGO map. Once AGO has finished processing, the next thing we would like to do with this dataset is to style it by our newly added Population field to show population distribution around London. Hover over your new layer, and then click the Symbology button. Next in Choose an attribute to show choose our POP2011 (population) field. AGO will automatically style your data for you as Counts and Amounts (Size), which is a useful way to view our dataset. This approach is also known as Proportional Symbols. We can see from just this initial styling that there are some differences in population size across our wards. You can click on the Options button to find more ways of altering how your data is currently styled. This also provides you with a histogram of the data (even though it is on its side!) to see how our data is distributed. As we can see, our dataset shows a normal Gaussian distribution. Understanding our data’s distribution is really important when it comes to thinking about how to style and visualise our data as well as understanding what sort of analysis techniques we can apply to our data – more on this next week. Alternatively to the Size option, you can also create a choropleth map from our dataset. Navigate back to the Change Style menu of the Symbology tab (this may involve clicking done to exit the previous menus). Click on Counts and Amounts (Color) – you’ll see the map change automatically to a choropleth map. We can change the colour scheme of our map, as well as the way in which AGO displays the data either via the Theme dropdown or by clicking the Classify Data box. The latter provides you with more control over the data’s classification scheme and details the different types of classification schemes you can use with your data: We’ll be looking at this in more detail next week, but for now, we’ll use the Natural Breaks option. Click on Natural Breaks and change it to 7 classes. You may also want to reduce the transparency. Then click OK and then DONE. :::note A little note on classification schemes Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data’s distribution and match your visualisation accordingly. Feel free to explore using the different options with your dataset at the moment – the results are almost instantaneous using AGO, which makes it a good playground to see how certain parameters or settings can change your output. ::: You should now be looking at something like this: You’ll be able to see that we have some missing data - and this is for several wards within the City of London. This is because census data is only recorded for 8 out of the 25 wards and therefore we have no data for the remaining wards. As a result, these wards are left blank, i.e. white, to represent a NODATA value. One thing to flag is that NODATA means no data - whereas 0, particularly in a scenario like this, would be an actual numeric value. It’s important to remember this when processing and visualising data, to make sure you do not represent a NODATA value incorrectly. Empty wards in the City of London In our Q-GIS tutorial, we would now go through the steps to exporting the data. When using AGO, we do not need to worry about this at the moment - make sure you save your map, and if you would like you can save your final Layer to your AGO content. To do this: Click on the More Options button when hovering your item and select Save Layer. Name your layer London_Ward_Population_2011 and add a few tags. Click create item. This layer should then appear in your AGO content. When looking at your layer in the AGO Content page (not the Map page we have been using), if you publish your layer, you will created a hosted layer than you can then download as a Shapefile for use within Desktop software etc from the AGO website. Next Steps: Joining our 2014/2015 and 2018/2019 data You now need to repeat this whole process for your 2015 and 2019 datasets. Remember, you need to: Zip/compress the respective Ward dataset prior to adding it to AGO Add the respective Ward dataset as the zipped file Load the respective Population csv Join the two datasets together using the Join Features tool. Style your data appropriately. Save your joined dataset as a layer within your AGO content. To then make accurate visual comparisions against our three datasets, theorectically we would need to standardise the breaks at which our classification schemes are set at. This can be a little fiddly with AGO, so for now, if you want, you can leave your symbolisation to the default settings. Alternatively, if you would like to standardise your classification breaks, you’ll need to return to the Classify Data option within the Symbology tab and manually change your breaks here. If you have any issues with AGO and joining your datasets (i.e. the processing takes longer than 10 minutes each), you can download the remaining pre-joined files here. You will need to download, then upload these datasets to style them appropriately for the next step. Exporting our maps for visual analysis To export each of your maps (as is) to submit to our Powerpoint from AGO: Click on Print –&gt; Map with Legend and either take a screenshot or use the File –&gt; Export as PDF and then trim your PDF to the map. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w2. Next week, we’ll look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do. To get a picture of each of your different layers, remember to turn on and off each layer (using the check box). Finally, remember to save your project! Assignment 3: Submit your final maps and a brief write-up Your final assignment for this week’s practical is to submit your maps to the second part of the Powerpoint presentation in your seminar’s folder. In addition to your maps, I would like you to write 1-3 bullet points summarising the changing spatial distributions of population (and population growth) in London at the ward level. You can find the Powerpoint here with an example template. Please make sure to submit your maps prior to your seminar in Week 4. And that’s it for this week’s practical! Whilst this has been a relatively straight-forward practical to introduce you to a) spatial data and b) ArcGIS Online, it is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical. We’ll delve into some of these in more detail in our discussion on Friday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. I really want to make sure these concepts are clear to you will be really important as we move forward with using R-Studio and the Command Line Interface for our spatial analysis and as we add in more technical requirements, such as thinking about projection systems, as well as a higher complexity of analysis techniques. Extension: Population as a Raster Dataset This Extension Task will be updated at the end of Week 2. Learning Objectives You should now hopefully be able to: Understand how we represent geographical phenomena and processes digitally within GIScience Explain the differences between discrete (object) and continuous (field) spatial data models Explain the differences between raster and vector spatial data formats and recognise their respective file types Know how to manage and import different vector and table data into a GIS software Learn how to use attributes to join table data to vector data Know a little more about Administrative Geographies within London. Symbolise a map in Q-GIS using graduated symbolisation. "],["week-3-practical-alternate-using-agol-for-crime-mapping.html", "Week 3 Practical Alternate: Using AGOL for Crime Mapping", " Week 3 Practical Alternate: Using AGOL for Crime Mapping For this week’s alternate practical, we will continue to use AGOL as our main GIS system to process and analyse our data, similar to the Q-GIS practical. This week, compared to last, we will however be dealing with two main ‘compromises’ in our use of AGOL vs. Q-GIS that you will need to be aware of: 1. Projections Within Q-GIS, as you will see if you read through the main practical (which I highly advised doing), setting the Project and Data CRSs are an essential step in successfully analysing and visualisng spatial data correctly. In our case, our practical data uses two CRS - BNG for the administrative boundaries and WGS84 for the crime data. As a result, in the main practical, we use a tool within Q-GIS to reproject our crime data into the same CRS as the administrative boundaries, i.e. BNG. AGOL, in comparision, uses WGS84/Mercator as default CRS for all its data mapping and visualisation - and can only be altered if you change the basemap to a dataset that is in your desired CRS/PS (although in their Beta version, it appears that there will be more user choice over choosing projections). AGOL will convert our data, such as our Administrative Boundaries (which are in British National Grid) “on the fly” to WGS84 - so we do not need to reproject it. However, this will mean we may forgot this step in the future - for example, when using R-Studio instead; therefore it is important to recognise that this aspect of our GIS workflow is missed in this tutorial. 2. Map-Making &amp; Visualisation AGOL also has relatively limited capacity for map-making. As a result, for this practical, I would recommend using a mixture of your output from AGOL alongside either a graphic software or even PowerPoint to make final additions that are needed to your map. You’ll see these recommendations below as my proposed workaround. Detailed cartography is one of the key advantages that Q-GIS and ArcGIS have over the use of programming tools, such as R-Studio. As you’ll see in future practicals, we can still make excellent maps in R-Studio, it just takes a little more time and experience than the “speed” of the traditional GIS software. With all that being said, we still have plenty of data analysis to learn - so let’s get started! Practical Instructions We now have our datasets downloaded and ready to process - we simply need to get them loaded onto our AGO map. Open your ArcGIS Online (AGO) home webpage and click on the Map tab. Save your map as Crime Analysis in London. You can add as many tags as you like – I used: crime | London | analysis. Let’s go ahead and start adding data to our map. Ward Population We already have our ward_population_2019.shp dataset complete from last week, so we can go ahead and add this directly to the map. Click on the Add button and select Add Layer from File: Add your `ward_population_2019.shp’ to the map using the Add -&gt; Add Layer From File tool. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Borough Population We, as yet, do not have a borough_population_2019.shp. To create our Borough population shapefile, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. We will let you complete this without full instructions as your first “GIS challenge”. Remember, you need to: Add the London_Borough_Excluding_MHW.shp file from the 2011 boundary data (in your raw data folder) to your map. Remember, to add a shapefile to AGOL, you need to compress it first into a zip file. Add the borough_population_2019.csv you have just created from your working folder to your map. This can just be added as a csv, but remember to add just as a table. Join the two datasets together using the Join Features tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the London_Borough dataset. Crime Data We now are ready to load and map our crime data. We will now add our all_theft_2020.csv from our raw folder - we will load this exactly like our previous population csv but this time, when presented with the option, we need to add the point coordinates to map our crime data as points. Before we can load our data, we actually need to do one final step of data cleaning (compared to the Q-GIS tutorial). Unfortunately AGOL cannot handle all of the data from 2020 - so we need to reduce the size of our dataset. For now, we will analyse theft crime for March in 2020. Open your all_theft_2020.csv from our raw folder in your number editing software, and extract all rows that the field Month is equal to 2020-03. I do not mind how you do this, but just make sure to save to a new CSV called: march_theft_2020.csv into your working folder. Once you have extracted this smaller dataset: Click on Add -&gt; Add Layer From File. AGOL should automatically detect the Longitude and Latitude columns and map your data. You may have an error message, but you can ignore this for now. Unlike Q-GIS, we do not need to reproject our data when using AGOL as the software has done this for us - we can, as a result, move on to the next step - counting the number of crimes in each of our Wards and Boroughs respectively. Counting Points-in-Polygons with AGOL The next step of our analysis is incrediby simple - as AGOL has an in-built tool for us to use. We will use the Aggregate Points tool within the Summarise Data option after clicking on the Perform Analysis button when hovering over the March_theft_2020 point dataset to count how many crimes have occured in both our Wards and our Boroughs. We will then have our count statistic which we will need to normalise by our population data to create our crime rate final statistic! Let’s get going and first start with calculating the crime rate for the borough scale: Hover over the March_theft_2020 point dataset and click the Perform Analysis button. Next, click on the Summarise Data option and then Aggregate Points. Set up your query as follows: Point Layer: march_theft_2020 Aggregation areas: borough_population Add Statistics: Field = UID | Statistic = Sum Result Layer Name: borough_march_theft No need to add anything to Option 4 (group by) Click Run Analysis Note, the processing for the borough level will take around 5 minutes to process. Once complete, re-run the same process for the Ward scale - note this will take even longer to process (approximately 10 minutes). Calculating Crime Rate in AGOL Whilst it’s great that we’ve got our crimecount, as we know, what we actually need is a crime rate to account for the different sizes in population in the boroughs and to avoid a population heat map. We therefore now want to add a Crime Rate statistic to our dataset - we want to normalise our crime count by our population data. Note, if your processing did not work OR is still processing after 10 minutes, you can find two pre-processed Ward and Borough shapefiles with population and crime count here. Let’s go ahead and calculate our Crime Rate statistic. To do this in AGOL, we actually need to access the Symbology menu. Click on the Change Style / Symbology button whilst hovering over your borough dataset that now contains your crime count. In 1: Choose Attribute, click on the drop-down next to the currently selected attribute, scroll to the bottom of the list and click on New Expression: A new pop-up window should appear - this is where we’ll add a new expression to calculate our crime rate. Edit the Custom name to crime_rate. In the expression box, remove the current comments. Add the expression: ($feature.crimecount/$feature.POP2019)*10000 You can double-click on the fields on the right of the box to add these if you want. Click on OK. You’ll now have a new field populated with the crime rate for each borough. Whilst you’re still in the Style tab, go ahead and change the styling to show the crime rate for each borough by creating a choropleth map: Click on Counts and Amounts (Colour) to access the correct style option. You can click on the Classify check box to change the type of classification scheme and the number of classes. Once you’re happy with your styling, click through the OKs and Dones to return to the main AGO map. Now you just need to repeat the above steps for your Ward crime data and we’ll have our maps ready to export. Remember to uncheck the box next to your borough layer, so this data does not show through on your ward map (and make sure you ward map has not shown through on your borough layer for that matter!). As an FYI, we won’t export our data from AGOL as I’ll provide you with the final shapefiles in Week 5 for the practical that week. Just remember to save your map once you’ve exported your maps as instructed below. Making our Crime Rate Maps for analysis in AGOL As stated at the top of this practical, AGOL does not have a huge amount of flexibility when it comes to cartography - so we’ll need to get a bit inventive. To create maps to submit for your assignment, these are the steps I recommend: Click on Print -&gt; Map with Legend and either take a screenshot or use the File -&gt; Export as PDF. Remember to save your final map outputs in your maps folder. You may want to create a folder for these maps titled w3. To add the various map components, open up PowerPoint - preferably find a slide size that is wider than it is taller. Insert your two maps onto your slide, placing them side by side. Now we have our two maps ready, we can add our main map elements: Title Orientation Data Source We will use PowerPoints text box and shape features to replicate this on our slide. We won’t at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy. Using the tools on PowerPoint: Add a north arrow: choose an arrow from PPTs shapes and draw it pointing upwards (i.e. north on your map) Add a title at the top of the page, and subtitles above the individual maps. Finally add a box detailing Data Sources, you can copy and paste the text below: Contains National Statistics data © Crown copyright and database right [2015] (Open Government Licence) Contains Ordnance Survey data © Crown copyright and database right [2015] Crime data obtained from data.police.uk (Open Government Licence). Feel free to customise your font etc. to give the final maps a good aesthetic. Once you have added these properties in, you should have something that looks a little like this: You’ll notice I got a bit creative with cropping my maps in various ways to try to create a similar format to the one I made in the Q-GIS tutorial. The only thing I haven’t managed to add is a scale bar as this would require accuracy in digitising that we do not have in PPT. This is as close as we can get with creating maps using AGO - it would be great if we could edit it further but this is the constraint with using an online tool. Export map Now we have our maps put together, we are finally ready to export our map! Export your slide as a PNG. Remember to also save your original slide. Assignment 1: Submit your final maps and a brief write-up Your one and only assignment for this week is to submit your maps your relevant seminar folder here. What I’d like you to do is, on your own computer, create a new Word document and set the orientation to Landscape. Copy over your map into the first page and ensure it takes up the whole page. On a second page, write a short answer (less than 100 words) to our original question set at the start of our practical: Does our perception of crime (and its distribution) in London vary at different scales? Export this to a PDF and upload to your relevant seminar folder. (Again, no need for names - but you might need to come up with a random code on your PDF name, just in case someone else has the same file name as you!) And that’s it for this week’s practical! This has been a long but (hopefully!) informative practical to introduce you to cartography and visualisation in AGOL. It is really important for you to reflect on the many practical, technical and conceptual ideas you’ve come across in this practical and from the lecture material earlier. We’ll delve into some of these in more detail in our discussion on Monday, but it would also be great for you to come to the seminar equipped with questions that might have arisen during this practical. If you feel you didn’t quite understand everything this week, do not worry too much - Week 5 will serve as a good revision of everything we’ve covered here! Extension Activity: Mapping Crime Rates using Averages If you have managed to get through all of this in record time and are still looking for some more work to do - one question I would ask you is: could we visualise our crime rate data in a better way? At the moment, we are looking at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. Could we use a different approach - using a diverging colour scheme - that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the wards or borough? I think so! But first, you’ll need to calculate these averages and then our individual ward/boroughs (%?) difference from this mean. In AGOL, you may find an option that does this for us. If not, you can use the New Expression builder to calculate these values. See if you can think how to calculate this - and then create your diverging maps. You can either just export an image of your results (in the main Q-GIS window) or you are welcome to update your current maps to reflect this new approach. Learning Objectives You should now hopefully be able to: Explain what a Geographic Reference System and a Projected Coordinate System is and their differences. Understand the limitations of different PCSs and recognise when to use each for specific anlaysis. Know what to include - and what not to include - on a map. Know how to represent different types of spatial data on a map. Explain what the Modifiable Areal Unit Problem is and why poses issues for spatial analysis. Reproject data in Q-GIS. Map event data using a ‘best-practice’ approach. Produce a map of publishable quality. Acknowledgements Acknowledgements are made in appropriate sections, but overall this week, as evident, has utilised the Q-GIS documentation extensively. "]]
